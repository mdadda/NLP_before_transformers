{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN & Glove Embeddings\n",
    "\n",
    "**CNNs and Multi Layer Perceptrons are feed-forward neural networks. They do not adequately model sequences.**\n",
    "\n",
    "A sequence is an ordered collection of items. Traditional machine learning assumes data points to be independently and identically distributed (IID), but in many situations, like with language, speech, and time-series data, one data item\n",
    "depends on the items that precede or follow it. Such data is also called sequence data. Sequential information is everywhere in human language. For example, speech can be considered a sequence of basic units called phonemes. \n",
    "In a language like English, words in a sentence are not haphazard. They might be constrained by the words that come before or after them. For example, in the English language, the preposition “of” is likely followed by the article “the”; for\n",
    "example, “The lion is the king of the jungle.” As another example, in many languages, including English, the number of a verb must agree with the number of the subject in a sentence.Sometimes these dependencies or constraints can be arbitrarily long.\n",
    "In short, **understanding sequences is essential to understanding human language.**\n",
    "\n",
    "We begin by introducing the most basic neural network sequence model: the **recurrent neural network (RNN)**.\n",
    "There are several different members in the RNN family, but here we work with only **the most basic form, sometimes called the Elman RNN or Vanilla RNN**. \n",
    "\n",
    "The goal of recurrent networks — both the basic Elman form and the more complicated forms — is to learn a representation of a sequence. This is done by maintaining a hidden state vector that captures the current state of the sequence.\n",
    "The hidden state vector is computed from both a current input vector and the previous hidden state vector. **The\n",
    "hidden vector of each time step is dependent on both the input at that time step and the hidden vector from the previous time step**.\n",
    "\n",
    "**A new hidden vector is computed using a hidden-to-hidden weight matrix to map the previous hidden state vector and an input-to-hidden weight matrix to map the input vector. Crucially, the hidden-to-hidden and input-to-hidden weights are shared across\n",
    "the different time steps. The intuition you should take away from this fact is that, during training, these weights will be adjusted so that the RNN is learning how to incorporate incoming information and maintain a state representation summarizing the input seen so far. The RNN does not have any way of knowing which time step it is on. Instead, it is simply learning how to transition from one time step to another and maintain a state representation that will minimize its loss function**.\n",
    "\n",
    "Using the same weights to transform inputs into outputs at every time step is another example of parameter sharing. We saw how CNNs share parameters across space. CNNs use parameters, called kernels, to compute outputs from subregions in the input data. You can think of an RNN sharing parameters across time and a CNN sharing parameters across space.\n",
    "\n",
    "Because words and sentences can be of different lengths, the RNN or any sequence model should be equipped to handle variable-length sequences. One possible technique is to restrict sequences to a fixed length artificially. Another technique is masking; in brief, **masking allows for the data to signal when certain inputs should not count toward the gradient or the eventual output**.\n",
    "\n",
    "\n",
    "**Word Vectors** are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses. Here, you will explore word vectors derived via **GloVe**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                 # to create 'serialised' directory\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json                               # for serialising dictionaries\n",
    "import pickle                             # for serialising numpy arrays\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from collections import Counter           # this is for stopwords removal\n",
    "from nltk.corpus import stopwords         # this is for stopwords removal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy                              # for tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_row', None)              # show all rows of a dataframe\n",
    "pd.set_option('display.max_column', None)           # show all columns of a dataframe\n",
    "pd.set_option('display.max_colwidth', None)         # show the full width of columns\n",
    "pd.set_option('precision', 2)                       # round to 2 decimal points\n",
    "pd.options.display.float_format = '{:,.2f}'.format  # comma separators and two decimal points: 4756.7890 => 4,756.79 and 4656 => 4,656.00 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)           # global seeding. Sklearn uses this internally therefore there is no need to set a random seed when using Sklearn\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOK = spacy.load('en_core_web_sm')                                         # for tokenisation\n",
    "stop_words = stopwords.words('english')                                    # for stopwords\n",
    "stopwords_dict = Counter(stop_words)                                       # for stopwords\n",
    "max_len = 100                                                              # for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mari\\miniconda3\\envs\\summer\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')      # pytorch cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "### 1. Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filepath, filename, encoding='utf-8'):\n",
    "    full_path = '{}\\{}'.format(filepath, filename)\n",
    "    return pd.read_csv(full_path, encoding=encoding, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenise(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords_dict])  # remove stopwords\n",
    "    \n",
    "    return [token.text for token in TOK.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Serialisation / Deserialisation\n",
    "\n",
    "https://www.tutorialspoint.com/object_oriented_python/object_oriented_python_serialization.htm\n",
    "\n",
    "In the context of data storage, serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted and reconstructed later.\n",
    "\n",
    "In serialization, an object is transformed into a format that can be stored, so as to be able to deserialize it later and recreate the original object from the serialized format.\n",
    "\n",
    "We can do this with Pickle, JSON, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialise(obj_type, filepath, filename, obj):\n",
    "    full_path = '{}\\{}'.format(filepath, filename)\n",
    "    if obj_type == 'python':\n",
    "        with open(full_path, 'w') as f:\n",
    "            json.dump(obj, f)\n",
    "            \n",
    "    if obj_type in ['numpy', 'pandas', 'tensor']:\n",
    "        with open(full_path, 'wb') as f:\n",
    "            pickle.dump(obj, f)                                         # protocol 0 is printable ASCII\n",
    "        \n",
    "        \n",
    "\n",
    "def deserialise(obj_type, filepath, filename):\n",
    "    full_path = '{}\\{}'.format(filepath, filename)\n",
    "    if obj_type == 'python':\n",
    "        with open(full_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "            \n",
    "    if obj_type in ['numpy', 'pandas', 'tensor']:\n",
    "        with open(full_path, 'rb') as f:\n",
    "            return pickle.load(f)                                        # protocol 0 is printable ASCII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vocabulary from the Traing Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_dict(series):\n",
    "    tokens = series.explode()\n",
    "    tokens = tokens.tolist()\n",
    "    tokens = Counter(tokens)                                    # frequency distribution\n",
    "\n",
    "    num_words_before = len(tokens.keys())\n",
    "    \n",
    "    #To avoid 'RuntimeError: dictionary changed size during iteration' error, we need to make a .copy() of the dictionary.\n",
    "    #This way we iterate over the original dictionary keys and delete elements on the fly.\n",
    "    for k,v in tokens.copy().items():\n",
    "        if v < 2:\n",
    "            del tokens[k]\n",
    "\n",
    "    tokens = {word: i for i,word in enumerate(tokens.keys())}    # word2index dictionary\n",
    "\n",
    "    tokens['<UNK>'] = len(tokens)\n",
    "    tokens['<MASK>'] = len(tokens)\n",
    "    tokens['<END>'] = len(tokens)\n",
    "    tokens['<START>'] = len(tokens)\n",
    "    tokens[''] = len(tokens)\n",
    "    \n",
    "    num_words_after = len(tokens.keys())\n",
    "    \n",
    "    return num_words_before, num_words_after, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. All Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_from_file(glove_filepath):\n",
    "    \n",
    "    embeddings_dict = {} \n",
    "    \n",
    "    with open(glove_filepath, mode='r', encoding=\"utf-8\") as f:\n",
    "        for index,line in enumerate(f):\n",
    "            line_split = line.split()\n",
    "            word = line_split[0]\n",
    "            embeddings_dict[word] = np.array(line_split[1:], 'float32')\n",
    "    \n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_matrix(embeddings_dict, tokens):  \n",
    "    \n",
    "    embedding_size = len(next(iter(embeddings_dict.values())))     # length of each word embedding (e.g.300 dimensions)\n",
    "    \n",
    "    embeddings_matrix = np.zeros((len(tokens)+1, embedding_size))  # len(tokens) is the length of the vocabulary (i.e. unique words in the corpus)\n",
    "                                                                   # the '+1' after len(tokens) is necessary because we want the first embedding (i.e. at index 0) to be a zero vector; this will be used for the paddings (we want all the '0' paddings to be paired with the zero vector)\n",
    "    not_in_glove = []\n",
    "    for token,i in tokens.items():\n",
    "        if token in embeddings_dict:\n",
    "            embeddings_matrix[i+1, :] = embeddings_dict[token]     # '+1' :see comment above\n",
    "        else:\n",
    "            not_in_glove.append(token)\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)             # if word is not in Glove emebeddings, creates a new vector with random numbers drawn from pytorch xavier_uniform distribution\n",
    "            embeddings_matrix[i+1, :] = embedding_i                # '+1' :see comment above\n",
    "\n",
    "    return not_in_glove, embeddings_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encoding(input_sequence, tokens, max_len):\n",
    "#     encoded = np.zeros(max_len, dtype=int)\n",
    "#     encoded_lst = np.array([tokens.get(word, tokens['<UNK>']) for word in input_sequence])\n",
    "#     length = min(max_len, len(encoded_lst))\n",
    "#     encoded[:length] = encoded_lst[:length]\n",
    "#     return encoded\n",
    "\n",
    "def encoding(input_sequence):\n",
    "    encoded = np.zeros(max_len, dtype=int)\n",
    "    encoded_lst = np.array([tokens.get(word, tokens['<UNK>']) for word in input_sequence])\n",
    "    length = min(max_len, len(encoded_lst))\n",
    "    encoded[:length] = encoded_lst[:length]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Vanilla RNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings,                      # embedding_size = 300, num_embeddings=95465\n",
    "                 rnn_hidden_size, output_dim, dropout_p, batch_first=True,\n",
    "                 pretrained_embeddings=None, padding_idx=0):                # pretrained_embeddings = embedding_matrix\n",
    "                                                                            # padding_idx=0 makes sure that the padding vector (which in our embedding matrix is at index 0) doesn't get updated during training when Freeze=False\n",
    "        super(RNNClassifier, self).__init__()                      \n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                    num_embeddings=num_embeddings,\n",
    "                                    padding_idx=padding_idx)        \n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            # freeze=False : the tensor does not get updated in the learning process. Equivalent to self.emb.weight.requires_grad = False\n",
    "            self.emb = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True, padding_idx=padding_idx)  \n",
    "            \n",
    "#             self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "#                                     num_embeddings=num_embeddings,\n",
    "#                                     padding_idx=padding_idx,\n",
    "#                                     _weight=pretrained_embeddings)\n",
    "            \n",
    "            \n",
    "            # nn.Embedding is a model parameter layer, which is by default trainable.\n",
    "            # If you want to fine-tune word vectors during training, these word vectors are treated as model parameters \n",
    "            # and are updated by backpropagation. You can also make it untrainable by freezing its gradient \n",
    "            # (False ==> freezes the backprop) \n",
    "#             self.emb.weight.requires_grad=False    \n",
    "            \n",
    "        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=rnn_hidden_size, batch_first=batch_first) #initialise RNN model\n",
    "        \n",
    "        self._dropout_p = dropout_p      \n",
    "        self.fc1 = nn.Linear(in_features=rnn_hidden_size, out_features=rnn_hidden_size)\n",
    "        self.fc2 = nn.Linear(in_features=rnn_hidden_size, out_features=output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "    # Note that we don't use the Sigmoid activation in our final layer during training because we use the \n",
    "    # nn.BCEWithLogitsLoss() loss function which automatically applies the the Sigmoid activation.\n",
    "        x_embedded = self.emb(x_in.long())                # x_in is the encoded review e.g. [3386  603 1112    0    0    0    0    0    0    0]. It comes from the train_loader\n",
    "        hiddens = self.rnn(x_embedded)                    # the output of self.rnn is a tuple\n",
    "        hiddens = hiddens[0]                              # we need the first item of the tuple (i.e. index 0)\n",
    "        hiddens = hiddens[:,-1,:]                         # for each batch item, we need the last hidden state (second dimension with value -1)\n",
    "        \n",
    "        hidden_layer = F.relu(F.dropout(self.fc1(hiddens), p=self._dropout_p))\n",
    "        output_layer = self.fc2(hidden_layer).squeeze(1)   # squeeze(1) to make the prediction the same shape as the target i.e. as a scaler not a 2d-vector\n",
    "\n",
    "        if apply_sigmoid: \n",
    "            output_layer = sef.sigmoid(output_layer)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.  Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_hat, y):\n",
    "    y_hat_label = torch.round(torch.sigmoid(y_hat))\n",
    "\n",
    "    correct_predictions_sum = (y_hat_label == y).sum().float()\n",
    "    acc = correct_predictions_sum/y.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params(full_embedding_layer_name):\n",
    "    for param in full_embedding_layer_name.parameters():\n",
    "        return param\n",
    "\n",
    "\n",
    "def check_params(classifier_name):\n",
    "    for name, child in classifier_name.named_children():\n",
    "        print('Layer name: {} --- {}'.format(name, child), end='\\n\\n')            \n",
    "        print('ToT Params: {:,}'. format(sum(p.numel() for p in child.parameters())), end='\\n\\n') \n",
    "    \n",
    "        count = 0\n",
    "        for param in child.parameters():\n",
    "            print('Param length: {:,}'.format(len(param)), end='\\n\\n')\n",
    "            print(param, end='\\n\\n')\n",
    "            print('Are parameters being updated during backprop? {}'.format(param.requires_grad), end='\\n\\n')\n",
    "            count += 1\n",
    "\n",
    "        print('Total Sets of Parameters: {}'.format(count), end='\\n\\n')\n",
    "        print('*' * 90)\n",
    "    \n",
    "\n",
    "def num_params(classifier_name):\n",
    "    \n",
    "    # PyTorch torch.numel() method returns the total number of elements in the input tensor\n",
    "    trainable_parameters = sum(param.numel() for param in classifier_name.parameters() if param.requires_grad)  \n",
    "    all_parameters = sum(param.numel() for param in classifier_name.parameters())  \n",
    "    \n",
    "    return trainable_parameters, all_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Inference on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        idx = 0\n",
    "        if depends_on:\n",
    "            idx = self.tasks.index(depends_on) + 1\n",
    "        def inner(f):\n",
    "            self.tasks.insert(idx, f)\n",
    "            return f\n",
    "        return inner\n",
    "    \n",
    "    # Add the run() method which should take in an 'input_' argument\n",
    "    def run(self, input_):\n",
    "        output = input_\n",
    "        # Iterate through the self.tasks property, and call each function with the previous output\n",
    "        for task in self.tasks:\n",
    "            output = task(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline.tasks = [clean_tokenise, encoding]   #calls the functions created at the top of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new functions to the pipeline\n",
    "\n",
    "@inference_pipeline.task(depends_on=encoding)\n",
    "def convert_to_tensor(text):\n",
    "    return torch.Tensor(text)\n",
    "\n",
    "@inference_pipeline.task(depends_on=convert_to_tensor)\n",
    "def infer(text):\n",
    "    # Disable grad\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction = classifier(text.unsqueeze(0))\n",
    "        probability_value = classifier.sigmoid(prediction).item()\n",
    "        \n",
    "        if probability_value < 0.5:\n",
    "            prediction_label = 'Negative'\n",
    "        else:\n",
    "            prediction_label = 'Positive'\n",
    "            \n",
    "    return prediction_label, probability_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = r'C:\\Users\\Mari\\Desktop\\MACHINE_LEARNING\\NLP_Stanford_University\\BOOK\\YELP\\dataset'\n",
    "train_set_filename = 'raw_train.csv'\n",
    "test_set_filename = 'raw_test.csv'\n",
    "\n",
    "serialise_directory = r'C:\\Users\\Mari\\Desktop\\MACHINE_LEARNING\\NLP_Stanford_University\\BOOK\\YELP\\serialised'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'serialised' directory if it doesn't exist\n",
    "\n",
    "try:\n",
    "    os.makedirs(serialise_directory)\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets\n",
    "train_original = read_csv(dataset_directory, train_set_filename)\n",
    "test = read_csv(dataset_directory, test_set_filename)\n",
    "\n",
    "# Add column names\n",
    "train_original.columns = ['target', 'review']\n",
    "test.columns = ['target', 'review']\n",
    "\n",
    "# Split Targets from Features\n",
    "X_train_original = train_original['review']\n",
    "y_train_original = train_original['target']\n",
    "X_test = test['review']\n",
    "y_test = test['target']\n",
    "\n",
    "# Re-label Target: In PyTorch labels need to start at 0\n",
    "# 1 ==> 0 (these are negative reviews)\n",
    "# 2 ==> 1 (these are positive reviews)\n",
    "y_train_original = y_train_original - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "# Split 'train_original' in 'train' and 'val' \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_original, y_train_original, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(X_train), X_train.shape)\n",
    "# print(type(X_val), X_val.shape)\n",
    "# print(type(X_test), X_test.shape)\n",
    "# print(type(y_train), y_train.shape)\n",
    "# print(type(y_val), y_val.shape)\n",
    "# print(type(y_test), y_test.shape, end='\\n\\n')\n",
    "# print(X_train.head(2), end='\\n\\n')\n",
    "# print(y_train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning And Tokenization\n",
    "\n",
    "In addition to creating a subset that has three partitions for training, validation, and testing, we also minimally clean the data by adding whitespace around punctuation symbols and removing extraneous symbols that aren’t punctuation for all the splits.\n",
    "\n",
    "\n",
    "1. **apply** works on a row / column basis of a DataFrame \n",
    "2. **applymap** works element-wise on a DataFrame\n",
    "3. **map** works element-wise on a Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.map(clean_tokenise)\n",
    "X_val = X_val.map(clean_tokenise)\n",
    "X_test = X_test.map(clean_tokenise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From glove txt, create a dictionary of all glove embeddings where KEY is a WORD, and VALUE is a NUMPY ARRAY:\n",
    "glove_embeddings = load_glove_from_file('C:/GloVe/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary From Training Corpus\n",
    "\n",
    "The embedding matrix (see later) is created only from the training dataset.\n",
    "\n",
    "The training dataset should be sufficiently rich/representative enough to cover all data you expect to see in the future.\n",
    "\n",
    "New data must have the same integer encoding as the training data prior to being mapped onto the embedding when making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DICT of the unique words in the training set\n",
    "num_words_before, num_words_after, tokens = tokens_dict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding is created from the training dataset.\n",
    "\n",
    "It should be sufficiently rich/representative enough to cover all data you expect to in the future.\n",
    "\n",
    "New data must have the same integer encoding as the training data prior to being mapped onto the embedding when making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_glove, embedding_matrix = make_embedding_matrix(glove_embeddings, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Training Dataset\n",
    "\n",
    "We need to convert our text into a numerical form that can be fed to our model as input.\n",
    "\n",
    "1. We have create a vocabulary (see section '10. Vocabulary') where each key is a unique word from the training corpus, and each value is the index of that word in the 'tokens' dictionary.\n",
    "2. Choose the maximum length of any review.\n",
    "3. Encode each list of tokens by replacing each word with its index from the 'tokens' dictionary.\n",
    "\n",
    "Note: **mean_len** (see below) is the mean of tokens length in the training set. We set the max length of the encoded reviews equal to the mean_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = X_train.map(lambda input_lst: encoding(input_lst, tokens=tokens, max_len=max_len))   \n",
    "X_val_encoded = X_val.map(lambda input_lst: encoding(input_lst, tokens=tokens, max_len=max_len))\n",
    "X_test_encoded = X_test.map(lambda input_lst: encoding(input_lst, tokens=tokens, max_len=max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pd.Series to PyTorch Tensors\n",
    "# NB: set the values in X_train, X_val and X_test as a list of arrays (as opposed to array of arrays) --- see above\n",
    "\n",
    "x_train_tensor = torch.Tensor(list(X_train_encoded.values))\n",
    "x_val_tensor = torch.Tensor(list(X_val_encoded.values))\n",
    "x_test_tensor = torch.Tensor(list(X_test_encoded.values))\n",
    "y_train_tensor = torch.Tensor(list(y_train.values))\n",
    "y_val_tensor = torch.Tensor(list(y_val.values))\n",
    "y_test_tensor = torch.Tensor(list(y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full dataset (like a DataFrame in Pandas) from the two tensors\n",
    "train_dataset =  TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise datasets after tokenisation\n",
    "serialise('pandas', serialise_directory, 'X_train', X_train)\n",
    "serialise('pandas', serialise_directory, 'X_val', X_val)\n",
    "serialise('pandas', serialise_directory, 'X_test', X_test)\n",
    "serialise('pandas', serialise_directory, 'y_train', y_train)\n",
    "serialise('pandas', serialise_directory, 'y_val', y_val)\n",
    "serialise('pandas', serialise_directory, 'y_test', y_test)\n",
    "serialise('pandas', serialise_directory, 'mean_len', mean_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise glove embeddings (althought 'glove_embeddings' is a dictionary, its values are numpy arrays therefore we need to choose 'numpy')\n",
    "serialise('numpy', serialise_directory, 'glove_embeddings', glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise tokens\n",
    "serialise('python', serialise_directory, 'tokens', tokens)\n",
    "serialise('python', serialise_directory, 'num_words_before', num_words_before)\n",
    "serialise('python', serialise_directory, 'num_words_after', num_words_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise datasets embedding matrix\n",
    "serialise('numpy', serialise_directory, 'embedding_matrix', embedding_matrix)\n",
    "serialise('python', serialise_directory, 'not_in_glove', not_in_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise encoded datasets\n",
    "serialise('pandas', serialise_directory, 'X_train_encoded', X_train_encoded)\n",
    "serialise('pandas', serialise_directory, 'X_val_encoded', X_val_encoded)\n",
    "serialise('pandas', serialise_directory, 'X_test_encoded', X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise tensors\n",
    "serialise('tensor', serialise_directory, 'x_train_tensor', x_train_tensor)\n",
    "serialise('tensor', serialise_directory, 'x_val_tensor', x_val_tensor)\n",
    "serialise('tensor', serialise_directory, 'x_test_tensor', x_test_tensor)\n",
    "serialise('tensor', serialise_directory, 'y_train_tensor', y_train_tensor)\n",
    "serialise('tensor', serialise_directory, 'y_val_tensor', y_val_tensor)\n",
    "serialise('tensor', serialise_directory, 'y_test_tensor', y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise PyTorch Dataset\n",
    "serialise('tensor', serialise_directory, 'train_dataset', train_dataset)\n",
    "serialise('tensor', serialise_directory, 'val_dataset', val_dataset)\n",
    "serialise('tensor', serialise_directory, 'test_dataset', test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deserialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise datasets after tokenisation\n",
    "X_train = deserialise('pandas', serialise_directory, 'X_train')\n",
    "X_val = deserialise('pandas', serialise_directory, 'X_val')\n",
    "X_test = deserialise('pandas', serialise_directory, 'X_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = deserialise('pandas', serialise_directory, 'y_train')\n",
    "y_val = deserialise('pandas', serialise_directory, 'y_val')\n",
    "y_test = deserialise('pandas', serialise_directory, 'y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(X_train), X_train.shape)\n",
    "# print(type(X_val), X_val.shape)\n",
    "# print(type(X_test), X_test.shape)\n",
    "# print(type(y_train), y_train.shape)\n",
    "# print(type(y_val), y_val.shape)\n",
    "# print(type(y_test), y_test.shape, end='\\n\\n')\n",
    "# print(X_train.head(2), end='\\n\\n')\n",
    "# print(y_train.head(2), end='\\n\\n')\n",
    "# print(type(mean_len))\n",
    "# print(mean_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise glove embeddings\n",
    "glove_embeddings = deserialise('numpy', serialise_directory, 'glove_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(glove_embeddings))\n",
    "# print(len(glove_embeddings))\n",
    "# print(glove_embeddings['car'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise datasets\n",
    "tokens = deserialise('python', serialise_directory, 'tokens')\n",
    "num_words_before = deserialise('python', serialise_directory, 'num_words_before')\n",
    "num_words_after = deserialise('python', serialise_directory, 'num_words_after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(tokens))\n",
    "# print(len(tokens))\n",
    "# print(tokens['car'], end='\\n\\n')\n",
    "# print(type(num_words_before))\n",
    "# print(num_words_before, end='\\n\\n')\n",
    "# print(type(num_words_after))\n",
    "# print(num_words_after, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise embedding matrix\n",
    "embedding_matrix = deserialise('numpy', serialise_directory, 'embedding_matrix')\n",
    "not_in_glove = deserialise('python', serialise_directory, 'not_in_glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(embedding_matrix))\n",
    "# print(embedding_matrix.shape)\n",
    "# print(embedding_matrix[56], end='\\n\\n')\n",
    "# print(type(not_in_glove))\n",
    "# print(len(not_in_glove))\n",
    "# print(not_in_glove[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise encoded datasets\n",
    "X_train_encoded = deserialise('numpy', serialise_directory, 'X_train_encoded')\n",
    "X_val_encoded = deserialise('numpy', serialise_directory, 'X_val_encoded')\n",
    "X_test_encoded = deserialise('numpy', serialise_directory, 'X_test_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(X_train_encoded))\n",
    "# print(X_train_encoded.shape)\n",
    "# print(type(X_val_encoded))\n",
    "# print(X_val_encoded.shape)\n",
    "# print(type(X_test_encoded))\n",
    "# print(X_test_encoded.shape, end='\\n\\n')\n",
    "# print(X_train.sample(2), end='\\n\\n')\n",
    "# print((X_train_encoded.map(lambda x: len(x))).mean())\n",
    "# print((X_val_encoded.map(lambda x: len(x))).mean())\n",
    "# print((X_test_encoded.map(lambda x: len(x))).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise tensors\n",
    "x_train_tensor = deserialise('tensor', serialise_directory, 'x_train_tensor')\n",
    "x_val_tensor = deserialise('tensor', serialise_directory, 'x_val_tensor')\n",
    "x_test_tensor = deserialise('tensor', serialise_directory, 'x_test_tensor')\n",
    "y_train_tensor = deserialise('tensor', serialise_directory, 'y_train_tensor')\n",
    "y_val_tensor = deserialise('tensor', serialise_directory, 'y_val_tensor')\n",
    "y_test_tensor = deserialise('tensor', serialise_directory, 'y_test_tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(x_train_tensor), x_train_tensor.shape)\n",
    "# print(type(x_val_tensor), x_val_tensor.shape)\n",
    "# print(type(x_test_tensor), x_test_tensor.shape)\n",
    "# print(type(y_train_tensor), y_train_tensor.shape)\n",
    "# print(type(y_val_tensor), y_val_tensor.shape)\n",
    "# print(type(y_test_tensor), y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise PyTorch Dataset\n",
    "train_dataset = deserialise('tensor', serialise_directory, 'train_dataset')\n",
    "val_dataset = deserialise('tensor', serialise_directory, 'val_dataset')\n",
    "test_dataset = deserialise('tensor', serialise_directory, 'test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(train_dataset), len(train_dataset))\n",
    "# print(type(val_dataset), len(val_dataset))\n",
    "# print(type(test_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For small dataset is fine to use the whole training data at every training step (i.e. batch gradient descent). \n",
    "# If we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. \n",
    "# Thus, we need to slice our dataset accordingly. Do you want to do it manually?! Me neither!\n",
    "# So we use the 'DataLoader' class for this job. We tell it which dataset to use, the desired mini-batch size and if we’d \n",
    "# like to shuffle it or not. That’s it!\n",
    "# Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1048, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1048, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1048, shuffle=False)\n",
    "\n",
    "# To retrieve a sample mini-batch, one can simply run the command below.\n",
    "# It will return a list containing two tensors: one for the features, another one for the labels:\n",
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Classifier\n",
    "\n",
    "At its core, the training routine is responsible for instantiating the model, iterating over the dataset, computing the output of the model when given the data as input, computing the loss (how wrong the model is), and updating the model proportional to the loss. \n",
    "\n",
    "Although this may seem like a lot of details to manage, there are not many places to change the training routine, and as such it will become habitual in your deep learning development process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "rnn_hidden_size = 64\n",
    "output_dim = 1\n",
    "dropout_p = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise classifier\n",
    "# We need to send our model to the same device where the data is. If our data is made of GPU tensors, \n",
    "# our model must “live” inside the GPU as well. That's what '.to(device)' is there for.\n",
    "\n",
    "classifier = RNNClassifier(embedding_size=embedding_dim, num_embeddings=num_embeddings, batch_first=True,\n",
    "                           rnn_hidden_size=rnn_hidden_size, output_dim=output_dim,\n",
    "                           dropout_p=dropout_p, pretrained_embeddings=embedding_matrix, padding_idx=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), lr=0.01)    #filtering only for the params that require updating doesn't speed up training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialise / Deserialise Embedding Parameters before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_before = params(classifier.emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise embedding params before training\n",
    "serialise('tensor', serialise_directory, 'params_before', params_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise embedding params before training\n",
    "params_before = deserialise('tensor', serialise_directory, 'params_before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2319, -0.1954,  0.0334,  ...,  0.1139,  0.1851, -0.3686],\n",
       "        [ 0.0206,  0.3291, -0.0162,  ..., -0.0801, -0.2023,  0.0679],\n",
       "        ...,\n",
       "        [-0.1062,  0.1081,  0.0919,  ..., -0.0722,  0.0423, -0.0406],\n",
       "        [-0.0733,  0.1390, -0.0326,  ...,  0.1367,  0.0150, -0.0859],\n",
       "        [-0.1353, -0.1157,  0.0118,  ..., -0.0008,  0.0778, -0.0832]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: emb --- Embedding(95399, 300, padding_idx=0)\n",
      "\n",
      "ToT Params: 28,619,700\n",
      "\n",
      "Param length: 95,399\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2319, -0.1954,  0.0334,  ...,  0.1139,  0.1851, -0.3686],\n",
      "        [ 0.0206,  0.3291, -0.0162,  ..., -0.0801, -0.2023,  0.0679],\n",
      "        ...,\n",
      "        [-0.1062,  0.1081,  0.0919,  ..., -0.0722,  0.0423, -0.0406],\n",
      "        [-0.0733,  0.1390, -0.0326,  ...,  0.1367,  0.0150, -0.0859],\n",
      "        [-0.1353, -0.1157,  0.0118,  ..., -0.0008,  0.0778, -0.0832]])\n",
      "\n",
      "Are parameters being updated during backprop? False\n",
      "\n",
      "Total Sets of Parameters: 1\n",
      "\n",
      "******************************************************************************************\n",
      "Layer name: rnn --- RNN(300, 64, batch_first=True)\n",
      "\n",
      "ToT Params: 23,424\n",
      "\n",
      "Param length: 64\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.0342,  0.0777,  0.0987,  ..., -0.1055, -0.0512,  0.0473],\n",
      "        [ 0.0541,  0.1067,  0.0610,  ...,  0.0290, -0.1078,  0.0907],\n",
      "        [ 0.0664, -0.0912, -0.0034,  ..., -0.0674, -0.0509, -0.0054],\n",
      "        ...,\n",
      "        [-0.0844, -0.0959, -0.0343,  ..., -0.0049, -0.1184,  0.0582],\n",
      "        [-0.1220, -0.0919,  0.0636,  ...,  0.0588, -0.0307, -0.1189],\n",
      "        [-0.0737,  0.0339, -0.0837,  ..., -0.0823,  0.0433,  0.0989]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 64\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.0118,  0.0018,  0.1124,  ..., -0.0354,  0.0373, -0.0056],\n",
      "        [ 0.0227,  0.0521,  0.0924,  ...,  0.0864,  0.0151,  0.0566],\n",
      "        [-0.0949, -0.0073,  0.0414,  ..., -0.0773,  0.0639,  0.0073],\n",
      "        ...,\n",
      "        [-0.0865, -0.0662, -0.0785,  ..., -0.1055,  0.0153,  0.0494],\n",
      "        [ 0.0060, -0.0662, -0.0310,  ..., -0.1045,  0.0832, -0.0946],\n",
      "        [-0.0795, -0.0830, -0.0348,  ..., -0.1167,  0.1138, -0.0824]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 64\n",
      "\n",
      "Parameter containing:\n",
      "tensor([-0.1085, -0.1052, -0.0435,  0.0848,  0.1161,  0.0066, -0.0907, -0.0494,\n",
      "         0.0519, -0.0032,  0.1219, -0.1193,  0.0086, -0.0815, -0.0166, -0.0100,\n",
      "         0.0378,  0.0166, -0.0391,  0.1094,  0.0498, -0.1025,  0.0096, -0.1136,\n",
      "         0.0995, -0.0489,  0.0971,  0.0666, -0.0636, -0.0343, -0.0099, -0.0212,\n",
      "         0.0485,  0.0787, -0.0293, -0.1086,  0.0539, -0.1200,  0.0224, -0.0901,\n",
      "        -0.1135, -0.0600, -0.1033, -0.1131,  0.0564,  0.0795,  0.0405, -0.0063,\n",
      "        -0.0358, -0.0647,  0.0091,  0.0790,  0.0434, -0.1181, -0.1125,  0.0304,\n",
      "        -0.0694, -0.0657, -0.0656,  0.1103, -0.0120, -0.1236,  0.0133,  0.1030],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 64\n",
      "\n",
      "Parameter containing:\n",
      "tensor([ 0.0882, -0.0517,  0.0008,  0.0833, -0.1089,  0.0275, -0.0115, -0.0150,\n",
      "         0.0433, -0.0400,  0.0547,  0.0166,  0.0544, -0.0128, -0.0488, -0.0175,\n",
      "         0.1082, -0.0963,  0.0409,  0.0082, -0.1074,  0.0960, -0.0356, -0.0763,\n",
      "        -0.0036,  0.0803,  0.0905, -0.0887,  0.1220,  0.0988, -0.0604,  0.0532,\n",
      "        -0.0339,  0.0645, -0.0229,  0.0397,  0.0066,  0.0450,  0.0140,  0.0887,\n",
      "         0.0034,  0.0144, -0.1009,  0.0638, -0.0734,  0.0894,  0.1115, -0.0736,\n",
      "         0.1145,  0.1061, -0.0460,  0.1221, -0.0125,  0.0663, -0.0343, -0.0412,\n",
      "        -0.1227, -0.1229, -0.1131, -0.0583, -0.0256, -0.0053, -0.1018,  0.0318],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Total Sets of Parameters: 4\n",
      "\n",
      "******************************************************************************************\n",
      "Layer name: fc1 --- Linear(in_features=64, out_features=64, bias=True)\n",
      "\n",
      "ToT Params: 4,160\n",
      "\n",
      "Param length: 64\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.1061,  0.0963, -0.0910,  ..., -0.0046,  0.1072, -0.0855],\n",
      "        [ 0.0852, -0.0808,  0.0962,  ...,  0.0190,  0.0475, -0.1155],\n",
      "        [ 0.0123,  0.0086,  0.0865,  ...,  0.0710, -0.1209,  0.0773],\n",
      "        ...,\n",
      "        [-0.0837, -0.1017, -0.1013,  ..., -0.0426, -0.1217, -0.1227],\n",
      "        [-0.0432,  0.0568, -0.0864,  ..., -0.0488, -0.1243, -0.0474],\n",
      "        [-0.0819, -0.1222, -0.0883,  ...,  0.1077,  0.0160, -0.0887]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 64\n",
      "\n",
      "Parameter containing:\n",
      "tensor([-0.1008, -0.1234, -0.0600, -0.0869, -0.0855, -0.0990, -0.0613, -0.0202,\n",
      "        -0.1101,  0.0119,  0.0590,  0.0332,  0.0104,  0.0805, -0.1121, -0.0587,\n",
      "        -0.1022, -0.0348, -0.0003,  0.0893,  0.0842, -0.0363, -0.0874, -0.0212,\n",
      "        -0.0448,  0.0325, -0.0850, -0.0092,  0.0024,  0.0719, -0.1171,  0.0104,\n",
      "         0.0456,  0.0671,  0.0358, -0.0487, -0.0851, -0.0953, -0.0219, -0.0421,\n",
      "         0.1044,  0.0907, -0.0611, -0.0175, -0.0825,  0.0874, -0.0268,  0.0614,\n",
      "         0.1062, -0.1052,  0.1113,  0.0936, -0.0676,  0.1162, -0.1169, -0.1139,\n",
      "         0.0500,  0.1111,  0.0531,  0.1131, -0.0939, -0.0514,  0.0825, -0.0287],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Total Sets of Parameters: 2\n",
      "\n",
      "******************************************************************************************\n",
      "Layer name: fc2 --- Linear(in_features=64, out_features=1, bias=True)\n",
      "\n",
      "ToT Params: 65\n",
      "\n",
      "Param length: 1\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.0881, -0.0781,  0.0277,  0.0014,  0.1132, -0.0906, -0.1123,  0.1132,\n",
      "          0.0466,  0.0138, -0.1120, -0.1249, -0.1207,  0.0144,  0.1008,  0.0278,\n",
      "         -0.0078, -0.0414,  0.0733, -0.0554,  0.0223, -0.0854, -0.0077,  0.0505,\n",
      "          0.0495, -0.0853,  0.0490, -0.0545,  0.1221,  0.0284,  0.0358,  0.0239,\n",
      "          0.0061,  0.1086,  0.1112, -0.0853,  0.1172, -0.0017,  0.0246,  0.1058,\n",
      "         -0.0161, -0.0502,  0.0820, -0.0600, -0.0908, -0.1194, -0.0247,  0.0748,\n",
      "          0.0790,  0.1116, -0.0665, -0.0735,  0.0660,  0.1020,  0.0992, -0.0977,\n",
      "          0.0614,  0.0840,  0.0444, -0.0051,  0.0840,  0.0975,  0.0227,  0.0435]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 1\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0.0396], requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Total Sets of Parameters: 2\n",
      "\n",
      "******************************************************************************************\n",
      "Layer name: sigmoid --- Sigmoid()\n",
      "\n",
      "ToT Params: 0\n",
      "\n",
      "Total Sets of Parameters: 0\n",
      "\n",
      "******************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# check params initialised by the classifier\n",
    "check_params(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 27,649 trainable parameters\n",
      "The model has 28,647,349 parameters overall\n"
     ]
    }
   ],
   "source": [
    "# trainable / all params before training\n",
    "trainable_params, all_params = num_params(classifier)\n",
    "print('The model has {:,} trainable parameters'.format(trainable_params))\n",
    "print('The model has {:,} parameters overall'.format(all_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "The training loop is composed of two loops: an inner loop over minibatches in the dataset, and an outer loop, which repeats the inner loop a number of times. In the inner loop, losses are computed for each minibatch, and the optimizer is used to\n",
    "update the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "\n",
      "Epoch: 0 | Train Loss: 0.684 | Val Loss: 0.687 | Train Acc: 55.355 | Val Acc: 54.634\n",
      "(27649, 28647349)\n",
      "\n",
      "Epoch: 1 | Train Loss: 0.686 | Val Loss: 0.683 | Train Acc: 54.923 | Val Acc: 55.292\n",
      "(27649, 28647349)\n",
      "\n",
      "Epoch: 2 | Train Loss: 0.683 | Val Loss: 0.687 | Train Acc: 55.235 | Val Acc: 54.447\n",
      "(27649, 28647349)\n",
      "\n",
      "Epoch: 3 | Train Loss: 0.685 | Val Loss: 0.686 | Train Acc: 54.960 | Val Acc: 54.727\n",
      "(27649, 28647349)\n",
      "\n",
      "Epoch: 4 | Train Loss: 0.684 | Val Loss: 0.688 | Train Acc: 54.928 | Val Acc: 54.373\n",
      "(27649, 28647349)\n",
      "\n",
      "Epoch: 5 | Train Loss: 0.686 | Val Loss: 0.686 | Train Acc: 54.680 | Val Acc: 54.720\n",
      "(27649, 28647349)\n",
      "\n",
      "Epoch: 6 | Train Loss: 0.684 | Val Loss: 0.684 | Train Acc: 55.163 | Val Acc: 55.093\n",
      "(27649, 28647349)\n",
      "\n",
      "Epoch: 7 | Train Loss: 0.686 | Val Loss: 0.687 | Train Acc: 54.872 | Val Acc: 54.416\n",
      "(27649, 28647349)\n",
      "\n",
      "Epoch: 8 | Train Loss: 0.685 | Val Loss: 0.684 | Train Acc: 54.920 | Val Acc: 55.000\n",
      "(27649, 28647349)\n",
      "\n",
      "Epoch: 9 | Train Loss: 0.681 | Val Loss: 0.677 | Train Acc: 55.696 | Val Acc: 56.478\n",
      "(27649, 28647349)\n",
      "\n",
      "\n",
      "Training complete\n",
      "\n",
      "1755.5734412670135\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "n_epochs = 10\n",
    "n_epoch_freezed = 10\n",
    "\n",
    "print('Starting training', end='\\n\\n')\n",
    "\n",
    "\n",
    "# Enumerate epochs\n",
    "epoch = 0\n",
    "\n",
    "# For a certain number of epochs (defined by 'n_epoch_freezed'), the emebdding matrix is frozen, then it is unfrozen \n",
    "# i.e. the embeddings get trained (except for the padding vector which remains 0)\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch < n_epoch_freezed:   \n",
    "        pass   # keep the embedding layer frozen (i.e. classifier.emb.weight.requires_grad=False as set in section 8 above)\n",
    "    else: \n",
    "        classifier.emb.weight.requires_grad=True\n",
    "\n",
    "    # Training part\n",
    "    classifier.train()\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_acc = 0\n",
    "    \n",
    "    for i, (x_train, y_train) in enumerate(train_loader):\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation: compute the model output (i.e. predictions)\n",
    "        y_pred = classifier(x_in=x_train)\n",
    "\n",
    "        #print(x_train.requires_grad, y_train.requires_grad, y_pred.requires_grad)\n",
    "                                                                                                        \n",
    "        # Loss calculation\n",
    "        t_loss = loss_func(y_pred, y_train)\n",
    "        \n",
    "        # Accuracy\n",
    "        t_acc = binary_acc(y_pred, y_train)\n",
    "        \n",
    "        # Backward propagation: use loss to produce gradients\n",
    "        t_loss.backward()\n",
    "        \n",
    "        # Weight optimization: use optimizer to take gradient step and update parameters (w,b) \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += t_loss.item()\n",
    "        epoch_train_acc += t_acc.item()\n",
    "                                                                                                             \n",
    "   \n",
    "    # Evaluation part\n",
    "    classifier.eval() # .eval() tells PyTorch that we do not want to perform back-propagation during inference\n",
    "    \n",
    "    epoch_val_loss = 0\n",
    "    epoch_val_acc = 0\n",
    "    \n",
    "    #We use torch.no_grad() which reduces memory usage and speeds up computation.\n",
    "    with torch.no_grad():     #https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/3 : torch.no_grad() deals with the autograd engine and stops it from calculating the gradients, which is the recommended way of doing validation\n",
    "        for i, (x_val, y_val) in enumerate(val_loader):\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "        \n",
    "            # Forward propagation: compute the model output (i.e. predictions)\n",
    "            y_pred = classifier(x_in=x_val)     #tensors of probabilities\n",
    "        \n",
    "            # Loss calculation\n",
    "            v_loss = loss_func(y_pred, y_val)  \n",
    "            \n",
    "            # Accuracy\n",
    "            v_acc = binary_acc(y_pred, y_val)\n",
    "            \n",
    "            epoch_val_loss += v_loss.item()\n",
    "            epoch_val_acc += v_acc.item()\n",
    "            \n",
    "    print('Epoch: {} | Train Loss: {:.3f} | Val Loss: {:.3f} | Train Acc: {:.3f} | Val Acc: {:.3f}'.format(epoch,\n",
    "                                                                                epoch_train_loss/len(train_loader),\n",
    "                                                                                epoch_val_loss/len(val_loader),\n",
    "                                                                                epoch_train_acc/len(train_loader),\n",
    "                                                                                epoch_val_acc/len(val_loader)))\n",
    "    \n",
    "    print(num_params(classifier), end='\\n\\n')\n",
    "    \n",
    "epoch += 1\n",
    "    \n",
    "\n",
    "print()\n",
    "print('Training complete', end='\\n\\n')\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialise / Deserialise Embedding Parameters after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_after = params(classifier.emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise embedding params after training\n",
    "serialise('tensor', serialise_directory, 'params_after', params_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise embedding params after training\n",
    "params_after = deserialise('tensor', serialise_directory, 'params_after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check changes in params\n",
    "params_after == params_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gradients\n",
    "classifier.emb.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainable / all params after training\n",
    "trainable_params, all_params = num_params(classifier)\n",
    "print('The model has {:,} trainable parameters'.format(trainable_params))\n",
    "print('The model has {:,} parameters overall'.format(all_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on test data\n",
    "\n",
    "To evaluate the data on the held-out test set, the code is exactly the same as the validation loop in the training routine we saw in the previous step. \n",
    "\n",
    "The test set should be run as little as possible. Each time you run a trained model on the test set, make a new model decision (such as changing the size of the layers), and remeasure the new retrained model on the test set, you are biasing your\n",
    "modeling decisions toward the test data. In other words, if you repeat that process often enough, the test set will become meaningless as an accurate measure of truly held-out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()  \n",
    "\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "    \n",
    "with torch.no_grad():     #torch.no_grad() deals with the autograd engine and stops it from calculating the gradients, which is the recommended way of doing validation\n",
    "    for i, (x_test, y_test) in enumerate(test_loader):\n",
    "        x_test = x_test.to(device) \n",
    "        y_test = y_test.to(device)\n",
    "        \n",
    "        # Forward propagation: compute the model output (i.e. predictions)\n",
    "        y_pred = classifier(x_in=x_test)     #tensors of probabilities\n",
    "        \n",
    "        # Loss calculation\n",
    "        tst_loss = loss_func(y_pred, y_test)\n",
    "        \n",
    "        # Accuracy\n",
    "        tst_acc = binary_acc(y_pred, y_test)\n",
    "            \n",
    "        test_loss += tst_loss.item()\n",
    "        test_acc += tst_acc.item()\n",
    "            \n",
    "print('Test Loss: {:.3f} | Test Acc: {:.3f}'.format(test_loss/len(test_loader), test_acc/len(test_loader)))\n",
    "\n",
    "print()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\"Suspended my account without warning and my order got delayed. I called them two days in a row to be fobbed off \\\n",
    "          and to wait for an email. No email. No access to my online account. 3 amazon prime payments taken from my card \\\n",
    "          without my permission. Just got access back to claim my money back 12 weeks later and didn't even get an apology \\\n",
    "          from the online chat. Didn't even get my order either.\",\n",
    "          'Fantastic!!!!',\n",
    "          'The products they sent me always work fine but... I have Amazon prime and they sometimes deliver days late or \\\n",
    "          just leave the product on your doorstep out in the open easy to be stolen. Overall, my experience with Amazon is \\\n",
    "          fairly good',\n",
    "          'Safe place never used…doorstep deliveries. I am a long standing Amazon customer and high spending. I have had \\\n",
    "          a safe place named for deliveries for a long time. The odd driver hitters to open the door to the bin shed and \\\n",
    "          out the parcels in. The majority just leave it on the doorstep. With thefts high, I find this utterly ridiculous. \\\n",
    "          What is the point of a safe place if it is simply ignored and never used? I live in a house with a small front \\\n",
    "          garden - not a difficult to access apartment block or something. Amazon - please hold your drivers accountable \\\n",
    "          to the requests of customers. You offer and unique and great service but this lets you down.',\n",
    "          'My beautiful Jenuh killed by incompetence! She went to Dewey Veterinary Medical Center 3x! My MISTAKE!! I was \\\n",
    "          told she had nothing but allergies! She was filled with injections that DID NOTHING for her along with \\\n",
    "          medications that were ridiculously priced and again that did nothing!! 5 days after her last visit with Dewey \\\n",
    "          I’m in another veterinary office with her in critical condition and them saying she was on oxygen and had a very \\\n",
    "          bad infection! They’re telling me the rash on her body was NOT allergies, but was in fact signs of trouble from \\\n",
    "          infection!!! She was never treated just given more shots and new medications and here she was suffering inside! \\\n",
    "          (All I was told was many pets were coming in suffering from allergies) My 8 month beautiful blue and green-eyed \\\n",
    "          Husky died all because Dewey Vet did NOTHING for her!! She lost her life and I didn’t get to enjoy loving her \\\n",
    "          for many year’s because her symptoms were not taken seriously ! She didn’t have to die SHE WAS MURDERED by lack \\\n",
    "          of care!!!!! As a pet mom you take your babies into veterinarians offices in hopes they will properly diagnose \\\n",
    "          and care for your kids basicaly!!! Jenuh was not cared for! This vet told me her impacted teeth (Adult teeth \\\n",
    "          came in but baby teeth didn’t fall out) wasn’t urgent she could get them out when they spayed her too which I \\\n",
    "          said I was told by other vet that I should wait between 1-1.5 years old which he blew off and said NOT TRUE! \\\n",
    "          (I read about the urgency to pull teeth online yet this was also blown off as nothing) His old school veterinary \\\n",
    "          services may actually be OUTDATED garbage! Maybe it’s time to retire!!!!',\n",
    "          \"It's a good search engine and info that shows up is mostly helpful, but lately the ads are flooding the face \\\n",
    "          page which is not always the best sites for what you are looking for.\",\n",
    "          \"I bought google play credit.I have the virtual card.But they blocked all my account transaction, when i \\\n",
    "          contacted them,they told me that they would solve it,Instead, they locked it.Even after submitting all proper \\\n",
    "          documents, they are refusing to reactive.Such bad experience\", \n",
    "          'I have had a long term rental association (as a renter) with my Property Manager Kiera Hannaford. I would like \\\n",
    "          to acknowledge her professional, friendly and thorough knowledge of all things related to rental property \\\n",
    "          management. Kiera always does her very best to help and goes to a great deal of trouble to ensure a positive \\\n",
    "          outcome with all queries and requests. Her honesty is such a valuable asset when there are challenging times. \\\n",
    "          As a renter I have always been able to rely on Kiera for help. I wish her the very best for a very bright \\\n",
    "          future and would recommend her if you ever need an excellent team member in the real estate industry. ',\n",
    "          \"It’s short from being the best because I’m concerned over some huge privacy details that may get leaked to the \\\n",
    "          greater public but otherwise it’s a good engine.\",\n",
    "          \"Always had good experience with google before but lately there's been a lot of ad popping up and not \\\n",
    "          necessarily nice sites just sites that paid money to google for google to put their sites on first page. \\\n",
    "          But google is a nice service with a lot of functions although I wonder how much personal information they \\\n",
    "          store of you in the database.\",\n",
    "          \"Kindred Healthcare physical therapists (PT) and occupational therapists (OT) in Okaloosa County, Florida, \\\n",
    "          helped me regain use of my right arm and increase my stamina and endurance despite COPD problems. During my \\\n",
    "          in-patient and outpatient therapy, they were always professional and polite while encouraging me to challenge \\\n",
    "          myself. PT and OT helped me regain independence, self-reliance, and dignity. I am grateful.\",\n",
    "          \"Had a problem. Resolved immediately. Great customer service!!!\",\n",
    "          \"had 3 hour drive ahead of me and Adam the fitter could see my distress at having to wait he jacked up the truck \\\n",
    "          in a flash and had me on my way, top sevices from Adam and the kwikfit team.\",\n",
    "          \"Online spas- Absolutely lovely guys and excellent customer service. The guys showed me how to set up and did \\\n",
    "          all the hard work for me. Nothing was any problem :). Lovely the hot tub and would highly recommend to \\\n",
    "          friends ! Thanks again!\",\n",
    "          \"Mr. Handyman is a great service. The staff isprofessional and all of the jobs get done. I have used Mr. \\\n",
    "          Handyman several times and I couldn’t be happier.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference_pipeline.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences = [inference_pipeline.run(r) for r in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,inf in enumerate(inferences):\n",
    "    print('{}. {} --- probability value {:.2%}'.format(i+1, inf[0], inf[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "# \"Suspended my account without warning and my order got delayed. I called them two days in a row to be fobbed off \\\n",
    "# and to wait for an email. No email. No access to my online account. 3 amazon prime payments taken from my card \\\n",
    "# without my permission. Just got access back to claim my money back 12 weeks later and didn't even get an apology \\\n",
    "# from the online chat. Didn't even get my order either.\" \n",
    "# 1. Negative --- probability value 3.39%    --- CORRECT\n",
    "\n",
    "# 2.\n",
    "# 'Fantastic!!!!'  \n",
    "# 2. Positive --- probability value 98.76%   --- CORRECT\n",
    "\n",
    "# 3.\n",
    "# 'The products they sent me always work fine but... I have Amazon prime and they sometimes deliver days late or \\\n",
    "# just leave the product on your doorstep out in the open easy to be stolen. Overall, my experience with Amazon is \\\n",
    "# fairly good' \n",
    "# 3. Positive --- probability value 66.64%   --- CORRECT\n",
    "\n",
    "# 4.\n",
    "# 'Safe place never used…doorstep deliveries. I am a long standing Amazon customer and high spending. I have had \\\n",
    "# a safe place named for deliveries for a long time. The odd driver hitters to open the door to the bin shed and \\\n",
    "# out the parcels in. The majority just leave it on the doorstep. With thefts high, I find this utterly ridiculous. \\\n",
    "# What is the point of a safe place if it is simply ignored and never used? I live in a house with a small front \\\n",
    "# garden - not a difficult to access apartment block or something. Amazon - please hold your drivers accountable \\\n",
    "# to the requests of customers. You offer and unique and great service but this lets you down.'\n",
    "# 4. Negative --- probability value 11.10%    --- CORRECT \n",
    "\n",
    "# 5.\n",
    "# 'My beautiful Jenuh killed by incompetence! She went to Dewey Veterinary Medical Center 3x! My MISTAKE!! I was \\\n",
    "# told she had nothing but allergies! She was filled with injections that DID NOTHING for her along with \\\n",
    "# medications that were ridiculously priced and again that did nothing!! 5 days after her last visit with Dewey \\\n",
    "# I’m in another veterinary office with her in critical condition and them saying she was on oxygen and had a very \\\n",
    "# bad infection! They’re telling me the rash on her body was NOT allergies, but was in fact signs of trouble from \\\n",
    "# infection!!! She was never treated just given more shots and new medications and here she was suffering inside! \\\n",
    "# (All I was told was many pets were coming in suffering from allergies) My 8 month beautiful blue and green-eyed \\\n",
    "# Husky died all because Dewey Vet did NOTHING for her!! She lost her life and I didn’t get to enjoy loving her \\\n",
    "# for many year’s because her symptoms were not taken seriously ! She didn’t have to die SHE WAS MURDERED by lack \\\n",
    "# of care!!!!! As a pet mom you take your babies into veterinarians offices in hopes they will properly diagnose \\\n",
    "# and care for your kids basicaly!!! Jenuh was not cared for! This vet told me her impacted teeth (Adult teeth \\\n",
    "# came in but baby teeth didn’t fall out) wasn’t urgent she could get them out when they spayed her too which I \\\n",
    "# said I was told by other vet that I should wait between 1-1.5 years old which he blew off and said NOT TRUE! \\\n",
    "# (I read about the urgency to pull teeth online yet this was also blown off as nothing) His old school veterinary \\\n",
    "# services may actually be OUTDATED garbage! Maybe it’s time to retire!!!!'\n",
    "# 5. Negative --- probability value 4.70%  --- CORRECT\n",
    "\n",
    "# 6.\n",
    "# \"It's a good search engine and info that shows up is mostly helpful, but lately the ads are flooding the face \\\n",
    "# page which is not always the best sites for what you are looking for.\"\n",
    "# 6. Positive --- probability value 93.89%  --- CORRECT \n",
    "\n",
    "#7.\n",
    "# \"I bought google play credit.I have the virtual card.But they blocked all my account transaction, when i \\\n",
    "# contacted them,they told me that they would solve it,Instead, they locked it.Even after submitting all proper \\\n",
    "# documents, they are refusing to reactive.Such bad experience\"\n",
    "# 7. Negative --- probability value 0.35%   --- CORRECT\n",
    "\n",
    "# 8.\n",
    "# 'I have had a long term rental association (as a renter) with my Property Manager Kiera Hannaford. I would like \\\n",
    "# to acknowledge her professional, friendly and thorough knowledge of all things related to rental property \\\n",
    "# management. Kiera always does her very best to help and goes to a great deal of trouble to ensure a positive \\\n",
    "# outcome with all queries and requests. Her honesty is such a valuable asset when there are challenging times. \\\n",
    "# As a renter I have always been able to rely on Kiera for help. I wish her the very best for a very bright \\\n",
    "# future and would recommend her if you ever need an excellent team member in the real estate industry. '\n",
    "# 8. Positive --- probability value 97.86%  --- CORRECT\n",
    "\n",
    "# 9.\n",
    "# \"It’s short from being the best because I’m concerned over some huge privacy details that may get leaked to the \\\n",
    "# greater public but otherwise it’s a good engine.\"\n",
    "# 9. Positive --- probability value 93.98% --- CORRECT\n",
    "\n",
    "# 10.\n",
    "# \"Always had good experience with google before but lately there's been a lot of ad popping up and not \\\n",
    "# necessarily nice sites just sites that paid money to google for google to put their sites on first page. \\\n",
    "# But google is a nice service with a lot of functions although I wonder how much personal information they \\\n",
    "# store of you in the database.\"\n",
    "# 10. Positive --- probability value 79.14%   --- CORRECT\n",
    "\n",
    "# 11.\n",
    "# \"Kindred Healthcare physical therapists (PT) and occupational therapists (OT) in Okaloosa County, Florida, \\\n",
    "# helped me regain use of my right arm and increase my stamina and endurance despite COPD problems. During my \\\n",
    "# in-patient and outpatient therapy, they were always professional and polite while encouraging me to challenge \\\n",
    "# myself. PT and OT helped me regain independence, self-reliance, and dignity. I am grateful.\"\n",
    "# 11. Positive --- probability value 94.97%   --- CORRECT\n",
    "\n",
    "# 12.\n",
    "# \"Had a problem. Resolved immediately. Great customer service!!!\"\n",
    "# 12. Positive --- probability value 79.75%   --- CORRECT \n",
    "\n",
    "# 13.\n",
    "# \"had 3 hour drive ahead of me and Adam the fitter could see my distress at having to wait he jacked up the truck \\\n",
    "# in a flash and had me on my way, top sevices from Adam and the kwikfit team.\"\n",
    "# 13. Positive --- probability value 64.67% --- CORRECT\n",
    "\n",
    "# 14.\n",
    "# \"Online spas- Absolutely lovely guys and excellent customer service. The guys showed me how to set up and did \\\n",
    "# all the hard work for me. Nothing was any problem :). Lovely the hot tub and would highly recommend to \\\n",
    "# friends ! Thanks again!\"\n",
    "# 14. Positive --- probability value 99.78%  --- CORRECT\n",
    "\n",
    "# 15.\n",
    "# \"Mr. Handyman is a great service. The staff isprofessional and all of the jobs get done. I have used Mr. \\\n",
    "# Handyman several times and I couldn’t be happier.\"\n",
    "# 15. Positive --- probability value 75.25% --- CORRECT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer",
   "language": "python",
   "name": "summer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
