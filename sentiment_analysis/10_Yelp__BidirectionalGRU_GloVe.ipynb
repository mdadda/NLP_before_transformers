{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-directional GRU & Glove Embeddings\n",
    "\n",
    "Sequence prediction tasks require us to label each item of a sequence. Such tasks are common in natural language processing. Some examples include language modeling in which we predict the next word given a sequence of words at each step;\n",
    "part-of-speech tagging, in which we predict the grammatical part of speech for each word; named entity recognition, in which we predict whether each word is part of a named entity, such as Person, Location, Product, or Organization;\n",
    "and so on. \n",
    "\n",
    "Sometimes, in NLP literature, the sequence prediction tasks are also referred to as sequence labeling.\n",
    "\n",
    "Although in theory we can use the **Vanilla/Elman recurrent neural networks for sequence prediction tasks, they fail to capture long-range dependencies well and perform poorly in practice**. \n",
    "\n",
    "Even though the vanilla/Elman RNN is well suited for modeling sequences, it has two issues that make it unsuitable for many tasks: the **inability to retain information for long-range predictions, and gradient stability**.\n",
    "Recall that at their core, RNNs are computing a hidden state vector at each time step using the hidden state vector of the previous time step and an input vector at the current time step. It is this core computation that makes the RNN so powerful, but it also creates drastic numerical issues.\n",
    "\n",
    "1. The first issue with Elman RNNs is the **difficulty in retaining long-range information**. At each time step the RNN simply updates the hidden state vector regardless of whether it made sense. As a consequence, the RNN has no control over which values are retained and which are discarded in the hidden state—that is entirely determined by the input. Intuitively, that doesn’t make sense. What is desired is some way for the RNN to decide if the update is optional, or if the update happens, by how much and what parts of the state vector, and so on.\n",
    "\n",
    "2. The second issue with Elman RNNs is their tendency to cause **gradients to spiral out of control to zero or to infinity**. Unstable gradients that can spiral out of control are called either vanishing gradients or exploding gradients depending\n",
    "on the direction in which the absolute values of the gradients are shrinking/growing. A really large absolute value of the gradient or a really small (less than 1) value can make the optimization procedure unstable. There are solutions to deal with these gradient problems in vanilla RNNs, such as the use of rectified linear units (ReLUs), gradient clipping, and careful\n",
    "initialization. But none of the proposed solutions work as reliably as the technique called **gating**.\n",
    "\n",
    "## Gating as a Solution to a Vanilla RNN’s Challenges\n",
    "\n",
    "To intuitively understand gating, suppose that you were adding two quantities, a and b, but you wanted to control how much of b gets into the sum.\n",
    "Mathematically, you can rewrite the sum a + b as:\n",
    "\n",
    "   **a + λb**\n",
    "\n",
    "where λ is a value between 0 and 1. If λ = 0, there is no contribution from b and if λ = 1, b contributes fully. Looking at it this way, you can interpret that λ acts as a “switch” or a “gate” in controlling the amount of b that gets into the sum.\n",
    "\n",
    "This is the intuition behind the gating mechanism. \n",
    "\n",
    "Now let’s revisit the Elman RNN and see how gating can be incorporated into vanilla RNNs to make conditional updates. If the previous hidden state was h_t-1 and the current input is x_t, the recurrent update in the Elman RNN would look something like:\n",
    "\n",
    "  **h_t = h_t-1 + F(h_t-1, x_t)**\n",
    "\n",
    "where F is the recurrent computation of the RNN. Obviously, **this is an unconditioned sum** and has the evils described earlier. \n",
    "\n",
    "Now imagine if, instead of a constant, the λ in the previous example was a function of the previous hidden state vector h_t-1 and the current input x_t, and still produced the desired gating behavior; that is, a value between 0 and 1. With this gating function, our RNN update equation would appear as follows:\n",
    "\n",
    "  **h_t = h_t-1 + λ(h_t-1, x_t) F(h_t-1, x_t)**\n",
    "  \n",
    "Now it becomes clear that the function λ controls how much of the current input gets to update the state h_t-1. Further, the function λ is context-dependent. This is the basic intuition behind all gated networks. The function λ is usually a sigmoid\n",
    "function, which we know to produce a value between 0 and 1.\n",
    "\n",
    "\n",
    "In the case of the **long short-term memory network (LSTM)** this basic intuition is extended carefully to incorporate not\n",
    "only conditional updates, but also intentional forgetting of the values in the previous hidden state h_t-1. This “forgetting” happens by multiplying the previous hidden state value h_t-1 with another function, μ, that also produces values\n",
    "between 0 and 1 and depends on the current input:\n",
    "\n",
    "  **h_t = μ(h_t-1, x_t) h_t-1 + λ(h_t-1, x_t) F(h_t-1, x_t)**\n",
    "  \n",
    "The LSTM is only one of the many gated variants of the RNN. Another variant that’s becoming increasingly popular is the **gated recurrent unit (GRU)**. Fortunately, in PyTorch, you can simply replace the nn.RNN or nn.RNNCell with nn.LSTM or nn.LSTMCell with no other code change to switch to an LSTM (mutatis mutandis for GRU)!\n",
    "\n",
    "The gating mechanism is an effective solution for Vanilla RNNs' problems seen earlier. It not only makes the updates controlled, but also keeps the gradient issues under check and makes training relatively easier. \n",
    "\n",
    "\n",
    "**Word Vectors** are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses. Here, you will explore word vectors derived via **GloVe**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Tricks for Training Sequence Models\n",
    "\n",
    "Sequence models can be challenging to train, and many issues crop up in the process. \n",
    "\n",
    "Here, we summarize a few tips and tricks that we have found useful in our work, and that others have reported in the literature.\n",
    "\n",
    "**1. When possible, use the gated variants**\n",
    "Gated architectures simplify training by addressing many of the numerical stability issues of nongated variants.\n",
    "\n",
    "**2. When possible, prefer GRUs over LSTMs**\n",
    "GRUs provide almost comparable performance to LSTMs and use far fewer parameters and compute resources. Fortunately, from the point of view of PyTorch, using a GRU rather than an LSTM simply requires using a different Module class.\n",
    "\n",
    "**3. Use Adam as your optimizer**\n",
    "We use only Adam as the optimizer, for good reason: it is reliable and typically converges faster than the alternatives.\n",
    "This is especially true for sequence models. If for some reason your models are not converging with Adam, switching to stochastic gradient descent might help.\n",
    "\n",
    "**4. Gradient clipping**\n",
    "If you notice numerical errors, instrument your code to plot the values of the gradients during training. After you know the range, clip any outliers. This will ensure smoother training. In PyTorch there is a helpful utility, **clip_grad_norm()**, to do this for you. In general, you should develop a habit of clipping gradients.\n",
    "\n",
    "**5. Early stopping**\n",
    "With sequence models, it is easy to overfit. We recommend that you stop the training procedure early, when the evaluation error, as measured on a development set, starts going up.\n",
    "\n",
    "\n",
    "### Applying gradient clipping in PyTorch\n",
    "\n",
    "In the training loop, after loss.backward():\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                 # to create 'serialised' directory\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json                               # for serialising dictionaries\n",
    "import pickle                             # for serialising numpy arrays\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from collections import Counter           # this is for stopwords removal\n",
    "from nltk.corpus import stopwords         # this is for stopwords removal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy                              # for tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_row', None)              # show all rows of a dataframe\n",
    "pd.set_option('display.max_column', None)           # show all columns of a dataframe\n",
    "pd.set_option('display.max_colwidth', None)         # show the full width of columns\n",
    "pd.set_option('precision', 2)                       # round to 2 decimal points\n",
    "pd.options.display.float_format = '{:,.2f}'.format  # comma separators and two decimal points: 4756.7890 => 4,756.79 and 4656 => 4,656.00 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOK = spacy.load('en_core_web_sm')                                         # for tokenisation\n",
    "stop_words = stopwords.words('english')                                    # for stopwords\n",
    "stopwords_dict = Counter(stop_words)                                       # for stopwords\n",
    "max_len = 100                                                              # for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mari\\miniconda3\\envs\\summer\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')      # pytorch cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seeding(seed_value=9):\n",
    "    random.seed(seed_value)                            # python \n",
    "    np.random.seed(seed_value)                         # numpy - global seeding. Sklearn uses this internally therefore there is no need to set a random seed when using Sklearn \n",
    "    torch.manual_seed(seed_value)                      # pytorch cpu\n",
    "    if device=='cuda': \n",
    "        torch.cuda.manual_seed_all(seed_value)         # pytorch gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filepath, filename, encoding='utf-8'):\n",
    "    full_path = '{}\\{}'.format(filepath, filename)\n",
    "    return pd.read_csv(full_path, encoding=encoding, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenise(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords_dict])  # remove stopwords\n",
    "    \n",
    "    return [token.text for token in TOK.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Serialisation / Deserialisation\n",
    "\n",
    "https://www.tutorialspoint.com/object_oriented_python/object_oriented_python_serialization.htm\n",
    "\n",
    "In the context of data storage, serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted and reconstructed later.\n",
    "\n",
    "In serialization, an object is transformed into a format that can be stored, so as to be able to deserialize it later and recreate the original object from the serialized format.\n",
    "\n",
    "We can do this with Pickle, JSON, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialise(obj_type, filepath, filename, obj):\n",
    "    full_path = '{}\\{}'.format(filepath, filename)\n",
    "    if obj_type == 'python':\n",
    "        with open(full_path, 'w') as f:\n",
    "            json.dump(obj, f)\n",
    "            \n",
    "    if obj_type in ['numpy', 'pandas', 'tensor']:\n",
    "        with open(full_path, 'wb') as f:\n",
    "            pickle.dump(obj, f)                                         # protocol 0 is printable ASCII\n",
    "        \n",
    "        \n",
    "\n",
    "def deserialise(obj_type, filepath, filename):\n",
    "    full_path = '{}\\{}'.format(filepath, filename)\n",
    "    if obj_type == 'python':\n",
    "        with open(full_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "            \n",
    "    if obj_type in ['numpy', 'pandas', 'tensor']:\n",
    "        with open(full_path, 'rb') as f:\n",
    "            return pickle.load(f)                                        # protocol 0 is printable ASCII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vocabulary from the Traing Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_dict(series):\n",
    "    tokens = series.explode()\n",
    "    tokens = tokens.tolist()\n",
    "    tokens = Counter(tokens)                                    # frequency distribution\n",
    "\n",
    "    num_words_before = len(tokens.keys())\n",
    "    \n",
    "    #To avoid 'RuntimeError: dictionary changed size during iteration' error, we need to make a .copy() of the dictionary.\n",
    "    #This way we iterate over the original dictionary keys and delete elements on the fly.\n",
    "    for k,v in tokens.copy().items():\n",
    "        if v < 2:\n",
    "            del tokens[k]\n",
    "\n",
    "    tokens = {word: i for i,word in enumerate(tokens.keys())}    # word2index dictionary\n",
    "\n",
    "    tokens['<UNK>'] = len(tokens)\n",
    "    tokens['<MASK>'] = len(tokens)\n",
    "    tokens['<END>'] = len(tokens)\n",
    "    tokens['<START>'] = len(tokens)\n",
    "    tokens[''] = len(tokens)\n",
    "    \n",
    "    num_words_after = len(tokens.keys())\n",
    "    \n",
    "    return num_words_before, num_words_after, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. All Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_from_file(glove_filepath):\n",
    "    \n",
    "    embeddings_dict = {} \n",
    "    \n",
    "    with open(glove_filepath, mode='r', encoding=\"utf-8\") as f:\n",
    "        for index,line in enumerate(f):\n",
    "            line_split = line.split()\n",
    "            word = line_split[0]\n",
    "            embeddings_dict[word] = np.array(line_split[1:], 'float32')\n",
    "    \n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_matrix(embeddings_dict, tokens):  \n",
    "    \n",
    "    embedding_size = len(next(iter(embeddings_dict.values())))     # length of each word embedding (e.g.300 dimensions)\n",
    "    \n",
    "    embeddings_matrix = np.zeros((len(tokens)+1, embedding_size))  # len(tokens) is the length of the vocabulary (i.e. unique words in the corpus)\n",
    "                                                                   # the '+1' after len(tokens) is necessary because we want the first embedding (i.e. at index 0) to be a zero vector; this will be used for the paddings (we want all the '0' paddings to be paired with the zero vector)\n",
    "    not_in_glove = []\n",
    "    for token,i in tokens.items():\n",
    "        if token in embeddings_dict:\n",
    "            embeddings_matrix[i+1, :] = embeddings_dict[token]     # '+1' :see comment above\n",
    "        else:\n",
    "            not_in_glove.append(token)\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)             # if word is not in Glove emebeddings, creates a new vector with random numbers drawn from pytorch xavier_uniform distribution\n",
    "            embeddings_matrix[i+1, :] = embedding_i                # '+1' :see comment above\n",
    "\n",
    "    return not_in_glove, embeddings_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(input_sequence):\n",
    "    encoded = np.zeros(max_len, dtype=int)\n",
    "    encoded_lst = np.array([tokens.get(word, tokens['<UNK>']) for word in input_sequence])\n",
    "    length = min(max_len, len(encoded_lst))\n",
    "    encoded[:length] = encoded_lst[:length]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Bidirectional GRU Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRUClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_layers,                 # embedding_size = 300, num_embeddings=95465\n",
    "                 hidden_size, output_dim, dropout_p, batch_first=True, \n",
    "                 bidirectional=True, pretrained_embeddings=None, padding_idx=0):   # pretrained_embeddings = embedding_matrix\n",
    "                                                                                   # padding_idx=0 makes sure that the padding vector (which in our embedding matrix is at index 0) doesn't get updated during training when Freeze=False\n",
    "        super(BiGRUClassifier, self).__init__()                      \n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                    num_embeddings=num_embeddings,\n",
    "                                    padding_idx=padding_idx)        \n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            # freeze=False : the tensor does not get updated in the learning process. Equivalent to self.emb.weight.requires_grad = False\n",
    "            self.emb = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True, padding_idx=padding_idx)  \n",
    "            \n",
    "#             self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "#                                     num_embeddings=num_embeddings,\n",
    "#                                     padding_idx=padding_idx,\n",
    "#                                     _weight=pretrained_embeddings)\n",
    "            \n",
    "            \n",
    "            # nn.Embedding is a model parameter layer, which is by default trainable.\n",
    "            # If you want to fine-tune word vectors during training, these word vectors are treated as model parameters \n",
    "            # and are updated by backpropagation. You can also make it untrainable by freezing its gradient \n",
    "            # (False ==> freezes the backprop) \n",
    "#             self.emb.weight.requires_grad=False    \n",
    "        \n",
    "    \n",
    "        self.gru = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "                            dropout=dropout_p, bidirectional=bidirectional, bias=True, batch_first=batch_first) #initialise GRU model\n",
    "        \n",
    "        self.D = 1\n",
    "        if bidirectional==True:\n",
    "            self.D = 2\n",
    "            \n",
    "        self._dropout_p = dropout_p      \n",
    "        self.fc1 = nn.Linear(in_features=self.D*hidden_size, out_features=self.D*hidden_size)\n",
    "        self.fc2 = nn.Linear(in_features=self.D*hidden_size, out_features=output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "    # Note that we don't use the Sigmoid activation in our final layer during training because we use the \n",
    "    # nn.BCEWithLogitsLoss() loss function which automatically applies the the Sigmoid activation.\n",
    "        x_embedded = self.emb(x_in.long())                # x_in is the encoded review e.g. [3386  603 1112    0    0    0    0    0    0    0]. It comes from the train_loader\n",
    "        batch_size = x_embedded.shape[0]\n",
    "        \n",
    "        h0 = torch.randn(self.D*num_layers, batch_size, hidden_size)\n",
    "        output, hn = self.gru(x_embedded, h0)\n",
    "        output = output[:, -1, :]\n",
    "               \n",
    "        hidden_layer = F.relu(F.dropout(self.fc1(output), p=self._dropout_p))\n",
    "        output_layer = self.fc2(hidden_layer).squeeze(1)   # squeeze(1) to make the prediction the same shape as the target i.e. as a scaler not a 2d-vector\n",
    "\n",
    "        if apply_sigmoid: \n",
    "            output_layer = sef.sigmoid(output_layer)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.  Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_hat, y):\n",
    "    y_hat_label = torch.round(torch.sigmoid(y_hat))\n",
    "\n",
    "    correct_predictions_sum = (y_hat_label == y).sum().float()\n",
    "    acc = correct_predictions_sum/y.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params(full_embedding_layer_name):\n",
    "    for param in full_embedding_layer_name.parameters():\n",
    "        return param\n",
    "\n",
    "\n",
    "def check_params(classifier_name):\n",
    "    for name, child in classifier_name.named_children():\n",
    "        print('Layer name: {} --- {}'.format(name, child), end='\\n\\n')            \n",
    "        print('ToT Params: {:,}'. format(sum(p.numel() for p in child.parameters())), end='\\n\\n') \n",
    "    \n",
    "        count = 0\n",
    "        for param in child.parameters():\n",
    "            print('Param length: {:,}'.format(len(param)), end='\\n\\n')\n",
    "            print(param, end='\\n\\n')\n",
    "            print('Are parameters being updated during backprop? {}'.format(param.requires_grad), end='\\n\\n')\n",
    "            count += 1\n",
    "\n",
    "        print('Total Sets of Parameters: {}'.format(count), end='\\n\\n')\n",
    "        print('*' * 90)\n",
    "    \n",
    "\n",
    "def num_params(classifier_name):\n",
    "    \n",
    "    # PyTorch torch.numel() method returns the total number of elements in the input tensor\n",
    "    trainable_parameters = sum(param.numel() for param in classifier_name.parameters() if param.requires_grad)  \n",
    "    all_parameters = sum(param.numel() for param in classifier_name.parameters())  \n",
    "    \n",
    "    return trainable_parameters, all_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Inference on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        idx = 0\n",
    "        if depends_on:\n",
    "            idx = self.tasks.index(depends_on) + 1\n",
    "        def inner(f):\n",
    "            self.tasks.insert(idx, f)\n",
    "            return f\n",
    "        return inner\n",
    "    \n",
    "    # Add the run() method which should take in an 'input_' argument\n",
    "    def run(self, input_):\n",
    "        output = input_\n",
    "        # Iterate through the self.tasks property, and call each function with the previous output\n",
    "        for task in self.tasks:\n",
    "            output = task(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline.tasks = [clean_tokenise, encoding]   #calls the functions created at the top of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new functions to the pipeline\n",
    "\n",
    "@inference_pipeline.task(depends_on=encoding)\n",
    "def convert_to_tensor(text):\n",
    "    return torch.Tensor(text)\n",
    "\n",
    "@inference_pipeline.task(depends_on=convert_to_tensor)\n",
    "def infer(text):\n",
    "    # Disable grad\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction = classifier(text.unsqueeze(0))\n",
    "        probability_value = classifier.sigmoid(prediction).item()\n",
    "        \n",
    "        if probability_value < 0.5:\n",
    "            prediction_label = 'Negative'\n",
    "        else:\n",
    "            prediction_label = 'Positive'\n",
    "            \n",
    "    return prediction_label, probability_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = r'C:\\Users\\Mari\\Desktop\\MACHINE_LEARNING\\NLP_Stanford_University\\BOOK\\YELP\\dataset'\n",
    "train_set_filename = 'raw_train.csv'\n",
    "test_set_filename = 'raw_test.csv'\n",
    "\n",
    "serialise_directory = r'C:\\Users\\Mari\\Desktop\\MACHINE_LEARNING\\NLP_Stanford_University\\BOOK\\YELP\\serialised'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'serialised' directory if it doesn't exist\n",
    "\n",
    "try:\n",
    "    os.makedirs(serialise_directory)\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets\n",
    "train_original = read_csv(dataset_directory, train_set_filename)\n",
    "test = read_csv(dataset_directory, test_set_filename)\n",
    "\n",
    "# Add column names\n",
    "train_original.columns = ['target', 'review']\n",
    "test.columns = ['target', 'review']\n",
    "\n",
    "# Split Targets from Features\n",
    "X_train_original = train_original['review']\n",
    "y_train_original = train_original['target']\n",
    "X_test = test['review']\n",
    "y_test = test['target']\n",
    "\n",
    "# Re-label Target: In PyTorch labels need to start at 0\n",
    "# 1 ==> 0 (these are negative reviews)\n",
    "# 2 ==> 1 (these are positive reviews)\n",
    "y_train_original = y_train_original - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "# Split 'train_original' in 'train' and 'val' \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_original, y_train_original, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(X_train), X_train.shape)\n",
    "# print(type(X_val), X_val.shape)\n",
    "# print(type(X_test), X_test.shape)\n",
    "# print(type(y_train), y_train.shape)\n",
    "# print(type(y_val), y_val.shape)\n",
    "# print(type(y_test), y_test.shape, end='\\n\\n')\n",
    "# print(X_train.head(2), end='\\n\\n')\n",
    "# print(y_train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning And Tokenization\n",
    "\n",
    "In addition to creating a subset that has three partitions for training, validation, and testing, we also minimally clean the data by adding whitespace around punctuation symbols and removing extraneous symbols that aren’t punctuation for all the splits.\n",
    "\n",
    "\n",
    "1. **apply** works on a row / column basis of a DataFrame \n",
    "2. **applymap** works element-wise on a DataFrame\n",
    "3. **map** works element-wise on a Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.map(clean_tokenise)\n",
    "X_val = X_val.map(clean_tokenise)\n",
    "X_test = X_test.map(clean_tokenise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From glove txt, create a dictionary of all glove embeddings where KEY is a WORD, and VALUE is a NUMPY ARRAY:\n",
    "glove_embeddings = load_glove_from_file('C:/GloVe/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary From Training Corpus\n",
    "\n",
    "The embedding matrix (see later) is created only from the training dataset.\n",
    "\n",
    "The training dataset should be sufficiently rich/representative enough to cover all data you expect to see in the future.\n",
    "\n",
    "New data must have the same integer encoding as the training data prior to being mapped onto the embedding when making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DICT of the unique words in the training set\n",
    "num_words_before, num_words_after, tokens = tokens_dict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding is created from the training dataset.\n",
    "\n",
    "It should be sufficiently rich/representative enough to cover all data you expect to in the future.\n",
    "\n",
    "New data must have the same integer encoding as the training data prior to being mapped onto the embedding when making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_glove, embedding_matrix = make_embedding_matrix(glove_embeddings, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Training Dataset\n",
    "\n",
    "We need to convert our text into a numerical form that can be fed to our model as input.\n",
    "\n",
    "1. We have create a vocabulary (see section '10. Vocabulary') where each key is a unique word from the training corpus, and each value is the index of that word in the 'tokens' dictionary.\n",
    "2. Choose the maximum length of any review.\n",
    "3. Encode each list of tokens by replacing each word with its index from the 'tokens' dictionary.\n",
    "\n",
    "Note: **mean_len** (see below) is the mean of tokens length in the training set. We set the max length of the encoded reviews equal to the mean_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = X_train.map(lambda input_lst: encoding(input_lst, tokens=tokens, max_len=max_len))   \n",
    "X_val_encoded = X_val.map(lambda input_lst: encoding(input_lst, tokens=tokens, max_len=max_len))\n",
    "X_test_encoded = X_test.map(lambda input_lst: encoding(input_lst, tokens=tokens, max_len=max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pd.Series to PyTorch Tensors\n",
    "# NB: set the values in X_train, X_val and X_test as a list of arrays (as opposed to array of arrays) --- see above\n",
    "\n",
    "x_train_tensor = torch.Tensor(list(X_train_encoded.values))\n",
    "x_val_tensor = torch.Tensor(list(X_val_encoded.values))\n",
    "x_test_tensor = torch.Tensor(list(X_test_encoded.values))\n",
    "y_train_tensor = torch.Tensor(list(y_train.values))\n",
    "y_val_tensor = torch.Tensor(list(y_val.values))\n",
    "y_test_tensor = torch.Tensor(list(y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full dataset (like a DataFrame in Pandas) from the two tensors\n",
    "train_dataset =  TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise datasets after tokenisation\n",
    "serialise('pandas', serialise_directory, 'X_train', X_train)\n",
    "serialise('pandas', serialise_directory, 'X_val', X_val)\n",
    "serialise('pandas', serialise_directory, 'X_test', X_test)\n",
    "serialise('pandas', serialise_directory, 'y_train', y_train)\n",
    "serialise('pandas', serialise_directory, 'y_val', y_val)\n",
    "serialise('pandas', serialise_directory, 'y_test', y_test)\n",
    "serialise('pandas', serialise_directory, 'mean_len', mean_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise glove embeddings (althought 'glove_embeddings' is a dictionary, its values are numpy arrays therefore we need to choose 'numpy')\n",
    "serialise('numpy', serialise_directory, 'glove_embeddings', glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise tokens\n",
    "serialise('python', serialise_directory, 'tokens', tokens)\n",
    "serialise('python', serialise_directory, 'num_words_before', num_words_before)\n",
    "serialise('python', serialise_directory, 'num_words_after', num_words_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise datasets embedding matrix\n",
    "serialise('numpy', serialise_directory, 'embedding_matrix', embedding_matrix)\n",
    "serialise('python', serialise_directory, 'not_in_glove', not_in_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise encoded datasets\n",
    "serialise('pandas', serialise_directory, 'X_train_encoded', X_train_encoded)\n",
    "serialise('pandas', serialise_directory, 'X_val_encoded', X_val_encoded)\n",
    "serialise('pandas', serialise_directory, 'X_test_encoded', X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise tensors\n",
    "serialise('tensor', serialise_directory, 'x_train_tensor', x_train_tensor)\n",
    "serialise('tensor', serialise_directory, 'x_val_tensor', x_val_tensor)\n",
    "serialise('tensor', serialise_directory, 'x_test_tensor', x_test_tensor)\n",
    "serialise('tensor', serialise_directory, 'y_train_tensor', y_train_tensor)\n",
    "serialise('tensor', serialise_directory, 'y_val_tensor', y_val_tensor)\n",
    "serialise('tensor', serialise_directory, 'y_test_tensor', y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise PyTorch Dataset\n",
    "serialise('tensor', serialise_directory, 'train_dataset', train_dataset)\n",
    "serialise('tensor', serialise_directory, 'val_dataset', val_dataset)\n",
    "serialise('tensor', serialise_directory, 'test_dataset', test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deserialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise datasets after tokenisation\n",
    "X_train = deserialise('pandas', serialise_directory, 'X_train')\n",
    "X_val = deserialise('pandas', serialise_directory, 'X_val')\n",
    "X_test = deserialise('pandas', serialise_directory, 'X_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = deserialise('pandas', serialise_directory, 'y_train')\n",
    "y_val = deserialise('pandas', serialise_directory, 'y_val')\n",
    "y_test = deserialise('pandas', serialise_directory, 'y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(X_train), X_train.shape)\n",
    "# print(type(X_val), X_val.shape)\n",
    "# print(type(X_test), X_test.shape)\n",
    "# print(type(y_train), y_train.shape)\n",
    "# print(type(y_val), y_val.shape)\n",
    "# print(type(y_test), y_test.shape, end='\\n\\n')\n",
    "# print(X_train.head(2), end='\\n\\n')\n",
    "# print(y_train.head(2), end='\\n\\n')\n",
    "# print(type(mean_len))\n",
    "# print(mean_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise glove embeddings\n",
    "glove_embeddings = deserialise('numpy', serialise_directory, 'glove_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(glove_embeddings))\n",
    "# print(len(glove_embeddings))\n",
    "# print(glove_embeddings['car'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise datasets\n",
    "tokens = deserialise('python', serialise_directory, 'tokens')\n",
    "num_words_before = deserialise('python', serialise_directory, 'num_words_before')\n",
    "num_words_after = deserialise('python', serialise_directory, 'num_words_after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(tokens))\n",
    "# print(len(tokens))\n",
    "# print(tokens['car'], end='\\n\\n')\n",
    "# print(type(num_words_before))\n",
    "# print(num_words_before, end='\\n\\n')\n",
    "# print(type(num_words_after))\n",
    "# print(num_words_after, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise embedding matrix\n",
    "embedding_matrix = deserialise('numpy', serialise_directory, 'embedding_matrix')\n",
    "not_in_glove = deserialise('python', serialise_directory, 'not_in_glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(embedding_matrix))\n",
    "# print(embedding_matrix.shape)\n",
    "# print(embedding_matrix[56], end='\\n\\n')\n",
    "# print(type(not_in_glove))\n",
    "# print(len(not_in_glove))\n",
    "# print(not_in_glove[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise encoded datasets\n",
    "X_train_encoded = deserialise('numpy', serialise_directory, 'X_train_encoded')\n",
    "X_val_encoded = deserialise('numpy', serialise_directory, 'X_val_encoded')\n",
    "X_test_encoded = deserialise('numpy', serialise_directory, 'X_test_encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(X_train_encoded))\n",
    "# print(X_train_encoded.shape)\n",
    "# print(type(X_val_encoded))\n",
    "# print(X_val_encoded.shape)\n",
    "# print(type(X_test_encoded))\n",
    "# print(X_test_encoded.shape, end='\\n\\n')\n",
    "# print(X_train.sample(2), end='\\n\\n')\n",
    "# print((X_train_encoded.map(lambda x: len(x))).mean())\n",
    "# print((X_val_encoded.map(lambda x: len(x))).mean())\n",
    "# print((X_test_encoded.map(lambda x: len(x))).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise tensors\n",
    "x_train_tensor = deserialise('tensor', serialise_directory, 'x_train_tensor')\n",
    "x_val_tensor = deserialise('tensor', serialise_directory, 'x_val_tensor')\n",
    "x_test_tensor = deserialise('tensor', serialise_directory, 'x_test_tensor')\n",
    "y_train_tensor = deserialise('tensor', serialise_directory, 'y_train_tensor')\n",
    "y_val_tensor = deserialise('tensor', serialise_directory, 'y_val_tensor')\n",
    "y_test_tensor = deserialise('tensor', serialise_directory, 'y_test_tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(x_train_tensor), x_train_tensor.shape)\n",
    "# print(type(x_val_tensor), x_val_tensor.shape)\n",
    "# print(type(x_test_tensor), x_test_tensor.shape)\n",
    "# print(type(y_train_tensor), y_train_tensor.shape)\n",
    "# print(type(y_val_tensor), y_val_tensor.shape)\n",
    "# print(type(y_test_tensor), y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise PyTorch Dataset\n",
    "train_dataset = deserialise('tensor', serialise_directory, 'train_dataset')\n",
    "val_dataset = deserialise('tensor', serialise_directory, 'val_dataset')\n",
    "test_dataset = deserialise('tensor', serialise_directory, 'test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(train_dataset), len(train_dataset))\n",
    "# print(type(val_dataset), len(val_dataset))\n",
    "# print(type(test_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For small dataset is fine to use the whole training data at every training step (i.e. batch gradient descent). \n",
    "# If we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. \n",
    "# Thus, we need to slice our dataset accordingly. Do you want to do it manually?! Me neither!\n",
    "# So we use the 'DataLoader' class for this job. We tell it which dataset to use, the desired mini-batch size and if we’d \n",
    "# like to shuffle it or not. That’s it!\n",
    "# Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1048, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1048, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1048, shuffle=False)\n",
    "\n",
    "# To retrieve a sample mini-batch, one can simply run the command below.\n",
    "# It will return a list containing two tensors: one for the features, another one for the labels:\n",
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Classifier\n",
    "\n",
    "At its core, the training routine is responsible for instantiating the model, iterating over the dataset, computing the output of the model when given the data as input, computing the loss (how wrong the model is), and updating the model proportional to the loss. \n",
    "\n",
    "Although this may seem like a lot of details to manage, there are not many places to change the training routine, and as such it will become habitual in your deep learning development process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "hidden_size = 64\n",
    "output_dim = 1\n",
    "dropout_p = 0.3\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise classifier\n",
    "# We need to send our model to the same device where the data is. If our data is made of GPU tensors, \n",
    "# our model must “live” inside the GPU as well. That's what '.to(device)' is there for.\n",
    "\n",
    "classifier = BiGRUClassifier(embedding_size=embedding_dim, num_embeddings=num_embeddings, num_layers=num_layers, \n",
    "                           batch_first=True, hidden_size=hidden_size, output_dim=output_dim, bidirectional=True,\n",
    "                           dropout_p=dropout_p, pretrained_embeddings=embedding_matrix, padding_idx=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), lr=0.01)    #filtering only for the params that require updating doesn't speed up training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialise / Deserialise Embedding Parameters before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_before = params(classifier.emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise embedding params before training\n",
    "serialise('tensor', serialise_directory, 'params_before', params_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise embedding params before training\n",
    "params_before = deserialise('tensor', serialise_directory, 'params_before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2319, -0.1954,  0.0334,  ...,  0.1139,  0.1851, -0.3686],\n",
       "        [ 0.0206,  0.3291, -0.0162,  ..., -0.0801, -0.2023,  0.0679],\n",
       "        ...,\n",
       "        [-0.1062,  0.1081,  0.0919,  ..., -0.0722,  0.0423, -0.0406],\n",
       "        [-0.0733,  0.1390, -0.0326,  ...,  0.1367,  0.0150, -0.0859],\n",
       "        [-0.1353, -0.1157,  0.0118,  ..., -0.0008,  0.0778, -0.0832]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: emb --- Embedding(95399, 300, padding_idx=0)\n",
      "\n",
      "ToT Params: 28,619,700\n",
      "\n",
      "Param length: 95,399\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2319, -0.1954,  0.0334,  ...,  0.1139,  0.1851, -0.3686],\n",
      "        [ 0.0206,  0.3291, -0.0162,  ..., -0.0801, -0.2023,  0.0679],\n",
      "        ...,\n",
      "        [-0.1062,  0.1081,  0.0919,  ..., -0.0722,  0.0423, -0.0406],\n",
      "        [-0.0733,  0.1390, -0.0326,  ...,  0.1367,  0.0150, -0.0859],\n",
      "        [-0.1353, -0.1157,  0.0118,  ..., -0.0008,  0.0778, -0.0832]])\n",
      "\n",
      "Are parameters being updated during backprop? False\n",
      "\n",
      "Total Sets of Parameters: 1\n",
      "\n",
      "******************************************************************************************\n",
      "Layer name: gru --- GRU(300, 64, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "\n",
      "ToT Params: 140,544\n",
      "\n",
      "Param length: 192\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.1010,  0.0226, -0.0537,  ...,  0.0552, -0.0178,  0.0806],\n",
      "        [ 0.1131, -0.0396,  0.0986,  ..., -0.0591,  0.0294,  0.0428],\n",
      "        [-0.0769, -0.1022, -0.0304,  ..., -0.1068, -0.0011, -0.0317],\n",
      "        ...,\n",
      "        [ 0.1167, -0.0337,  0.0684,  ..., -0.0760, -0.1142, -0.0693],\n",
      "        [ 0.0394, -0.0351, -0.0348,  ...,  0.0859, -0.0965, -0.0189],\n",
      "        [-0.0537,  0.0495, -0.1244,  ...,  0.0827,  0.0277, -0.1102]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 192\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.0331,  0.0798,  0.0372,  ...,  0.0711,  0.1086,  0.0710],\n",
      "        [-0.1151,  0.0192,  0.0079,  ..., -0.0527,  0.1172, -0.0492],\n",
      "        [ 0.1091, -0.1150, -0.1208,  ...,  0.0647, -0.0866, -0.1090],\n",
      "        ...,\n",
      "        [-0.0017,  0.0105, -0.0722,  ...,  0.0772, -0.0106, -0.1014],\n",
      "        [-0.0768,  0.0501, -0.1191,  ...,  0.1080, -0.0255,  0.0294],\n",
      "        [-0.0088, -0.0696,  0.0802,  ..., -0.1044,  0.0433, -0.1153]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 192\n",
      "\n",
      "Parameter containing:\n",
      "tensor([ 0.0417,  0.0296, -0.0679, -0.0827,  0.0228, -0.1227, -0.0122, -0.0026,\n",
      "         0.0295,  0.0949,  0.0635,  0.1003,  0.0151, -0.1017,  0.0007,  0.0201,\n",
      "        -0.0285,  0.1249, -0.0518, -0.1142,  0.1114,  0.0463,  0.0900, -0.1226,\n",
      "         0.0846,  0.0527, -0.0321,  0.0835, -0.0257, -0.0957,  0.0187, -0.0026,\n",
      "        -0.0065, -0.0806,  0.0198, -0.0880,  0.0348, -0.1192, -0.0516, -0.0256,\n",
      "         0.0421,  0.0862,  0.0732, -0.0650, -0.0994, -0.0886, -0.1192, -0.0228,\n",
      "        -0.0937,  0.0522, -0.0739,  0.1093,  0.1000,  0.0854,  0.0178, -0.0965,\n",
      "        -0.0909,  0.0286, -0.0464,  0.0550, -0.1032, -0.0795, -0.0132, -0.0536,\n",
      "        -0.1116,  0.1002, -0.0538,  0.0339,  0.0670,  0.0984,  0.0449,  0.0408,\n",
      "         0.1120, -0.0765,  0.1210, -0.0984,  0.0951, -0.0313, -0.0441, -0.1088,\n",
      "        -0.0849,  0.0549,  0.0095,  0.0644, -0.0777, -0.0157, -0.0473,  0.0182,\n",
      "        -0.1056,  0.1074,  0.0237, -0.0345, -0.0346,  0.0030, -0.0740,  0.0389,\n",
      "         0.0713, -0.0616,  0.0758,  0.0667,  0.1082, -0.0142, -0.0706, -0.0190,\n",
      "        -0.1241, -0.0655,  0.0252, -0.1068, -0.0978,  0.0035,  0.0660,  0.1198,\n",
      "         0.0623, -0.0720,  0.0753, -0.0905, -0.1079, -0.0029,  0.0638,  0.0304,\n",
      "        -0.0848,  0.0160,  0.0042,  0.0527,  0.1051, -0.0754,  0.0464, -0.0180,\n",
      "        -0.0356, -0.0602,  0.0014,  0.0468,  0.0762,  0.0358,  0.0943, -0.0697,\n",
      "        -0.0266,  0.0837,  0.0192,  0.0058,  0.0868,  0.1056,  0.1069,  0.0594,\n",
      "        -0.0469,  0.0656,  0.0682,  0.0478,  0.0735,  0.0309,  0.1123,  0.1061,\n",
      "        -0.0705,  0.1019, -0.1236,  0.0977, -0.0475, -0.0628,  0.0142, -0.0322,\n",
      "         0.0825, -0.0560,  0.0891, -0.0507, -0.0552, -0.0841,  0.0528, -0.0049,\n",
      "         0.0339, -0.0299, -0.0609, -0.0843, -0.0273, -0.1005, -0.0657, -0.0391,\n",
      "        -0.0021, -0.1157, -0.0817, -0.0440, -0.1230, -0.0205,  0.0484,  0.0490,\n",
      "         0.0760,  0.1072,  0.0820, -0.1151,  0.0398, -0.1093, -0.0940, -0.0123],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 192\n",
      "\n",
      "Parameter containing:\n",
      "tensor([ 0.0922, -0.0827,  0.0622, -0.0207,  0.0640, -0.0133,  0.0363, -0.0499,\n",
      "        -0.0593, -0.0370,  0.0646, -0.0690,  0.0536, -0.0556, -0.0782, -0.0734,\n",
      "        -0.1143,  0.0965,  0.0655,  0.0025,  0.0185,  0.0313, -0.1162, -0.1029,\n",
      "         0.0825, -0.0152,  0.0827, -0.0452,  0.0888, -0.0048,  0.0122, -0.1098,\n",
      "        -0.0132, -0.0699, -0.0077,  0.0159, -0.0153,  0.1006,  0.0660,  0.1246,\n",
      "        -0.0957, -0.0114,  0.0060,  0.1190,  0.0997,  0.1157, -0.0726,  0.1087,\n",
      "        -0.0882, -0.0859,  0.0190,  0.0152, -0.0121,  0.0546, -0.0538, -0.0584,\n",
      "         0.1211,  0.0996,  0.0768, -0.0148, -0.0364,  0.0686, -0.0962,  0.1139,\n",
      "         0.1206, -0.0922, -0.0914, -0.0807, -0.0157, -0.0479,  0.0520,  0.1084,\n",
      "        -0.0504,  0.1076, -0.0674, -0.0388, -0.0400, -0.0491,  0.0118,  0.0923,\n",
      "        -0.0380,  0.1035, -0.0981, -0.0896,  0.1058,  0.0731,  0.0867, -0.0579,\n",
      "         0.1064, -0.1145,  0.0195, -0.1161,  0.1161,  0.0727, -0.0609, -0.0886,\n",
      "        -0.0445, -0.1117, -0.0782,  0.0365, -0.0393,  0.0941,  0.0198,  0.0640,\n",
      "         0.1069,  0.1029,  0.0407,  0.1026, -0.0795, -0.0623,  0.0522,  0.0253,\n",
      "         0.0508,  0.0319, -0.0822, -0.0881,  0.0667, -0.1099,  0.0235, -0.0316,\n",
      "        -0.0945, -0.0047, -0.1092,  0.0929, -0.0718, -0.1137,  0.1044,  0.1172,\n",
      "        -0.0366, -0.0567, -0.0660,  0.0058, -0.1214,  0.0196,  0.0788, -0.0684,\n",
      "        -0.1031,  0.0582, -0.0779, -0.0371,  0.1223, -0.0022,  0.0890, -0.0480,\n",
      "         0.1121,  0.1015, -0.1152,  0.0473,  0.0083,  0.0359,  0.1243, -0.0478,\n",
      "         0.0999, -0.0117,  0.0977,  0.0622, -0.0442,  0.0379,  0.0577,  0.0192,\n",
      "         0.0015,  0.0561,  0.1162, -0.0512, -0.1133,  0.0076,  0.0487, -0.0974,\n",
      "         0.0996, -0.0770, -0.0242,  0.1059,  0.0793,  0.0237,  0.0610,  0.0386,\n",
      "         0.0993,  0.0941, -0.1125,  0.0072, -0.0444,  0.0490, -0.1210, -0.1247,\n",
      "         0.0141, -0.0879, -0.0317,  0.0482,  0.0578, -0.0653,  0.0388, -0.0445],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 192\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.0929,  0.0492, -0.0939,  ..., -0.0570, -0.0656, -0.0127],\n",
      "        [-0.0610,  0.0809,  0.1104,  ...,  0.0123,  0.0336, -0.0602],\n",
      "        [ 0.1022,  0.0748, -0.0352,  ..., -0.0229, -0.0493,  0.0917],\n",
      "        ...,\n",
      "        [ 0.0497,  0.0343, -0.1029,  ...,  0.0143, -0.0195, -0.0707],\n",
      "        [-0.0888, -0.0544,  0.0965,  ..., -0.0095, -0.1203, -0.0451],\n",
      "        [-0.0553, -0.0894,  0.0548,  ...,  0.0226,  0.0694, -0.0822]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 192\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.1223,  0.0543, -0.0725,  ...,  0.0560,  0.0122, -0.0369],\n",
      "        [ 0.0559, -0.1164, -0.0019,  ..., -0.1206, -0.0914, -0.0533],\n",
      "        [-0.0585,  0.0561, -0.0543,  ...,  0.0965, -0.0419,  0.0294],\n",
      "        ...,\n",
      "        [ 0.0039, -0.0415,  0.1148,  ...,  0.0645, -0.0386,  0.0404],\n",
      "        [ 0.1020, -0.1063,  0.1175,  ...,  0.0491, -0.1185,  0.0732],\n",
      "        [ 0.1154,  0.0692,  0.0888,  ..., -0.0153,  0.0796,  0.0003]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 192\n",
      "\n",
      "Parameter containing:\n",
      "tensor([ 0.0162,  0.0432,  0.1079, -0.0440, -0.0351,  0.0110, -0.0821,  0.0854,\n",
      "         0.0499, -0.0469, -0.0296, -0.0405, -0.1216,  0.0280, -0.1145, -0.0938,\n",
      "        -0.0146,  0.0851, -0.0947,  0.0462, -0.0215, -0.0272, -0.0613,  0.0379,\n",
      "        -0.0125, -0.0200,  0.0716, -0.1227,  0.0438,  0.0104, -0.0018,  0.0630,\n",
      "         0.1169,  0.0740, -0.0535, -0.0721, -0.0059,  0.0990,  0.1144, -0.0709,\n",
      "        -0.0074,  0.1156, -0.0214,  0.0761,  0.0936, -0.0396, -0.0138, -0.1203,\n",
      "         0.0448,  0.0187,  0.0195,  0.1113,  0.0620,  0.0794, -0.0308, -0.0952,\n",
      "        -0.1102,  0.0021,  0.0936,  0.0392, -0.0120, -0.0363, -0.0477,  0.0654,\n",
      "         0.0676,  0.1014, -0.0247,  0.0530,  0.0112,  0.0941, -0.0862, -0.0389,\n",
      "         0.0648, -0.0333, -0.0781,  0.1113,  0.0136,  0.0642,  0.0176, -0.0238,\n",
      "         0.0608,  0.1044,  0.1189, -0.0633, -0.0455, -0.0527, -0.0644,  0.0127,\n",
      "         0.1120,  0.0204,  0.0091,  0.0822, -0.0405,  0.0085,  0.0880,  0.1107,\n",
      "         0.0919,  0.0139,  0.0692, -0.0076, -0.0529,  0.1123, -0.1155,  0.1141,\n",
      "        -0.0258,  0.0616,  0.0753,  0.0849, -0.0834, -0.0043, -0.0621, -0.0240,\n",
      "         0.0821, -0.0870,  0.0119, -0.0765,  0.0574,  0.0702,  0.1244,  0.0154,\n",
      "         0.1096,  0.0687, -0.0352,  0.0958,  0.1068, -0.0779,  0.1008, -0.1127,\n",
      "        -0.1147, -0.0080,  0.0127, -0.1042, -0.0102,  0.1151, -0.0842,  0.0807,\n",
      "         0.0977, -0.0500,  0.0659,  0.0798,  0.0560,  0.0683, -0.0477,  0.0447,\n",
      "         0.0690, -0.1164,  0.0586,  0.0568,  0.0193, -0.1012,  0.1039, -0.0678,\n",
      "        -0.0531, -0.0881, -0.0967, -0.0027, -0.1019, -0.0861,  0.0426,  0.0307,\n",
      "         0.0982,  0.0274,  0.0868,  0.0630, -0.0460, -0.0193,  0.0631,  0.1227,\n",
      "        -0.0605, -0.0336, -0.0322,  0.0917,  0.0470,  0.1110, -0.0966,  0.0694,\n",
      "        -0.0057, -0.0942, -0.0915,  0.0145,  0.0163, -0.0943,  0.0025, -0.1104,\n",
      "        -0.1181,  0.0802, -0.0829,  0.0449, -0.0183, -0.0225,  0.0323,  0.0771],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 192\n",
      "\n",
      "Parameter containing:\n",
      "tensor([-0.0189,  0.0593,  0.0140,  0.0889, -0.1205, -0.0882,  0.0798,  0.0620,\n",
      "         0.0695, -0.0646,  0.0662, -0.0902,  0.0400,  0.0971,  0.0490, -0.0452,\n",
      "         0.0217, -0.0245, -0.0882,  0.0592,  0.0938, -0.0114,  0.0971, -0.0798,\n",
      "        -0.0892,  0.0280,  0.1110, -0.0369, -0.1186, -0.0410,  0.1159,  0.0616,\n",
      "        -0.0033,  0.1132,  0.0054, -0.0894,  0.0004,  0.0067,  0.1029, -0.1112,\n",
      "        -0.0234, -0.0634, -0.0223,  0.0884, -0.0646,  0.1204, -0.0288, -0.0532,\n",
      "         0.0213, -0.0628, -0.0887, -0.0771, -0.1241, -0.0443, -0.0447,  0.0577,\n",
      "         0.1116,  0.0544, -0.0665, -0.1151,  0.0029, -0.0463,  0.1016, -0.0391,\n",
      "        -0.0668,  0.0367, -0.0100,  0.0474, -0.0323, -0.1137,  0.1094, -0.1010,\n",
      "         0.0617, -0.1062, -0.1005,  0.1212, -0.1236,  0.0077, -0.0546, -0.0250,\n",
      "        -0.0573, -0.1131, -0.1083,  0.0543,  0.0179,  0.0659, -0.0552, -0.0383,\n",
      "         0.1004, -0.0786,  0.0178,  0.0092,  0.0441, -0.0280,  0.1159,  0.0726,\n",
      "         0.0959, -0.0091,  0.1196, -0.0004, -0.0433,  0.0604, -0.0342,  0.1041,\n",
      "         0.1192, -0.0152,  0.0550,  0.0278, -0.1218, -0.0209, -0.0454,  0.0874,\n",
      "        -0.0157,  0.1082,  0.0917, -0.0214,  0.0789, -0.0310,  0.0375, -0.0036,\n",
      "         0.0260,  0.0513,  0.0199, -0.0916,  0.0527, -0.0782,  0.1055, -0.0171,\n",
      "        -0.0311,  0.0628,  0.0346,  0.0255, -0.0156, -0.0618, -0.0399, -0.0798,\n",
      "         0.0622, -0.0691,  0.0355, -0.0938,  0.0284,  0.0379, -0.1003, -0.1129,\n",
      "         0.0648, -0.0436, -0.1004, -0.0886,  0.0700,  0.0591,  0.0642, -0.1104,\n",
      "         0.1084,  0.0793, -0.0011, -0.0747,  0.0544, -0.0522,  0.1242,  0.0659,\n",
      "         0.0316, -0.0673,  0.0442, -0.0300,  0.0667, -0.0577,  0.0868,  0.1125,\n",
      "         0.0908, -0.0416, -0.1136, -0.1088,  0.0882,  0.1190,  0.0543,  0.0094,\n",
      "        -0.0540, -0.0552, -0.1137,  0.0345, -0.1191, -0.0620,  0.0757,  0.0522,\n",
      "        -0.1013, -0.0603,  0.0987, -0.0977,  0.0993,  0.0827, -0.0359,  0.0853],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Total Sets of Parameters: 8\n",
      "\n",
      "******************************************************************************************\n",
      "Layer name: fc1 --- Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "ToT Params: 16,512\n",
      "\n",
      "Param length: 128\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.0091, -0.0257,  0.0292,  ..., -0.0591, -0.0575,  0.0866],\n",
      "        [ 0.0851, -0.0039, -0.0110,  ...,  0.0328, -0.0769, -0.0556],\n",
      "        [-0.0213,  0.0706, -0.0167,  ...,  0.0416, -0.0188, -0.0474],\n",
      "        ...,\n",
      "        [-0.0706,  0.0804, -0.0067,  ...,  0.0805,  0.0274, -0.0676],\n",
      "        [-0.0814,  0.0349,  0.0421,  ..., -0.0130,  0.0766, -0.0020],\n",
      "        [ 0.0706,  0.0349, -0.0611,  ..., -0.0464, -0.0231, -0.0222]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 128\n",
      "\n",
      "Parameter containing:\n",
      "tensor([ 0.0811,  0.0379,  0.0620, -0.0400, -0.0600,  0.0856,  0.0816, -0.0326,\n",
      "        -0.0865,  0.0476,  0.0394,  0.0513, -0.0317, -0.0050,  0.0419, -0.0408,\n",
      "         0.0371,  0.0278,  0.0522, -0.0550, -0.0753, -0.0555,  0.0157, -0.0025,\n",
      "        -0.0533,  0.0162, -0.0456,  0.0220, -0.0615, -0.0732, -0.0347,  0.0527,\n",
      "         0.0421,  0.0757,  0.0666,  0.0270, -0.0682,  0.0861, -0.0720,  0.0161,\n",
      "        -0.0750, -0.0330,  0.0877,  0.0483,  0.0182, -0.0590, -0.0734, -0.0680,\n",
      "        -0.0060, -0.0646, -0.0668,  0.0386,  0.0309,  0.0764, -0.0335,  0.0494,\n",
      "        -0.0013,  0.0734, -0.0052, -0.0655,  0.0631,  0.0656,  0.0243, -0.0798,\n",
      "        -0.0313, -0.0146, -0.0494, -0.0097, -0.0046, -0.0082,  0.0426, -0.0189,\n",
      "        -0.0462, -0.0304, -0.0631, -0.0611, -0.0865, -0.0710,  0.0850,  0.0678,\n",
      "        -0.0603, -0.0276,  0.0065,  0.0383,  0.0162,  0.0680, -0.0621, -0.0697,\n",
      "         0.0675,  0.0543,  0.0003, -0.0040, -0.0608,  0.0290,  0.0614, -0.0772,\n",
      "         0.0542, -0.0059, -0.0116, -0.0352, -0.0848, -0.0180,  0.0203,  0.0413,\n",
      "         0.0112, -0.0496,  0.0699, -0.0571, -0.0873,  0.0747,  0.0531, -0.0475,\n",
      "        -0.0778, -0.0776, -0.0025,  0.0823,  0.0535,  0.0674,  0.0226, -0.0769,\n",
      "         0.0338, -0.0291,  0.0436,  0.0800,  0.0652, -0.0730, -0.0603, -0.0069],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Total Sets of Parameters: 2\n",
      "\n",
      "******************************************************************************************\n",
      "Layer name: fc2 --- Linear(in_features=128, out_features=1, bias=True)\n",
      "\n",
      "ToT Params: 129\n",
      "\n",
      "Param length: 1\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.0755,  0.0429, -0.0420, -0.0491, -0.0288,  0.0278,  0.0621, -0.0542,\n",
      "         -0.0723,  0.0515, -0.0174, -0.0548,  0.0347,  0.0685, -0.0819, -0.0395,\n",
      "         -0.0478,  0.0002, -0.0799,  0.0488, -0.0884,  0.0249, -0.0241,  0.0048,\n",
      "          0.0538,  0.0484, -0.0366,  0.0086, -0.0388, -0.0263,  0.0532,  0.0467,\n",
      "         -0.0209, -0.0116, -0.0596, -0.0500,  0.0481,  0.0128,  0.0877,  0.0460,\n",
      "         -0.0643,  0.0358,  0.0796,  0.0089, -0.0688, -0.0532,  0.0212, -0.0099,\n",
      "          0.0814,  0.0466, -0.0722,  0.0833,  0.0822,  0.0071,  0.0648,  0.0401,\n",
      "         -0.0802, -0.0730,  0.0527, -0.0173,  0.0435,  0.0640, -0.0181, -0.0334,\n",
      "          0.0723,  0.0540,  0.0032, -0.0616,  0.0795,  0.0680,  0.0857,  0.0463,\n",
      "         -0.0349, -0.0817,  0.0124,  0.0505, -0.0579, -0.0596,  0.0045,  0.0874,\n",
      "          0.0418,  0.0647, -0.0240, -0.0350,  0.0425, -0.0302,  0.0048, -0.0362,\n",
      "         -0.0053, -0.0796, -0.0326,  0.0373, -0.0045, -0.0821,  0.0090, -0.0269,\n",
      "          0.0771,  0.0002,  0.0835,  0.0008,  0.0438,  0.0849, -0.0133,  0.0577,\n",
      "          0.0028, -0.0785,  0.0525,  0.0416,  0.0694,  0.0878,  0.0379, -0.0147,\n",
      "          0.0248,  0.0270,  0.0323,  0.0033,  0.0164,  0.0470, -0.0028, -0.0298,\n",
      "          0.0204, -0.0821, -0.0556,  0.0511, -0.0366, -0.0503, -0.0230,  0.0407]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Param length: 1\n",
      "\n",
      "Parameter containing:\n",
      "tensor([-0.0226], requires_grad=True)\n",
      "\n",
      "Are parameters being updated during backprop? True\n",
      "\n",
      "Total Sets of Parameters: 2\n",
      "\n",
      "******************************************************************************************\n",
      "Layer name: sigmoid --- Sigmoid()\n",
      "\n",
      "ToT Params: 0\n",
      "\n",
      "Total Sets of Parameters: 0\n",
      "\n",
      "******************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# check params initialised by the classifier\n",
    "check_params(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 157,185 trainable parameters\n",
      "The model has 28,776,885 parameters overall\n"
     ]
    }
   ],
   "source": [
    "# trainable / all params before training\n",
    "trainable_params, all_params = num_params(classifier)\n",
    "print('The model has {:,} trainable parameters'.format(trainable_params))\n",
    "print('The model has {:,} parameters overall'.format(all_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "The training loop is composed of two loops: an inner loop over minibatches in the dataset, and an outer loop, which repeats the inner loop a number of times. In the inner loop, losses are computed for each minibatch, and the optimizer is used to\n",
    "update the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "\n",
      "Epoch: 0 | Train Loss: 0.521 | Val Loss: 0.376 | Train Acc: 73.541 | Val Acc: 82.963\n",
      "(157185, 28776885)\n",
      "\n",
      "Epoch: 1 | Train Loss: 0.338 | Val Loss: 0.356 | Train Acc: 85.256 | Val Acc: 84.435\n",
      "(157185, 28776885)\n",
      "\n",
      "Epoch: 2 | Train Loss: 0.297 | Val Loss: 0.286 | Train Acc: 87.299 | Val Acc: 87.795\n",
      "(157185, 28776885)\n",
      "\n",
      "Epoch: 3 | Train Loss: 0.273 | Val Loss: 0.274 | Train Acc: 88.485 | Val Acc: 88.379\n",
      "(157185, 28776885)\n",
      "\n",
      "Epoch: 4 | Train Loss: 0.257 | Val Loss: 0.268 | Train Acc: 89.221 | Val Acc: 88.714\n",
      "(157185, 28776885)\n",
      "\n",
      "Epoch: 5 | Train Loss: 0.222 | Val Loss: 0.210 | Train Acc: 90.877 | Val Acc: 91.516\n",
      "(28776885, 28776885)\n",
      "\n",
      "Epoch: 6 | Train Loss: 0.178 | Val Loss: 0.201 | Train Acc: 93.032 | Val Acc: 91.994\n",
      "(28776885, 28776885)\n",
      "\n",
      "Epoch: 7 | Train Loss: 0.148 | Val Loss: 0.205 | Train Acc: 94.336 | Val Acc: 92.006\n",
      "(28776885, 28776885)\n",
      "\n",
      "Epoch: 8 | Train Loss: 0.123 | Val Loss: 0.221 | Train Acc: 95.437 | Val Acc: 91.627\n",
      "(28776885, 28776885)\n",
      "\n",
      "Epoch: 9 | Train Loss: 0.100 | Val Loss: 0.269 | Train Acc: 96.413 | Val Acc: 90.826\n",
      "(28776885, 28776885)\n",
      "\n",
      "\n",
      "Training complete\n",
      "\n",
      "8301.74523806572\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "n_epochs = 10\n",
    "n_epoch_freezed = 5\n",
    "\n",
    "print('Starting training', end='\\n\\n')\n",
    "\n",
    "\n",
    "# Enumerate epochs\n",
    "epoch = 0\n",
    "\n",
    "# For a certain number of epochs (defined by 'n_epoch_freezed'), the emebdding matrix is frozen, then it is unfrozen \n",
    "# i.e. the embeddings get trained (except for the padding vector which remains 0)\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch < n_epoch_freezed:   \n",
    "        pass   # keep the embedding layer frozen (i.e. classifier.emb.weight.requires_grad=False as set in section 8 above)\n",
    "    else: \n",
    "        classifier.emb.weight.requires_grad=True\n",
    "\n",
    "    # Training part\n",
    "    classifier.train()\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_acc = 0\n",
    "    \n",
    "    for i, (x_train, y_train) in enumerate(train_loader):\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        \n",
    "        #print(x_train.shape)\n",
    "        #print(y_train.shape)\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation: compute the model output (i.e. predictions)\n",
    "        y_pred = classifier(x_in=x_train)\n",
    "\n",
    "        #print(x_train.requires_grad, y_train.requires_grad, y_pred.requires_grad)\n",
    "                                                                                                        \n",
    "        # Loss calculation\n",
    "        t_loss = loss_func(y_pred, y_train)\n",
    "        \n",
    "        # Accuracy\n",
    "        t_acc = binary_acc(y_pred, y_train)\n",
    "        \n",
    "        # Backward propagation: use loss to produce gradients\n",
    "        t_loss.backward()\n",
    "        \n",
    "        # Weight optimization: use optimizer to take gradient step and update parameters (w,b) \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += t_loss.item()\n",
    "        epoch_train_acc += t_acc.item()\n",
    "                                                                                                             \n",
    "   \n",
    "    # Evaluation part\n",
    "    classifier.eval() # .eval() tells PyTorch that we do not want to perform back-propagation during inference\n",
    "    \n",
    "    epoch_val_loss = 0\n",
    "    epoch_val_acc = 0\n",
    "    \n",
    "    #We use torch.no_grad() which reduces memory usage and speeds up computation.\n",
    "    with torch.no_grad():     #https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/3 : torch.no_grad() deals with the autograd engine and stops it from calculating the gradients, which is the recommended way of doing validation\n",
    "        for i, (x_val, y_val) in enumerate(val_loader):\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "        \n",
    "            # Forward propagation: compute the model output (i.e. predictions)\n",
    "            y_pred = classifier(x_in=x_val)     #tensors of probabilities\n",
    "        \n",
    "            # Loss calculation\n",
    "            v_loss = loss_func(y_pred, y_val)  \n",
    "            \n",
    "            # Accuracy\n",
    "            v_acc = binary_acc(y_pred, y_val)\n",
    "            \n",
    "            epoch_val_loss += v_loss.item()\n",
    "            epoch_val_acc += v_acc.item()\n",
    "            \n",
    "    print('Epoch: {} | Train Loss: {:.3f} | Val Loss: {:.3f} | Train Acc: {:.3f} | Val Acc: {:.3f}'.format(epoch,\n",
    "                                                                                epoch_train_loss/len(train_loader),\n",
    "                                                                                epoch_val_loss/len(val_loader),\n",
    "                                                                                epoch_train_acc/len(train_loader),\n",
    "                                                                                epoch_val_acc/len(val_loader)))\n",
    "    \n",
    "    print(num_params(classifier), end='\\n\\n')\n",
    "\n",
    "epoch += 1\n",
    "    \n",
    "\n",
    "print()\n",
    "print('Training complete', end='\\n\\n')\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialise / Deserialise Embedding Parameters after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_after = params(classifier.emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise embedding params after training\n",
    "serialise('tensor', serialise_directory, 'params_after', params_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise embedding params after training\n",
    "params_after = deserialise('tensor', serialise_directory, 'params_after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True],\n",
       "        [ True,  True,  True,  ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check changes in params\n",
    "params_after == params_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-2.5473e-04, -7.1561e-04,  6.9707e-05,  ..., -4.6912e-04,\n",
       "         -7.5195e-04,  9.0984e-04],\n",
       "        [ 9.3345e-07, -2.3603e-07,  4.1082e-08,  ...,  1.2100e-07,\n",
       "          1.3294e-06,  7.3628e-07],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check gradients\n",
    "classifier.emb.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 28,776,885 trainable parameters\n",
      "The model has 28,776,885 parameters overall\n"
     ]
    }
   ],
   "source": [
    "# trainable / all params after training\n",
    "trainable_params, all_params = num_params(classifier)\n",
    "print('The model has {:,} trainable parameters'.format(trainable_params))\n",
    "print('The model has {:,} parameters overall'.format(all_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on test data\n",
    "\n",
    "To evaluate the data on the held-out test set, the code is exactly the same as the validation loop in the training routine we saw in the previous step. \n",
    "\n",
    "The test set should be run as little as possible. Each time you run a trained model on the test set, make a new model decision (such as changing the size of the layers), and remeasure the new retrained model on the test set, you are biasing your\n",
    "modeling decisions toward the test data. In other words, if you repeat that process often enough, the test set will become meaningless as an accurate measure of truly held-out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.264 | Test Acc: 90.946\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "classifier.eval()  \n",
    "\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "    \n",
    "with torch.no_grad():     #torch.no_grad() deals with the autograd engine and stops it from calculating the gradients, which is the recommended way of doing validation\n",
    "    for i, (x_test, y_test) in enumerate(test_loader):\n",
    "        x_test = x_test.to(device) \n",
    "        y_test = y_test.to(device)\n",
    "        \n",
    "        # Forward propagation: compute the model output (i.e. predictions)\n",
    "        y_pred = classifier(x_in=x_test)     #tensors of probabilities\n",
    "        \n",
    "        # Loss calculation\n",
    "        tst_loss = loss_func(y_pred, y_test)\n",
    "        \n",
    "        # Accuracy\n",
    "        tst_acc = binary_acc(y_pred, y_test)\n",
    "            \n",
    "        test_loss += tst_loss.item()\n",
    "        test_acc += tst_acc.item()\n",
    "            \n",
    "print('Test Loss: {:.3f} | Test Acc: {:.3f}'.format(test_loss/len(test_loader), test_acc/len(test_loader)))\n",
    "\n",
    "print()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\"Suspended my account without warning and my order got delayed. I called them two days in a row to be fobbed off \\\n",
    "          and to wait for an email. No email. No access to my online account. 3 amazon prime payments taken from my card \\\n",
    "          without my permission. Just got access back to claim my money back 12 weeks later and didn't even get an apology \\\n",
    "          from the online chat. Didn't even get my order either.\",\n",
    "          'Fantastic!!!!',\n",
    "          'The products they sent me always work fine but... I have Amazon prime and they sometimes deliver days late or \\\n",
    "          just leave the product on your doorstep out in the open easy to be stolen. Overall, my experience with Amazon is \\\n",
    "          fairly good',\n",
    "          'Safe place never used…doorstep deliveries. I am a long standing Amazon customer and high spending. I have had \\\n",
    "          a safe place named for deliveries for a long time. The odd driver hitters to open the door to the bin shed and \\\n",
    "          out the parcels in. The majority just leave it on the doorstep. With thefts high, I find this utterly ridiculous. \\\n",
    "          What is the point of a safe place if it is simply ignored and never used? I live in a house with a small front \\\n",
    "          garden - not a difficult to access apartment block or something. Amazon - please hold your drivers accountable \\\n",
    "          to the requests of customers. You offer and unique and great service but this lets you down.',\n",
    "          'My beautiful Jenuh killed by incompetence! She went to Dewey Veterinary Medical Center 3x! My MISTAKE!! I was \\\n",
    "          told she had nothing but allergies! She was filled with injections that DID NOTHING for her along with \\\n",
    "          medications that were ridiculously priced and again that did nothing!! 5 days after her last visit with Dewey \\\n",
    "          I’m in another veterinary office with her in critical condition and them saying she was on oxygen and had a very \\\n",
    "          bad infection! They’re telling me the rash on her body was NOT allergies, but was in fact signs of trouble from \\\n",
    "          infection!!! She was never treated just given more shots and new medications and here she was suffering inside! \\\n",
    "          (All I was told was many pets were coming in suffering from allergies) My 8 month beautiful blue and green-eyed \\\n",
    "          Husky died all because Dewey Vet did NOTHING for her!! She lost her life and I didn’t get to enjoy loving her \\\n",
    "          for many year’s because her symptoms were not taken seriously ! She didn’t have to die SHE WAS MURDERED by lack \\\n",
    "          of care!!!!! As a pet mom you take your babies into veterinarians offices in hopes they will properly diagnose \\\n",
    "          and care for your kids basicaly!!! Jenuh was not cared for! This vet told me her impacted teeth (Adult teeth \\\n",
    "          came in but baby teeth didn’t fall out) wasn’t urgent she could get them out when they spayed her too which I \\\n",
    "          said I was told by other vet that I should wait between 1-1.5 years old which he blew off and said NOT TRUE! \\\n",
    "          (I read about the urgency to pull teeth online yet this was also blown off as nothing) His old school veterinary \\\n",
    "          services may actually be OUTDATED garbage! Maybe it’s time to retire!!!!',\n",
    "          \"It's a good search engine and info that shows up is mostly helpful, but lately the ads are flooding the face \\\n",
    "          page which is not always the best sites for what you are looking for.\",\n",
    "          \"I bought google play credit.I have the virtual card.But they blocked all my account transaction, when i \\\n",
    "          contacted them,they told me that they would solve it,Instead, they locked it.Even after submitting all proper \\\n",
    "          documents, they are refusing to reactive.Such bad experience\", \n",
    "          'I have had a long term rental association (as a renter) with my Property Manager Kiera Hannaford. I would like \\\n",
    "          to acknowledge her professional, friendly and thorough knowledge of all things related to rental property \\\n",
    "          management. Kiera always does her very best to help and goes to a great deal of trouble to ensure a positive \\\n",
    "          outcome with all queries and requests. Her honesty is such a valuable asset when there are challenging times. \\\n",
    "          As a renter I have always been able to rely on Kiera for help. I wish her the very best for a very bright \\\n",
    "          future and would recommend her if you ever need an excellent team member in the real estate industry. ',\n",
    "          \"It’s short from being the best because I’m concerned over some huge privacy details that may get leaked to the \\\n",
    "          greater public but otherwise it’s a good engine.\",\n",
    "          \"Always had good experience with google before but lately there's been a lot of ad popping up and not \\\n",
    "          necessarily nice sites just sites that paid money to google for google to put their sites on first page. \\\n",
    "          But google is a nice service with a lot of functions although I wonder how much personal information they \\\n",
    "          store of you in the database.\",\n",
    "          \"Kindred Healthcare physical therapists (PT) and occupational therapists (OT) in Okaloosa County, Florida, \\\n",
    "          helped me regain use of my right arm and increase my stamina and endurance despite COPD problems. During my \\\n",
    "          in-patient and outpatient therapy, they were always professional and polite while encouraging me to challenge \\\n",
    "          myself. PT and OT helped me regain independence, self-reliance, and dignity. I am grateful.\",\n",
    "          \"Had a problem. Resolved immediately. Great customer service!!!\",\n",
    "          \"had 3 hour drive ahead of me and Adam the fitter could see my distress at having to wait he jacked up the truck \\\n",
    "          in a flash and had me on my way, top sevices from Adam and the kwikfit team.\",\n",
    "          \"Online spas- Absolutely lovely guys and excellent customer service. The guys showed me how to set up and did \\\n",
    "          all the hard work for me. Nothing was any problem :). Lovely the hot tub and would highly recommend to \\\n",
    "          friends ! Thanks again!\",\n",
    "          \"Mr. Handyman is a great service. The staff isprofessional and all of the jobs get done. I have used Mr. \\\n",
    "          Handyman several times and I couldn’t be happier.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference_pipeline.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences = [inference_pipeline.run(r) for r in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Negative --- probability value 0.08%\n",
      "2. Positive --- probability value 99.87%\n",
      "3. Negative --- probability value 5.88%\n",
      "4. Negative --- probability value 0.07%\n",
      "5. Negative --- probability value 0.51%\n",
      "6. Positive --- probability value 87.73%\n",
      "7. Negative --- probability value 0.24%\n",
      "8. Positive --- probability value 99.93%\n",
      "9. Positive --- probability value 74.55%\n",
      "10. Negative --- probability value 27.76%\n",
      "11. Positive --- probability value 99.90%\n",
      "12. Positive --- probability value 98.50%\n",
      "13. Positive --- probability value 94.47%\n",
      "14. Positive --- probability value 92.99%\n",
      "15. Positive --- probability value 88.39%\n"
     ]
    }
   ],
   "source": [
    "for i,inf in enumerate(inferences):\n",
    "    print('{}. {} --- probability value {:.2%}'.format(i+1, inf[0], inf[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "# \"Suspended my account without warning and my order got delayed. I called them two days in a row to be fobbed off \\\n",
    "# and to wait for an email. No email. No access to my online account. 3 amazon prime payments taken from my card \\\n",
    "# without my permission. Just got access back to claim my money back 12 weeks later and didn't even get an apology \\\n",
    "# from the online chat. Didn't even get my order either.\" \n",
    "# 1. Negative --- probability value 3.39%    --- CORRECT\n",
    "\n",
    "# 2.\n",
    "# 'Fantastic!!!!'  \n",
    "# 2. Positive --- probability value 98.76%   --- CORRECT\n",
    "\n",
    "# 3.\n",
    "# 'The products they sent me always work fine but... I have Amazon prime and they sometimes deliver days late or \\\n",
    "# just leave the product on your doorstep out in the open easy to be stolen. Overall, my experience with Amazon is \\\n",
    "# fairly good' \n",
    "# 3. Positive --- probability value 66.64%   --- CORRECT\n",
    "\n",
    "# 4.\n",
    "# 'Safe place never used…doorstep deliveries. I am a long standing Amazon customer and high spending. I have had \\\n",
    "# a safe place named for deliveries for a long time. The odd driver hitters to open the door to the bin shed and \\\n",
    "# out the parcels in. The majority just leave it on the doorstep. With thefts high, I find this utterly ridiculous. \\\n",
    "# What is the point of a safe place if it is simply ignored and never used? I live in a house with a small front \\\n",
    "# garden - not a difficult to access apartment block or something. Amazon - please hold your drivers accountable \\\n",
    "# to the requests of customers. You offer and unique and great service but this lets you down.'\n",
    "# 4. Negative --- probability value 11.10%    --- CORRECT \n",
    "\n",
    "# 5.\n",
    "# 'My beautiful Jenuh killed by incompetence! She went to Dewey Veterinary Medical Center 3x! My MISTAKE!! I was \\\n",
    "# told she had nothing but allergies! She was filled with injections that DID NOTHING for her along with \\\n",
    "# medications that were ridiculously priced and again that did nothing!! 5 days after her last visit with Dewey \\\n",
    "# I’m in another veterinary office with her in critical condition and them saying she was on oxygen and had a very \\\n",
    "# bad infection! They’re telling me the rash on her body was NOT allergies, but was in fact signs of trouble from \\\n",
    "# infection!!! She was never treated just given more shots and new medications and here she was suffering inside! \\\n",
    "# (All I was told was many pets were coming in suffering from allergies) My 8 month beautiful blue and green-eyed \\\n",
    "# Husky died all because Dewey Vet did NOTHING for her!! She lost her life and I didn’t get to enjoy loving her \\\n",
    "# for many year’s because her symptoms were not taken seriously ! She didn’t have to die SHE WAS MURDERED by lack \\\n",
    "# of care!!!!! As a pet mom you take your babies into veterinarians offices in hopes they will properly diagnose \\\n",
    "# and care for your kids basicaly!!! Jenuh was not cared for! This vet told me her impacted teeth (Adult teeth \\\n",
    "# came in but baby teeth didn’t fall out) wasn’t urgent she could get them out when they spayed her too which I \\\n",
    "# said I was told by other vet that I should wait between 1-1.5 years old which he blew off and said NOT TRUE! \\\n",
    "# (I read about the urgency to pull teeth online yet this was also blown off as nothing) His old school veterinary \\\n",
    "# services may actually be OUTDATED garbage! Maybe it’s time to retire!!!!'\n",
    "# 5. Negative --- probability value 4.70%  --- CORRECT\n",
    "\n",
    "# 6.\n",
    "# \"It's a good search engine and info that shows up is mostly helpful, but lately the ads are flooding the face \\\n",
    "# page which is not always the best sites for what you are looking for.\"\n",
    "# 6. Positive --- probability value 93.89%  --- CORRECT \n",
    "\n",
    "#7.\n",
    "# \"I bought google play credit.I have the virtual card.But they blocked all my account transaction, when i \\\n",
    "# contacted them,they told me that they would solve it,Instead, they locked it.Even after submitting all proper \\\n",
    "# documents, they are refusing to reactive.Such bad experience\"\n",
    "# 7. Negative --- probability value 0.35%   --- CORRECT\n",
    "\n",
    "# 8.\n",
    "# 'I have had a long term rental association (as a renter) with my Property Manager Kiera Hannaford. I would like \\\n",
    "# to acknowledge her professional, friendly and thorough knowledge of all things related to rental property \\\n",
    "# management. Kiera always does her very best to help and goes to a great deal of trouble to ensure a positive \\\n",
    "# outcome with all queries and requests. Her honesty is such a valuable asset when there are challenging times. \\\n",
    "# As a renter I have always been able to rely on Kiera for help. I wish her the very best for a very bright \\\n",
    "# future and would recommend her if you ever need an excellent team member in the real estate industry. '\n",
    "# 8. Positive --- probability value 97.86%  --- CORRECT\n",
    "\n",
    "# 9.\n",
    "# \"It’s short from being the best because I’m concerned over some huge privacy details that may get leaked to the \\\n",
    "# greater public but otherwise it’s a good engine.\"\n",
    "# 9. Positive --- probability value 93.98% --- CORRECT\n",
    "\n",
    "# 10.\n",
    "# \"Always had good experience with google before but lately there's been a lot of ad popping up and not \\\n",
    "# necessarily nice sites just sites that paid money to google for google to put their sites on first page. \\\n",
    "# But google is a nice service with a lot of functions although I wonder how much personal information they \\\n",
    "# store of you in the database.\"\n",
    "# 10. Positive --- probability value 79.14%   --- CORRECT\n",
    "\n",
    "# 11.\n",
    "# \"Kindred Healthcare physical therapists (PT) and occupational therapists (OT) in Okaloosa County, Florida, \\\n",
    "# helped me regain use of my right arm and increase my stamina and endurance despite COPD problems. During my \\\n",
    "# in-patient and outpatient therapy, they were always professional and polite while encouraging me to challenge \\\n",
    "# myself. PT and OT helped me regain independence, self-reliance, and dignity. I am grateful.\"\n",
    "# 11. Positive --- probability value 94.97%   --- CORRECT\n",
    "\n",
    "# 12.\n",
    "# \"Had a problem. Resolved immediately. Great customer service!!!\"\n",
    "# 12. Positive --- probability value 79.75%   --- CORRECT \n",
    "\n",
    "# 13.\n",
    "# \"had 3 hour drive ahead of me and Adam the fitter could see my distress at having to wait he jacked up the truck \\\n",
    "# in a flash and had me on my way, top sevices from Adam and the kwikfit team.\"\n",
    "# 13. Positive --- probability value 64.67% --- CORRECT\n",
    "\n",
    "# 14.\n",
    "# \"Online spas- Absolutely lovely guys and excellent customer service. The guys showed me how to set up and did \\\n",
    "# all the hard work for me. Nothing was any problem :). Lovely the hot tub and would highly recommend to \\\n",
    "# friends ! Thanks again!\"\n",
    "# 14. Positive --- probability value 99.78%  --- CORRECT\n",
    "\n",
    "# 15.\n",
    "# \"Mr. Handyman is a great service. The staff isprofessional and all of the jobs get done. I have used Mr. \\\n",
    "# Handyman several times and I couldn’t be happier.\"\n",
    "# 15. Positive --- probability value 75.25% --- CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer",
   "language": "python",
   "name": "summer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
