{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cA9J3r2NIREU"
   },
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "The NMT method performs translation via deep learning. \n",
    "\n",
    "This methodology comprises an encoder and a decoder. It vectorizes the source sentence through the encoder, condenses the information as a context vector, and generates the translated target sentence in the decoder based on the condensed information. \n",
    "\n",
    "Methodologies utilized for NMT include recurrent neural networks (RNNs), convolutional neural networks (CNNs), and the\n",
    "Transformer model.\n",
    "The Transformer model has exhibited better performance than the other approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPZ-bSe-IREW"
   },
   "source": [
    "## Encoder-Decoder GRU & Glove Embeddings\n",
    "\n",
    "1. one vocab for english, one different vocab for french\n",
    "2. one embedding matrix for english, one different embedding matrix for french\n",
    "3. add the END token to english sentences, and french sentences\n",
    "4. the START token is only used in the decoder as the first token\n",
    "\n",
    "We pad all sequences in the batch with 0s up to the length of the longest sequence (this is a classic process in variable length batches and can you find plenty of posts on this subject online). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53PWBj9cIREX"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:06.604872Z",
     "iopub.status.busy": "2021-10-22T17:49:06.604060Z",
     "iopub.status.idle": "2021-10-22T17:49:06.608910Z",
     "shell.execute_reply": "2021-10-22T17:49:06.607792Z",
     "shell.execute_reply.started": "2021-10-22T17:49:06.604818Z"
    },
    "id": "CrlsE9C1anaV",
    "outputId": "a0065e83-099b-4b15-b15f-046f60e2e2e7"
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:06.650791Z",
     "iopub.status.busy": "2021-10-22T17:49:06.650474Z",
     "iopub.status.idle": "2021-10-22T17:49:06.654201Z",
     "shell.execute_reply": "2021-10-22T17:49:06.653487Z",
     "shell.execute_reply.started": "2021-10-22T17:49:06.650748Z"
    },
    "id": "aPodUm6HanyA",
    "outputId": "aca92d47-ac38-40f6-a942-b5970df2f0b3"
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download fr_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:06.656641Z",
     "iopub.status.busy": "2021-10-22T17:49:06.656141Z",
     "iopub.status.idle": "2021-10-22T17:49:06.665888Z",
     "shell.execute_reply": "2021-10-22T17:49:06.664758Z",
     "shell.execute_reply.started": "2021-10-22T17:49:06.656604Z"
    },
    "id": "Cp3rnrdfIREX"
   },
   "outputs": [],
   "source": [
    "import os                                 # to create 'serialised' directory\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import unicodedata                        # for data cleaning\n",
    "import spacy                              # for tokenisation\n",
    "from collections import Counter           # this is for tokens dictionary function\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "from torch.nn.utils import clip_grad_value_, clip_grad_norm_\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:06.667808Z",
     "iopub.status.busy": "2021-10-22T17:49:06.667295Z",
     "iopub.status.idle": "2021-10-22T17:49:06.678978Z",
     "shell.execute_reply": "2021-10-22T17:49:06.678165Z",
     "shell.execute_reply.started": "2021-10-22T17:49:06.667768Z"
    },
    "id": "ygUBbPPKIREZ"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_row', None)              # show all rows of a dataframe\n",
    "pd.set_option('display.max_column', None)           # show all columns of a dataframe\n",
    "pd.set_option('display.max_colwidth', None)         # show the full width of columns\n",
    "pd.set_option('precision', 2)                       # round to 2 decimal points\n",
    "pd.options.display.float_format = '{:,.2f}'.format  # comma separators and two decimal points: 4756.7890 => 4,756.79 and 4656 => 4,656.00\n",
    "torch.set_printoptions(profile='full')              # prints the whole tensor\n",
    "# torch.set_printoptions(profile=\"default\")         # reset to printing the truncated tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "284-5u_ZKCEm"
   },
   "source": [
    "Import Spacy and Load the languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:06.681012Z",
     "iopub.status.busy": "2021-10-22T17:49:06.680437Z",
     "iopub.status.idle": "2021-10-22T17:49:16.067982Z",
     "shell.execute_reply": "2021-10-22T17:49:16.067184Z",
     "shell.execute_reply.started": "2021-10-22T17:49:06.680975Z"
    },
    "id": "OBK14hYuIREZ",
    "outputId": "86cc0677-5cd8-4bf7-d7ec-b510f0e79605"
   },
   "outputs": [],
   "source": [
    "S_TOK = spacy.load('en_core_web_lg')                                         # for tokenisation\n",
    "T_TOK = spacy.load('fr_core_news_lg')                                        # for tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.071318Z",
     "iopub.status.busy": "2021-10-22T17:49:16.070771Z",
     "iopub.status.idle": "2021-10-22T17:49:16.075398Z",
     "shell.execute_reply": "2021-10-22T17:49:16.074674Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.071276Z"
    },
    "id": "3-Io7AnFcPRa"
   },
   "outputs": [],
   "source": [
    "batch_size = 32                                                             # for dataloader and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.077289Z",
     "iopub.status.busy": "2021-10-22T17:49:16.076798Z",
     "iopub.status.idle": "2021-10-22T17:49:16.089731Z",
     "shell.execute_reply": "2021-10-22T17:49:16.089089Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.077251Z"
    },
    "id": "Tc2WHPeDIREa",
    "outputId": "0ee93420-b2ba-41b9-a3b9-bff417f8ec81"
   },
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.093368Z",
     "iopub.status.busy": "2021-10-22T17:49:16.092938Z",
     "iopub.status.idle": "2021-10-22T17:49:16.099750Z",
     "shell.execute_reply": "2021-10-22T17:49:16.099068Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.093340Z"
    },
    "id": "VRY4yI2VIREb"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    torch.cuda.get_device_name(0)\n",
    "except:\n",
    "    print('no cuda available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.101909Z",
     "iopub.status.busy": "2021-10-22T17:49:16.101525Z",
     "iopub.status.idle": "2021-10-22T17:49:16.112067Z",
     "shell.execute_reply": "2021-10-22T17:49:16.111154Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.101854Z"
    },
    "id": "S0Si9cNUIREc",
    "outputId": "c3c827b9-c140-4bad-a44e-da1533e6148e"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')      # pytorch cuda\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKJqZJMbIREd"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Gzfl_oSIREd"
   },
   "source": [
    "### Random Seeds\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/randomness.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.114013Z",
     "iopub.status.busy": "2021-10-22T17:49:16.113501Z",
     "iopub.status.idle": "2021-10-22T17:49:16.119870Z",
     "shell.execute_reply": "2021-10-22T17:49:16.119213Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.113973Z"
    },
    "id": "4oTkGxsdIREd"
   },
   "outputs": [],
   "source": [
    "seed_value = 9\n",
    "\n",
    "def random_seeding(seed_value=seed_value):\n",
    "    random.seed(seed_value)                            # python \n",
    "    np.random.seed(seed_value)                         # numpy - global seeding. Sklearn uses this internally therefore there is no need to set a random seed when using Sklearn \n",
    "    torch.manual_seed(seed_value)                      # pytorch cpu\n",
    "#     torch.set_deterministic(True)                    # this raises an error when running the decoder                                       \n",
    "    try:\n",
    "        if device=='cuda': \n",
    "            torch.cuda.manual_seed_all(seed_value)     # pytorch gpu\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lu6_maE-IREe"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.121794Z",
     "iopub.status.busy": "2021-10-22T17:49:16.121335Z",
     "iopub.status.idle": "2021-10-22T17:49:16.132746Z",
     "shell.execute_reply": "2021-10-22T17:49:16.132098Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.121757Z"
    },
    "id": "XAfCkH4PIREe"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII: e.g. 'gar√ßon' to 'garcon'\n",
    "def unicodeToAscii(text):\n",
    "    return ''.join(char for char in unicodedata.normalize('NFD', text) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "# Clean sentences\n",
    "def clean_tokenise(text, TOK):                            # to be used when tokenizing with spacy\n",
    "    text = text.lower()\n",
    "    text = unicodeToAscii(text)\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)              # r\" \\1\" : adds a white space before whatever is referenced in the group r\"()\" in the first part of the regex\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)           # remove non-letter characters, e.g. \"he's\" becomes \"he s\"\n",
    "    text = [token.text for token in TOK.tokenizer(text)]   # for tokenization with spacy\n",
    "    text.append('<EOS>')\n",
    "    text.insert(0, '<SOS>')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n0RY4s4IREf"
   },
   "source": [
    "### Vocabulary from the Traing Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.134946Z",
     "iopub.status.busy": "2021-10-22T17:49:16.134753Z",
     "iopub.status.idle": "2021-10-22T17:49:16.144244Z",
     "shell.execute_reply": "2021-10-22T17:49:16.143561Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.134923Z"
    },
    "id": "Cb9TTOT2IREf"
   },
   "outputs": [],
   "source": [
    "def tokens_dict(series):\n",
    "    tokens = series.explode()\n",
    "    tokens = tokens.tolist()\n",
    "    tokens = Counter(tokens)                                    # frequency distribution. This basically is a dictionary\n",
    "\n",
    "    unique_words_before = len(tokens.keys())\n",
    "    \n",
    "    #To avoid 'RuntimeError: dictionary changed size during iteration' error, we need to make a .copy() of the dictionary.\n",
    "    #This way we iterate over the original dictionary keys and delete elements on the fly.\n",
    "    for k,v in tokens.copy().items():\n",
    "        if v < 3:\n",
    "            del tokens[k]\n",
    "\n",
    "    tokens_dictionary = {word: i+1 for i,word in enumerate(tokens.keys())}   # word2index dictionary. The '+1' is to avoid the first word in the vocabulary to have index=0 because this will be reserved for padding (see comments in 'make_embedding_matrix())\n",
    "    \n",
    "    unique_words_after = len(tokens_dictionary.keys())\n",
    "    \n",
    "#     tokens_dictionary['<SOS>'] = len(tokens_dictionary)+1\n",
    "    tokens_dictionary['<UNK>'] = len(tokens_dictionary)+1\n",
    "    tokens_dictionary[''] = len(tokens_dictionary)+1\n",
    "    \n",
    "    final_unique_words = len(tokens_dictionary.keys())\n",
    "    \n",
    "    reversed_dict = {int(v):k for k,v in tokens_dictionary.items()}\n",
    "    \n",
    "    return unique_words_before, unique_words_after, final_unique_words, tokens_dictionary, reversed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s72XQN50IREf"
   },
   "source": [
    "### All Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.145794Z",
     "iopub.status.busy": "2021-10-22T17:49:16.145447Z",
     "iopub.status.idle": "2021-10-22T17:49:16.154798Z",
     "shell.execute_reply": "2021-10-22T17:49:16.154027Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.145758Z"
    },
    "id": "9CoYSvQ9IREf"
   },
   "outputs": [],
   "source": [
    "def load_glove_from_file(glove_filepath):\n",
    "    \n",
    "    glove_embeddings_dict = {} \n",
    "    \n",
    "    with open(glove_filepath, mode='r', encoding=\"utf-8\") as f:\n",
    "        for index,line in enumerate(f):\n",
    "            line_split = line.split()\n",
    "            word = line_split[0]\n",
    "            glove_embeddings_dict[word] = np.array(line_split[1:], 'float32')\n",
    "    \n",
    "    return glove_embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UDhZUSbIREf"
   },
   "source": [
    "### Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.156685Z",
     "iopub.status.busy": "2021-10-22T17:49:16.156174Z",
     "iopub.status.idle": "2021-10-22T17:49:16.164532Z",
     "shell.execute_reply": "2021-10-22T17:49:16.163739Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.156646Z"
    },
    "id": "Zvhj_hUoIREg"
   },
   "outputs": [],
   "source": [
    "def make_embedding_matrix(glove_embeddings_dict, tokens_dictionary):  \n",
    "    \n",
    "    embedding_size = len(next(iter(glove_embeddings_dict.values())))          # length of each word embedding (e.g.300 dimensions)\n",
    "    \n",
    "    embeddings_matrix = np.zeros((len(tokens_dictionary)+1, embedding_size))  # len(tokens_dictionary) is the length of the vocabulary (i.e. unique words in the corpus)\n",
    "                                                                              # the '+1' after len(tokens_dictionary) is necessary because we want the first embedding (i.e. at index 0) to be a zero vector; this will be used for the paddings (we want all the '0' paddings to be paired with the zero vector)\n",
    "    not_in_glove = []\n",
    "    for token,i in tokens_dictionary.items():\n",
    "        if token in glove_embeddings_dict:\n",
    "            embeddings_matrix[i, :] = glove_embeddings_dict[token]     \n",
    "        else:\n",
    "            not_in_glove.append(token)\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)                       # if word is not in Glove emebeddings, creates a new vector with random numbers drawn from pytorch xavier_uniform distribution\n",
    "            embeddings_matrix[i, :] = embedding_i                \n",
    "\n",
    "    return not_in_glove, embeddings_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Db7gul6IREg"
   },
   "source": [
    "### Encoding & Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.168900Z",
     "iopub.status.busy": "2021-10-22T17:49:16.168499Z",
     "iopub.status.idle": "2021-10-22T17:49:16.176260Z",
     "shell.execute_reply": "2021-10-22T17:49:16.175527Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.168850Z"
    },
    "id": "Kmk3hvfUIREg"
   },
   "outputs": [],
   "source": [
    "def encoding(input_sequence, tokens_dictionary, max_len):\n",
    "    encoded = np.zeros(max_len, dtype=int)\n",
    "    \n",
    "    encoded_lst = []\n",
    "    if not input_sequence:\n",
    "        encoded_lst.append(tokens_dictionary[''])\n",
    "    else:\n",
    "        for word in input_sequence:\n",
    "            encoded_lst.append(tokens_dictionary.get(word, tokens_dictionary['<UNK>']))\n",
    "            \n",
    "    encoded_lst = np.array(encoded_lst)\n",
    "    \n",
    "    length = min(max_len, len(encoded_lst))\n",
    "    encoded[:length] = encoded_lst[:length]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beq7-YKCIREg"
   },
   "source": [
    "### Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.178175Z",
     "iopub.status.busy": "2021-10-22T17:49:16.177910Z",
     "iopub.status.idle": "2021-10-22T17:49:16.194333Z",
     "shell.execute_reply": "2021-10-22T17:49:16.193537Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.178141Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_dim, emb_num, hidden_size, num_layers, dropout_p, batch_first=True, bias=True,\n",
    "                 bidirectional=True, pretrained_embeddings=None, padding_idx=0):  \n",
    "        \n",
    "        # pretrained_embeddings = embedding_matrix;\n",
    "        # padding_idx=0 makes sure that the padding vector (which in our embedding matrix is at index 0) \n",
    "        # doesn't get updated during training when Freeze=False\n",
    "        \n",
    "        super(Encoder, self).__init__()                      \n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_num = emb_num\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.bidirectional = bidirectional\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        self.D = 1\n",
    "        if self.bidirectional==True:\n",
    "            self.D = 2\n",
    "            \n",
    "        # Initialise embedding layer    \n",
    "        if self.pretrained_embeddings is None:\n",
    "\n",
    "            self.emb = nn.Embedding(num_embeddings=self.emb_num,\n",
    "                                    embedding_dim=self.emb_dim,\n",
    "                                    padding_idx=self.padding_idx)      \n",
    "        else:\n",
    "            self.pretrained_embeddings = torch.from_numpy(self.pretrained_embeddings).float()\n",
    "            # freeze=False : the tensor does not get updated in the learning process. \n",
    "            #Equivalent to self.emb.weight.requires_grad = False\n",
    "            self.emb = nn.Embedding.from_pretrained(self.pretrained_embeddings, freeze=True, padding_idx=self.padding_idx)  \n",
    "            \n",
    "#             self.emb = nn.Embedding(embedding_dim=self.emb_dim,\n",
    "#                                     num_embeddings=self.emb_num,\n",
    "#                                     padding_idx=self.padding_idx,\n",
    "#                                     _weight=self.pretrained_embeddings)\n",
    "            \n",
    "            \n",
    "            # nn.Embedding is a model parameter layer, which is by default trainable.\n",
    "            # If you want to fine-tune word vectors during training, these word vectors are treated as model parameters \n",
    "            # and are updated by backpropagation. You can also make it untrainable by freezing its gradient \n",
    "            # (False ==> freezes the backprop) \n",
    "#             self.emb.weight.requires_grad=False    \n",
    "        \n",
    "        \n",
    "        # Initialise GRU model\n",
    "        self.gru = nn.GRU(input_size=self.emb_dim, hidden_size=self.hidden_size, num_layers=self.num_layers, \n",
    "                          dropout=self.dropout_p, bidirectional=self.bidirectional, bias=self.bias, \n",
    "                          batch_first=self.batch_first) \n",
    "        \n",
    "        # Regularization parameter\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):        \n",
    "        # length of each sequence in the batch, ignoring the padding. This is needed for Packed Sequence\n",
    "        lengths = ((x > 0)*1).sum(dim=1)    \n",
    "        lengths = lengths.cpu()\n",
    "        \n",
    "        # input_ is a list of lists. It comes from the train_loader. E.g. [[3386, 603, 1112, 0],\n",
    "        #                                                                  [176, 40, 97, 0]]\n",
    "        embedding = self.dropout(self.emb(x.long()))\n",
    "\n",
    "        batch_size = embedding.shape[0]\n",
    "\n",
    "        # hidden_state = torch.randn(self.D*self.num_layers, batch_size, self.hidden_size)\n",
    "        hidden_state = torch.zeros(self.D*self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        packed = pack_padded_sequence(embedding, lengths, batch_first=self.batch_first, enforce_sorted=False)\n",
    "        packed_output, hidden_state = self.gru(packed, hidden_state)\n",
    "        \n",
    "        unpacked_out, unpacked_lens = pad_packed_sequence(packed_output)\n",
    "        outputs = unpacked_out.permute(1, 2, 0)\n",
    "        \n",
    "        hidden_state = hidden_state[-1,:,:].unsqueeze(0)           # this extract the last hidden state in case of multiple GRU layers\n",
    "        outputs = outputs[:, -self.hidden_size:, :]      # this extract the bottom hidden states in case of bidirectional GRUs\n",
    "        outputs = outputs.permute(2,0,1)\n",
    "        \n",
    "        return outputs, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.196219Z",
     "iopub.status.busy": "2021-10-22T17:49:16.195703Z",
     "iopub.status.idle": "2021-10-22T17:49:16.215493Z",
     "shell.execute_reply": "2021-10-22T17:49:16.214748Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.196183Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder_Bahdanau_GRU(nn.Module):\n",
    "    def __init__(self, emb_dim, emb_num, encoder_hidden_size, decoder_hidden_size, \n",
    "                 v_dim=3, num_layers=1, dropout_p=0.1, batch_first=True, bias=True,\n",
    "                 bidirectional=False, pretrained_embeddings=None, padding_idx=0):\n",
    "    \n",
    "        super(Decoder_Bahdanau_GRU, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_num = emb_num\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.v_dim = v_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.bidirectional = bidirectional\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        self.D = 1\n",
    "        if self.bidirectional==True:\n",
    "            self.D = 2\n",
    "        \n",
    "        # Initialise Embedding Layer\n",
    "        if self.pretrained_embeddings is None:\n",
    "\n",
    "            self.emb = nn.Embedding(embedding_dim=self.emb_dim,\n",
    "                                    num_embeddings=self.emb_num,\n",
    "                                    padding_idx=self.padding_idx)     \n",
    "        else:\n",
    "            self.pretrained_embeddings = torch.from_numpy(self.pretrained_embeddings).float()\n",
    "            # freeze=False : the tensor does not get updated in the learning process.\n",
    "            # Equivalent to self.emb.weight.requires_grad = False\n",
    "            self.emb = nn.Embedding.from_pretrained(self.pretrained_embeddings, freeze=True, padding_idx=self.padding_idx)\n",
    "        \n",
    "        # Initialise Attention\n",
    "        self.v = nn.Linear(self.v_dim, 1, bias=False)\n",
    "        self.w1 = nn.Linear(self.decoder_hidden_size, self.v_dim, bias=False)                # decoder_hidden_state\n",
    "        self.w2 = nn.Linear(self.encoder_hidden_size, self.v_dim, bias=False)                # encoder_output\n",
    "        \n",
    "        # Initialise GRU\n",
    "        self.gru = nn.GRU(input_size=self.emb_dim+self.decoder_hidden_size, hidden_size=self.decoder_hidden_size, \n",
    "                          num_layers=self.num_layers, dropout=self.dropout_p, bidirectional=self.bidirectional, \n",
    "                          bias=self.bias, batch_first=self.batch_first)\n",
    "        \n",
    "        # Initialise Linear Layer\n",
    "        self.fc1 = nn.Linear(self.decoder_hidden_size, self.emb_num, bias=False)\n",
    "        \n",
    "        # Regularization parameter\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "\n",
    "# input_ is a tensor of shape [batch_size, 1]; for instance if batch_size=3: [[1.],\n",
    "#                                                                             [1.],\n",
    "#                                                                             [1.]]\n",
    "    def forward(self, x, hidden_state, output):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.emb(x.long())\n",
    "        \n",
    "        w1h = self.w1(hidden_state)\n",
    "        w1h = w1h.permute(1,0,2)\n",
    "        w2eo = self.w2(output.permute(1,0,2)) \n",
    "        sum_ = w2eo + w1h.expand_as(w2eo)\n",
    "        tanh_ = torch.tanh(sum_)\n",
    "        alignment_scores = self.v(tanh_)\n",
    "        \n",
    "        # Mask alignment_scores so that the softmax calculation to get the attn_weights will ingnore the paddings\n",
    "        mask = (alignment_scores == 0) * 1\n",
    "        mask = mask.bool()\n",
    "        masked_alignments = alignment_scores.masked_fill(mask=mask, value=-np.inf)\n",
    "        \n",
    "        attn_weights = F.softmax(masked_alignments, dim=1)                # this is the normal softmax \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vector = torch.matmul(output.permute(1,2,0), attn_weights)\n",
    "        # context_vector = torch.bmm(output.permute(1,2,0), attn_weights)  #equivalent to line above\n",
    "        \n",
    "        concat = torch.cat((embedding.permute(1,2,0), context_vector), dim=1)\n",
    "        concat = concat.permute(0,2,1)\n",
    "        \n",
    "        # Cannot use packing in the decoder because tokens are processed one at a time\n",
    "        outputs, hidden_state = self.gru(concat, hidden_state)\n",
    "\n",
    "        predictions = self.fc1(outputs)        \n",
    "        predictions = predictions.squeeze(1)\n",
    "\n",
    "        return predictions, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.217263Z",
     "iopub.status.busy": "2021-10-22T17:49:16.216998Z",
     "iopub.status.idle": "2021-10-22T17:49:16.233628Z",
     "shell.execute_reply": "2021-10-22T17:49:16.232921Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.217214Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder_dot_GRU(nn.Module):\n",
    "    def __init__(self, emb_dim, emb_num, encoder_hidden_size, decoder_hidden_size, num_layers=1, dropout_p=0.1, \n",
    "                 batch_first=True, bias=True, bidirectional=False, pretrained_embeddings=None, padding_idx=0):\n",
    "    \n",
    "        super(Decoder_dot_GRU, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_num = emb_num\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.bidirectional = bidirectional\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        self.D = 1\n",
    "        if self.bidirectional==True:\n",
    "            self.D = 2\n",
    "        \n",
    "        # Initialise Embedding Layer\n",
    "        if self.pretrained_embeddings is None:\n",
    "\n",
    "            self.emb = nn.Embedding(embedding_dim=self.emb_dim,\n",
    "                                    num_embeddings=self.emb_num,\n",
    "                                    padding_idx=self.padding_idx)     \n",
    "        else:\n",
    "            self.pretrained_embeddings = torch.from_numpy(self.pretrained_embeddings).float()\n",
    "            # freeze=False : the tensor does not get updated in the learning process.\n",
    "            # Equivalent to self.emb.weight.requires_grad = False\n",
    "            self.emb = nn.Embedding.from_pretrained(self.pretrained_embeddings, freeze=True, padding_idx=self.padding_idx)\n",
    "        \n",
    "        # Initialise GRU\n",
    "        self.gru = nn.GRU(input_size=self.emb_dim, hidden_size=self.decoder_hidden_size, num_layers=self.num_layers,\n",
    "                          dropout=self.dropout_p, bidirectional=self.bidirectional, bias=self.bias,\n",
    "                          batch_first=self.batch_first)\n",
    "        \n",
    "        # Initialise Linear Layer\n",
    "        self.fc1 = nn.Linear(self.encoder_hidden_size+self.decoder_hidden_size, self.emb_num, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "            \n",
    "# input_ is a tensor of shape [batch_size, 1]; for instance if batch_size=3: [[1.],\n",
    "#                                                                             [1.],\n",
    "#                                                                             [1.]]\n",
    "    def forward(self, x, hidden_state, output):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.emb(x.long())\n",
    "        \n",
    "        # Cannot use packing in the decoder because tokens are processed one at a time\n",
    "        doutput, hidden_state = self.gru(embedding.permute(1,0,2), hidden_state)\n",
    "        \n",
    "        # Attention scores\n",
    "        #.matmul works on matrix-vector multiplication, whereas .bmm only works on matrix-matrix multiplications\n",
    "        alignment_scores = torch.matmul(hidden_state.permute(1,0,2), output.permute(1,2,0))\n",
    "        alignment_scores = alignment_scores.permute(0,2,1)\n",
    "        \n",
    "        # Mask alignment_scores so that the softmax calculation to get the attn_weights will ingnore the paddings\n",
    "        mask = (alignment_scores == 0) * 1\n",
    "        mask = mask.bool()\n",
    "        masked_alignments = alignment_scores.masked_fill(mask=mask, value=-np.inf)\n",
    "         \n",
    "        attn_weights = F.softmax(masked_alignments, dim=1)                # this is the normal softmax \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vector = torch.matmul(output.permute(1,2,0), attn_weights)\n",
    "        concat = torch.cat((hidden_state.permute(1,2,0), context_vector), dim=1)\n",
    "        concat = concat.permute(0,2,1)\n",
    "        \n",
    "        predictions = self.fc1(concat.squeeze(0))\n",
    "        predictions = predictions.squeeze(1)\n",
    "\n",
    "        return predictions, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.235898Z",
     "iopub.status.busy": "2021-10-22T17:49:16.235219Z",
     "iopub.status.idle": "2021-10-22T17:49:16.253352Z",
     "shell.execute_reply": "2021-10-22T17:49:16.252676Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.235835Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder_Luong_GRU(nn.Module):\n",
    "    def __init__(self, emb_dim, emb_num, encoder_hidden_size, decoder_hidden_size, num_layers=1, dropout_p=0.1, \n",
    "                 batch_first=True, bias=True, bidirectional=False, pretrained_embeddings=None, padding_idx=0):\n",
    "    \n",
    "        super(Decoder_Luong_GRU, self).__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_num = emb_num\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.bidirectional = bidirectional\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        self.D = 1\n",
    "        if self.bidirectional==True:\n",
    "            self.D = 2\n",
    "        \n",
    "        # Initialise Embedding Layer\n",
    "        if self.pretrained_embeddings is None:\n",
    "\n",
    "            self.emb = nn.Embedding(embedding_dim=self.emb_dim,\n",
    "                                    num_embeddings=self.emb_num,\n",
    "                                    padding_idx=self.padding_idx)     \n",
    "        else:\n",
    "            self.pretrained_embeddings = torch.from_numpy(self.pretrained_embeddings).float()\n",
    "            # freeze=False : the tensor does not get updated in the learning process.\n",
    "            # Equivalent to self.emb.weight.requires_grad = False\n",
    "            self.emb = nn.Embedding.from_pretrained(self.pretrained_embeddings, freeze=True, padding_idx=self.padding_idx)\n",
    "        \n",
    "        # Initialise GRU\n",
    "        self.gru = nn.GRU(input_size=self.emb_dim, hidden_size=self.decoder_hidden_size, num_layers=self.num_layers,\n",
    "                          dropout=self.dropout_p, bidirectional=self.bidirectional, bias=self.bias,\n",
    "                          batch_first=self.batch_first)\n",
    "        \n",
    "        # Initialise Attention\n",
    "        self.w = nn.Linear(self.decoder_hidden_size, self.encoder_hidden_size, bias=False)\n",
    "        \n",
    "        # Initialise Linear Layer\n",
    "        self.fc1 = nn.Linear(self.encoder_hidden_size+self.decoder_hidden_size, self.emb_num, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "            \n",
    "# input_ is a tensor of shape [batch_size, 1]; for instance if batch_size=3: [[1.],\n",
    "#                                                                             [1.],\n",
    "#                                                                             [1.]]\n",
    "    def forward(self, x, hidden_state, output):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.emb(x.long())\n",
    "        \n",
    "        # Cannot use packing in the decoder because tokens are processed one at a time\n",
    "        doutput, hidden_state = self.gru(embedding.permute(1,0,2), hidden_state)\n",
    "        \n",
    "        # Attention scores\n",
    "        attn_hidden = self.w(hidden_state)\n",
    "        alignment_scores = torch.matmul(attn_hidden.permute(1,0,2), output.permute(1,2,0))\n",
    "        alignment_scores = alignment_scores.permute(0,2,1)\n",
    "        \n",
    "        # Mask alignment_scores so that the softmax calculation to get the attn_weights will ingnore the paddings\n",
    "        mask = (alignment_scores == 0) * 1\n",
    "        mask = mask.bool()\n",
    "        masked_alignments = alignment_scores.masked_fill(mask=mask, value=-np.inf)\n",
    "        \n",
    "        \n",
    "        attn_weights = F.softmax(masked_alignments, dim=1)                # this is the normal softmax \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vector = torch.matmul(output.permute(1,2,0), attn_weights)\n",
    "        concat = torch.cat((hidden_state.permute(1,2,0), context_vector), dim=1)\n",
    "        concat = concat.permute(0,2,1)\n",
    "        \n",
    "        predictions = self.fc1(concat.squeeze(0))\n",
    "        predictions = predictions.squeeze(1)\n",
    "\n",
    "        return predictions, hidden_state        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.256293Z",
     "iopub.status.busy": "2021-10-22T17:49:16.256104Z",
     "iopub.status.idle": "2021-10-22T17:49:16.265308Z",
     "shell.execute_reply": "2021-10-22T17:49:16.264579Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.256269Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder, target_vocab_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        \n",
    "    def forward(self, source, target, tfr=0.5):\n",
    "        batch_size, target_len = target.shape\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, self.target_vocab_size).to(device)\n",
    "\n",
    "        e_output, hidden_state = self.Encoder(source)\n",
    "\n",
    "        x = target[:,0] # Trigger token <SOS>\n",
    "\n",
    "        for i in range(1, target_len):\n",
    "            d_output, hidden_state = self.Decoder(x, hidden_state, e_output)\n",
    "            outputs[i] = d_output\n",
    "            best_guess = d_output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
    "            x = target[:,i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.267082Z",
     "iopub.status.busy": "2021-10-22T17:49:16.266453Z",
     "iopub.status.idle": "2021-10-22T17:49:16.277599Z",
     "shell.execute_reply": "2021-10-22T17:49:16.276917Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.267044Z"
    }
   },
   "outputs": [],
   "source": [
    "def params(full_embedding_layer_name):\n",
    "    for param in full_embedding_layer_name.parameters():\n",
    "        return param\n",
    "\n",
    "\n",
    "def check_params(classifier_name):\n",
    "    for name, child in classifier_name.named_children():\n",
    "        print('Layer name: {} --- {}'.format(name, child), end='\\n\\n')            \n",
    "        print('ToT Params: {:,}'. format(sum(p.numel() for p in child.parameters())), end='\\n\\n') \n",
    "    \n",
    "        count = 0\n",
    "        for param in child.parameters():\n",
    "            print('Param length: {:,}'.format(len(param)), end='\\n\\n')\n",
    "            print(param, end='\\n\\n')\n",
    "            print('Are parameters being updated during backprop? {}'.format(param.requires_grad), end='\\n\\n')\n",
    "            count += 1\n",
    "\n",
    "        print('Total Sets of Parameters: {}'.format(count), end='\\n\\n')\n",
    "        print('*' * 90)\n",
    "    \n",
    "\n",
    "def num_params(classifier_name):\n",
    "    \n",
    "    # PyTorch torch.numel() method returns the total number of elements in the input tensor\n",
    "    trainable_parameters = sum(param.numel() for param in classifier_name.parameters() if param.requires_grad)  \n",
    "    all_parameters = sum(param.numel() for param in classifier_name.parameters())  \n",
    "    \n",
    "    return trainable_parameters, all_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.279452Z",
     "iopub.status.busy": "2021-10-22T17:49:16.278902Z",
     "iopub.status.idle": "2021-10-22T17:49:16.289499Z",
     "shell.execute_reply": "2021-10-22T17:49:16.288819Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.279414Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate(encoded_s, max_length, tgt_vocab_w2i, tgt_vocab_i2w):\n",
    "    # Convert to tensor\n",
    "    tensor = torch.Tensor(encoded_s)\n",
    "    tensor = tensor.unsqueeze(0).long().to(device)\n",
    "#     print('tensor shape: {}'.format(tensor.shape))\n",
    "#     print('tensor: {}'.format(tensor))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outp, hidden = model.Encoder(tensor)\n",
    "        \n",
    "    outputs = torch.tensor(tgt_vocab_w2i['<SOS>']).reshape(1, 1).to(device)\n",
    "#     print('outputs shape: {}'.format(outputs.shape))\n",
    "#     print('outputs: {}'.format(outputs))\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        previous_word = outputs[:,-1]\n",
    "#         print('previous_word: {}'.format(previous_word))\n",
    "        output, hidden = model.Decoder(previous_word, hidden, outp)\n",
    "        best_guess = output.argmax(1).item()\n",
    "#         print('best_guess: {}'.format(best_guess))\n",
    "        best_guess = torch.tensor(best_guess).reshape(1,1).to(device)\n",
    "#         print('best_guess shape: {}'.format(best_guess.shape))\n",
    "#         print('best_guess: {}'.format(best_guess))\n",
    "        \n",
    "        outputs = torch.cat((outputs, best_guess), 1)\n",
    "#         print('outputs type: {}'.format(type(outputs)))\n",
    "#         print('outputs shape: {}'.format(outputs.shape))\n",
    "#         print('outputs: {}'.format(outputs))\n",
    "#         print()\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if best_guess == tgt_vocab_w2i['<EOS>']:\n",
    "            break\n",
    "            \n",
    "    outputs = outputs.squeeze(0)\n",
    "#     print('outputs type: {}'.format(type(outputs)))\n",
    "#     print('outputs shape: {}'.format(outputs.shape))\n",
    "#     print('outputs: {}'.format(outputs))\n",
    "#     print()\n",
    "    \n",
    "    outputs = outputs.tolist()\n",
    "#     print('outputs type: {}'.format(type(outputs)))\n",
    "#     print('outputs: {}'.format(outputs))\n",
    "#     print()\n",
    "    \n",
    "    translated_sentence = [tgt_vocab_i2w[idx] for idx in outputs]\n",
    "    return translated_sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.291372Z",
     "iopub.status.busy": "2021-10-22T17:49:16.290894Z",
     "iopub.status.idle": "2021-10-22T17:49:16.300454Z",
     "shell.execute_reply": "2021-10-22T17:49:16.299723Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.291317Z"
    }
   },
   "outputs": [],
   "source": [
    "def translation_e2e(sentences, max_length):\n",
    "    if type(sentences) != pd.core.series.Series:\n",
    "        # 1. Convert list to pandas series\n",
    "        sentences = pd.Series(sentences)\n",
    "    \n",
    "    # 2. Clean sentences\n",
    "    sentences_clean = sentences.map(lambda x: clean_tokenise(text=x, TOK=S_TOK))\n",
    "\n",
    "    # 3. Encode sentences\n",
    "    sentences_encoded = sentences_clean.map(lambda x: encoding(input_sequence=x, \n",
    "                                                               tokens_dictionary=X_dict_w2i, \n",
    "                                                               max_len=s_max_len))\n",
    "    \n",
    "    # 4. Translate\n",
    "    translations = sentences_encoded.map(lambda x: translate(encoded_s=x, \n",
    "                                                             max_length=max_length,\n",
    "                                                             tgt_vocab_w2i=y_dict_w2i,\n",
    "                                                             tgt_vocab_i2w=y_dict_i2w))\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.302401Z",
     "iopub.status.busy": "2021-10-22T17:49:16.301927Z",
     "iopub.status.idle": "2021-10-22T17:49:16.310558Z",
     "shell.execute_reply": "2021-10-22T17:49:16.309801Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.302354Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer(source_samples, max_length=20):\n",
    "    if type(source_samples) != pd.core.series.Series:\n",
    "        source_samples = pd.Series(source_samples)\n",
    "        \n",
    "    translations = translation_e2e(source_samples, max_length=max_length)\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.313748Z",
     "iopub.status.busy": "2021-10-22T17:49:16.312993Z",
     "iopub.status.idle": "2021-10-22T17:49:16.320356Z",
     "shell.execute_reply": "2021-10-22T17:49:16.319733Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.313711Z"
    }
   },
   "outputs": [],
   "source": [
    "def bleu(source_samples, references_corpus, max_length=20, TOK=T_TOK):\n",
    "    candidate_corpus = infer(source_samples, max_length=max_length)\n",
    "    candidate_corpus = candidate_corpus.map(lambda x: x[:-1])\n",
    "    \n",
    "    references_corpus = references_corpus.map(lambda x: clean_tokenise(text=x, TOK=TOK))\n",
    "    references_corpus = references_corpus.map(lambda x: x[1:-1])\n",
    "    \n",
    "    bleu = bleu_score(candidate_corpus, references_corpus)\n",
    "    \n",
    "    return candidate_corpus, references_corpus, bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhumpOtAIREl"
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC4OBnz5IREl"
   },
   "source": [
    "### Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.322137Z",
     "iopub.status.busy": "2021-10-22T17:49:16.321628Z",
     "iopub.status.idle": "2021-10-22T17:49:16.328787Z",
     "shell.execute_reply": "2021-10-22T17:49:16.328137Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.322097Z"
    },
    "id": "iXALO_JLIREl"
   },
   "outputs": [],
   "source": [
    "random_seeding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtShpHLrIREm"
   },
   "source": [
    "### Read / Split / Clean Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.330764Z",
     "iopub.status.busy": "2021-10-22T17:49:16.330140Z",
     "iopub.status.idle": "2021-10-22T17:49:16.681190Z",
     "shell.execute_reply": "2021-10-22T17:49:16.680426Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.330728Z"
    },
    "id": "_gIusSL4OYBg",
    "outputId": "52d38fd7-f02a-4745-bc88-330cb4d68da9"
   },
   "outputs": [],
   "source": [
    "dir_csv = '../input/simplest-eng-fra/simplest_eng_fra.csv'  # Kaggle directory\n",
    "dir_txt = '../input/engfra/eng-fra.txt'                     # Kaggle directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cwd = os.getcwd()  # get current directory on CPU (i.e. where the notebook is saved)\n",
    "# print(cwd)\n",
    "\n",
    "# dir_csv = cwd + 'data\\simplest_eng_fra.csv'\n",
    "# dir_txt = cwd + '\\data\\eng-fra.txt'\n",
    "# print(dir_csv)\n",
    "# print(dir_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(dir_csv, encoding='utf-8', sep=',', header=0, index_col=0)\n",
    "inf_data = pd.read_csv(dir_txt, encoding='utf-8', sep='\\t', header=0, names=['eng', 'fra'])   # for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.684092Z",
     "iopub.status.busy": "2021-10-22T17:49:16.683551Z",
     "iopub.status.idle": "2021-10-22T17:49:16.693325Z",
     "shell.execute_reply": "2021-10-22T17:49:16.692523Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.684051Z"
    }
   },
   "outputs": [],
   "source": [
    "print(inf_data.shape)\n",
    "print(inf_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.695636Z",
     "iopub.status.busy": "2021-10-22T17:49:16.695186Z",
     "iopub.status.idle": "2021-10-22T17:49:16.703792Z",
     "shell.execute_reply": "2021-10-22T17:49:16.703049Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.695597Z"
    },
    "id": "1rcRQeoIQHfB",
    "outputId": "e24a8357-7002-4ba3-96b1-ae75188e84f4"
   },
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.705607Z",
     "iopub.status.busy": "2021-10-22T17:49:16.705184Z",
     "iopub.status.idle": "2021-10-22T17:49:16.724015Z",
     "shell.execute_reply": "2021-10-22T17:49:16.723310Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.705569Z"
    },
    "id": "AB3DA-UtIREm",
    "outputId": "a4d71d72-2daa-421f-99a0-ecf470809777"
   },
   "outputs": [],
   "source": [
    "# drop duplicate translations\n",
    "data = data.drop_duplicates()\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.725936Z",
     "iopub.status.busy": "2021-10-22T17:49:16.725191Z",
     "iopub.status.idle": "2021-10-22T17:49:16.735668Z",
     "shell.execute_reply": "2021-10-22T17:49:16.734915Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.725899Z"
    },
    "id": "pVMjNoSBIREm",
    "outputId": "403775cd-4452-4956-fdea-e30ad356dfd3"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data to remove bias in dev set selection.\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.737788Z",
     "iopub.status.busy": "2021-10-22T17:49:16.737076Z",
     "iopub.status.idle": "2021-10-22T17:49:16.750727Z",
     "shell.execute_reply": "2021-10-22T17:49:16.749979Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.737753Z"
    },
    "id": "IeJOXemXIREm",
    "outputId": "651a5651-dfad-4d5c-d3ed-8d6e481b6c03"
   },
   "outputs": [],
   "source": [
    "# Remove empty rows\n",
    "data = data.dropna()\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.752944Z",
     "iopub.status.busy": "2021-10-22T17:49:16.752636Z",
     "iopub.status.idle": "2021-10-22T17:49:16.775614Z",
     "shell.execute_reply": "2021-10-22T17:49:16.774990Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.752908Z"
    },
    "id": "W6GaTAhLIREn"
   },
   "outputs": [],
   "source": [
    "X_train_full = data.loc[data['split'] == 'train', 'source_language'].copy(deep=True)\n",
    "X_val_full = data.loc[data['split'] == 'val', 'source_language'].copy(deep=True)\n",
    "X_test_full = data.loc[data['split'] == 'test', 'source_language'].copy(deep=True)\n",
    "y_train_full = data.loc[data['split'] == 'train', 'target_language'].copy(deep=True)\n",
    "y_val_full = data.loc[data['split'] == 'val', 'target_language'].copy(deep=True)\n",
    "y_test_full = data.loc[data['split'] == 'test', 'target_language'].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.777304Z",
     "iopub.status.busy": "2021-10-22T17:49:16.776664Z",
     "iopub.status.idle": "2021-10-22T17:49:16.784904Z",
     "shell.execute_reply": "2021-10-22T17:49:16.784197Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.777268Z"
    },
    "id": "N7bK6UE1IREn",
    "outputId": "29c298a0-12b6-45e7-e886-94c6e1dbf083"
   },
   "outputs": [],
   "source": [
    "print(type(X_train_full), X_train_full.shape)\n",
    "print(type(X_val_full), X_val_full.shape)\n",
    "print(type(X_test_full), X_test_full.shape)\n",
    "print(type(y_train_full), y_train_full.shape)\n",
    "print(type(y_val_full), y_val_full.shape)\n",
    "print(type(y_test_full), y_test_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RULPg12gIREn"
   },
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.786426Z",
     "iopub.status.busy": "2021-10-22T17:49:16.786047Z",
     "iopub.status.idle": "2021-10-22T17:49:16.791667Z",
     "shell.execute_reply": "2021-10-22T17:49:16.790816Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.786389Z"
    },
    "id": "KdIZ5v0sIREn"
   },
   "outputs": [],
   "source": [
    "# train_rows = 5\n",
    "# val_rows = 5\n",
    "# test_rows = 5\n",
    "train_rows = len(X_train_full)\n",
    "val_rows = len(X_val_full)\n",
    "test_rows = len(X_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.793347Z",
     "iopub.status.busy": "2021-10-22T17:49:16.792935Z",
     "iopub.status.idle": "2021-10-22T17:49:16.803582Z",
     "shell.execute_reply": "2021-10-22T17:49:16.802656Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.793310Z"
    },
    "id": "U_vCJf8oIREn",
    "outputId": "c6d87c6f-c2f2-4674-d847-a50df23479a6"
   },
   "outputs": [],
   "source": [
    "X_train_subset = X_train_full.head(train_rows).copy(deep=True)\n",
    "y_train_subset = y_train_full.head(train_rows).copy(deep=True)\n",
    "print(X_train_subset.shape)\n",
    "print(y_train_subset.shape)\n",
    "print(X_train_subset.head())\n",
    "print(y_train_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.805587Z",
     "iopub.status.busy": "2021-10-22T17:49:16.805065Z",
     "iopub.status.idle": "2021-10-22T17:49:16.814249Z",
     "shell.execute_reply": "2021-10-22T17:49:16.813433Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.805551Z"
    },
    "id": "Ikt8oJFfIREn",
    "outputId": "ec469581-900b-47e1-96af-da519e084088"
   },
   "outputs": [],
   "source": [
    "X_val_subset = X_val_full.head(val_rows).copy(deep=True)\n",
    "y_val_subset = y_val_full.head(val_rows).copy(deep=True)\n",
    "print(X_val_subset.shape)\n",
    "print(y_val_subset.shape)\n",
    "print(X_val_subset.head())\n",
    "print(y_val_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.816223Z",
     "iopub.status.busy": "2021-10-22T17:49:16.815641Z",
     "iopub.status.idle": "2021-10-22T17:49:16.825623Z",
     "shell.execute_reply": "2021-10-22T17:49:16.824644Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.816102Z"
    },
    "id": "Tgd8-yluIREo",
    "outputId": "a0c2c661-7eaf-4047-fce2-d88020740d67"
   },
   "outputs": [],
   "source": [
    "X_test_subset = X_test_full.head(test_rows).copy(deep=True)\n",
    "y_test_subset = y_test_full.head(test_rows).copy(deep=True)\n",
    "print(X_test_subset.shape)\n",
    "print(y_test_subset.shape)\n",
    "print(X_test_subset.head())\n",
    "print(y_test_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4akv3AeVIREo"
   },
   "source": [
    "### Data Cleaning And Tokenization\n",
    "\n",
    "In addition to creating a subset that has three partitions for training, validation, and testing, we also minimally clean the data by adding whitespace around punctuation symbols and removing extraneous symbols that aren‚Äôt punctuation for all the splits.\n",
    "\n",
    "\n",
    "1. **apply** works on a row / column basis of a DataFrame \n",
    "2. **applymap** works element-wise on a DataFrame\n",
    "3. **map** works element-wise on a Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:16.827476Z",
     "iopub.status.busy": "2021-10-22T17:49:16.826732Z",
     "iopub.status.idle": "2021-10-22T17:49:18.694474Z",
     "shell.execute_reply": "2021-10-22T17:49:18.693776Z",
     "shell.execute_reply.started": "2021-10-22T17:49:16.827438Z"
    },
    "id": "lFXOUNo0IREo"
   },
   "outputs": [],
   "source": [
    "X_train = X_train_subset.map(lambda x: clean_tokenise(text=x, TOK=S_TOK))\n",
    "X_val = X_val_subset.map(lambda x: clean_tokenise(text=x, TOK=S_TOK))\n",
    "X_test = X_test_subset.map(lambda x: clean_tokenise(text=x, TOK=S_TOK))\n",
    "y_train = y_train_subset.map(lambda x: clean_tokenise(text=x, TOK=T_TOK))\n",
    "y_val = y_val_subset.map(lambda x: clean_tokenise(text=x, TOK=T_TOK))\n",
    "y_test = y_test_subset.map(lambda x: clean_tokenise(text=x, TOK=T_TOK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:18.697122Z",
     "iopub.status.busy": "2021-10-22T17:49:18.696684Z",
     "iopub.status.idle": "2021-10-22T17:49:18.714697Z",
     "shell.execute_reply": "2021-10-22T17:49:18.713651Z",
     "shell.execute_reply.started": "2021-10-22T17:49:18.697083Z"
    },
    "id": "JkUaubEcIREo",
    "outputId": "5b7a0470-a1ab-47e9-ab31-122c1095baaa"
   },
   "outputs": [],
   "source": [
    "print(X_train.head(), '\\n')\n",
    "print(y_train.head(), '\\n\\n')\n",
    "\n",
    "print(X_val.head(), '\\n')\n",
    "print(y_val.head(), '\\n\\n')\n",
    "\n",
    "print(X_test.head(), '\\n')\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQI9IYnYIREo"
   },
   "source": [
    "### Max length of source and target sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:18.716410Z",
     "iopub.status.busy": "2021-10-22T17:49:18.715992Z",
     "iopub.status.idle": "2021-10-22T17:49:18.739320Z",
     "shell.execute_reply": "2021-10-22T17:49:18.738694Z",
     "shell.execute_reply.started": "2021-10-22T17:49:18.716367Z"
    },
    "id": "PTr0PC0CIREo"
   },
   "outputs": [],
   "source": [
    "X_train_max_len = X_train.map(lambda x: len(x)).max()\n",
    "X_val_max_len = X_val.map(lambda x: len(x)).max()\n",
    "X_test_max_len = X_test.map(lambda x: len(x)).max()\n",
    "y_train_max_len = y_train.map(lambda x: len(x)).max()\n",
    "y_val_max_len = y_val.map(lambda x: len(x)).max()\n",
    "y_test_max_len = y_test.map(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:18.745123Z",
     "iopub.status.busy": "2021-10-22T17:49:18.744932Z",
     "iopub.status.idle": "2021-10-22T17:49:18.750996Z",
     "shell.execute_reply": "2021-10-22T17:49:18.750171Z",
     "shell.execute_reply.started": "2021-10-22T17:49:18.745100Z"
    },
    "id": "vlo0udyaIREo",
    "outputId": "d130c917-ff01-4368-dab1-4d3a14361447"
   },
   "outputs": [],
   "source": [
    "print(X_train_max_len)\n",
    "print(X_val_max_len)\n",
    "print(X_test_max_len)\n",
    "print(y_train_max_len)\n",
    "print(y_val_max_len)\n",
    "print(y_test_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:18.752755Z",
     "iopub.status.busy": "2021-10-22T17:49:18.752216Z",
     "iopub.status.idle": "2021-10-22T17:49:18.757158Z",
     "shell.execute_reply": "2021-10-22T17:49:18.756440Z",
     "shell.execute_reply.started": "2021-10-22T17:49:18.752720Z"
    },
    "id": "jnfIzkC3IREp"
   },
   "outputs": [],
   "source": [
    "# for encoding and padding  \n",
    "s_max_len = max(X_train_max_len, X_val_max_len, X_test_max_len)\n",
    "t_max_len = max(y_train_max_len, y_val_max_len, y_test_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:18.759047Z",
     "iopub.status.busy": "2021-10-22T17:49:18.758501Z",
     "iopub.status.idle": "2021-10-22T17:49:18.765988Z",
     "shell.execute_reply": "2021-10-22T17:49:18.765227Z",
     "shell.execute_reply.started": "2021-10-22T17:49:18.758896Z"
    },
    "id": "MIt_PQeeIREp",
    "outputId": "373306e8-d438-4f6b-9e65-6778c7112f49"
   },
   "outputs": [],
   "source": [
    "print(s_max_len)\n",
    "print(t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:18.768010Z",
     "iopub.status.busy": "2021-10-22T17:49:18.767519Z",
     "iopub.status.idle": "2021-10-22T17:49:18.783013Z",
     "shell.execute_reply": "2021-10-22T17:49:18.782358Z",
     "shell.execute_reply.started": "2021-10-22T17:49:18.767971Z"
    },
    "id": "phH_ge9-IREp"
   },
   "outputs": [],
   "source": [
    "# This is needed to calculate the epoch loss per token during training\n",
    "# We do not include the <SOS> token in the count!\n",
    "y_train_tot_tokens = y_train.map(lambda x: len(x)).sum()\n",
    "y_val_tot_tokens = y_val.map(lambda x: len(x)).sum()\n",
    "y_test_tot_tokens = y_test.map(lambda x: len(x)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:18.785691Z",
     "iopub.status.busy": "2021-10-22T17:49:18.785506Z",
     "iopub.status.idle": "2021-10-22T17:49:18.790101Z",
     "shell.execute_reply": "2021-10-22T17:49:18.789367Z",
     "shell.execute_reply.started": "2021-10-22T17:49:18.785669Z"
    },
    "id": "fYBx7N0sIREp",
    "outputId": "3851c96c-6a8a-4028-9117-350f38b747cf"
   },
   "outputs": [],
   "source": [
    "print(y_train_tot_tokens)\n",
    "print(y_val_tot_tokens)\n",
    "print(y_test_tot_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs1fsp2UIREp"
   },
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:49:18.791807Z",
     "iopub.status.busy": "2021-10-22T17:49:18.791418Z",
     "iopub.status.idle": "2021-10-22T17:50:04.897781Z",
     "shell.execute_reply": "2021-10-22T17:50:04.896418Z",
     "shell.execute_reply.started": "2021-10-22T17:49:18.791771Z"
    },
    "id": "h6eq5g30IREp"
   },
   "outputs": [],
   "source": [
    "# From glove txt, create a dictionary of all glove embeddings where KEY is a WORD, and VALUE is a NUMPY ARRAY:\n",
    "# glove_embeddings = load_glove_from_file('C:/GloVe/glove.6B.300d.txt')\n",
    "glove_embeddings = load_glove_from_file('../input/glove6b300dtxt/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:04.900938Z",
     "iopub.status.busy": "2021-10-22T17:50:04.900626Z",
     "iopub.status.idle": "2021-10-22T17:50:04.910478Z",
     "shell.execute_reply": "2021-10-22T17:50:04.909697Z",
     "shell.execute_reply.started": "2021-10-22T17:50:04.900896Z"
    },
    "id": "pimR5WBPmTZ_",
    "outputId": "87db2cbc-807a-4101-bb10-36e638fe5de0"
   },
   "outputs": [],
   "source": [
    "print(type(glove_embeddings))\n",
    "print(len(glove_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEaqc2J1IREp"
   },
   "source": [
    "### Create Vocabulary From Training Corpus\n",
    "\n",
    "The embedding matrix (see later) is created only from the training dataset.\n",
    "\n",
    "The training dataset should be sufficiently rich/representative enough to cover all data you expect to see in the future.\n",
    "\n",
    "New data must have the same integer encoding as the training data prior to being mapped onto the embedding when making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:04.913142Z",
     "iopub.status.busy": "2021-10-22T17:50:04.912482Z",
     "iopub.status.idle": "2021-10-22T17:50:04.968951Z",
     "shell.execute_reply": "2021-10-22T17:50:04.967973Z",
     "shell.execute_reply.started": "2021-10-22T17:50:04.913101Z"
    },
    "id": "8pSR7_lIIREp"
   },
   "outputs": [],
   "source": [
    "# create a DICT of the unique words in the training set\n",
    "X_words_before, X_words_after, X_final_words, X_dict_w2i, X_dict_i2w = tokens_dict(X_train)\n",
    "y_words_before, y_words_after, y_final_words, y_dict_w2i, y_dict_i2w = tokens_dict(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:04.970873Z",
     "iopub.status.busy": "2021-10-22T17:50:04.970585Z",
     "iopub.status.idle": "2021-10-22T17:50:04.979120Z",
     "shell.execute_reply": "2021-10-22T17:50:04.978182Z",
     "shell.execute_reply.started": "2021-10-22T17:50:04.970822Z"
    },
    "id": "UZItRx1OIREq",
    "outputId": "5ae1932a-e863-4256-a520-2ef3e597ef77"
   },
   "outputs": [],
   "source": [
    "print(X_words_before)\n",
    "print(X_words_after)\n",
    "print(len(X_dict_w2i))\n",
    "print()\n",
    "print(y_words_before)\n",
    "print(y_words_after)\n",
    "print(len(y_dict_w2i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h4G4SlHIREq"
   },
   "source": [
    "### Create Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CKwC4AIIREq"
   },
   "source": [
    "The embedding is created from the training dataset.\n",
    "\n",
    "It should be sufficiently rich/representative enough to cover all data you expect to in the future.\n",
    "\n",
    "New data must have the same integer encoding as the training data prior to being mapped onto the embedding when making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:04.981793Z",
     "iopub.status.busy": "2021-10-22T17:50:04.981107Z",
     "iopub.status.idle": "2021-10-22T17:50:05.054634Z",
     "shell.execute_reply": "2021-10-22T17:50:05.053921Z",
     "shell.execute_reply.started": "2021-10-22T17:50:04.981745Z"
    },
    "id": "7l6f7L2xIREq"
   },
   "outputs": [],
   "source": [
    "X_excluded, X_embeddings_matrix = make_embedding_matrix(glove_embeddings, X_dict_w2i) # embedding matrix is a numpy array\n",
    "y_excluded, y_embeddings_matrix = make_embedding_matrix(glove_embeddings, y_dict_w2i) # embedding matrix is a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:05.056403Z",
     "iopub.status.busy": "2021-10-22T17:50:05.056159Z",
     "iopub.status.idle": "2021-10-22T17:50:05.062699Z",
     "shell.execute_reply": "2021-10-22T17:50:05.060804Z",
     "shell.execute_reply.started": "2021-10-22T17:50:05.056372Z"
    },
    "id": "SAb6pf_qIREq",
    "outputId": "3832ab36-6a36-4fad-fa5f-67e823406c85"
   },
   "outputs": [],
   "source": [
    "# print(X_excluded)\n",
    "# print(len(X_excluded))\n",
    "# print(type(X_embeddings_matrix))\n",
    "# print(X_embeddings_matrix.shape)\n",
    "# print()\n",
    "# print(y_excluded)\n",
    "# print(len(y_excluded))\n",
    "# print(type(y_embeddings_matrix))\n",
    "# print(y_embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhhnfPeGIREq"
   },
   "source": [
    "### Encoding Training Dataset\n",
    "\n",
    "We need to convert our text into a numerical form that can be fed to our model as input.\n",
    "\n",
    "1. We have create a vocabulary (see section '10. Vocabulary') where each key is a unique word from the training corpus, and each value is the index of that word in the 'tokens' dictionary.\n",
    "2. Choose the maximum length of any review.\n",
    "3. Encode each list of tokens by replacing each word with its index from the 'tokens' dictionary.\n",
    "\n",
    "Note: **mean_len** (see below) is the mean of tokens length in the training set. We set the max length of the encoded reviews equal to the mean_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:05.065999Z",
     "iopub.status.busy": "2021-10-22T17:50:05.064592Z",
     "iopub.status.idle": "2021-10-22T17:50:05.281152Z",
     "shell.execute_reply": "2021-10-22T17:50:05.279416Z",
     "shell.execute_reply.started": "2021-10-22T17:50:05.065961Z"
    },
    "id": "hbeM_cqoIREq"
   },
   "outputs": [],
   "source": [
    "X_train_encoded = X_train.map(lambda x: encoding(input_sequence=x, tokens_dictionary=X_dict_w2i, max_len=s_max_len))\n",
    "X_val_encoded = X_val.map(lambda x: encoding(input_sequence=x, tokens_dictionary=X_dict_w2i, max_len=s_max_len))\n",
    "X_test_encoded = X_test.map(lambda x: encoding(input_sequence=x, tokens_dictionary=X_dict_w2i, max_len=s_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:05.287505Z",
     "iopub.status.busy": "2021-10-22T17:50:05.285501Z",
     "iopub.status.idle": "2021-10-22T17:50:05.470288Z",
     "shell.execute_reply": "2021-10-22T17:50:05.469560Z",
     "shell.execute_reply.started": "2021-10-22T17:50:05.287434Z"
    },
    "id": "f_FvISVIIREq"
   },
   "outputs": [],
   "source": [
    "y_train_encoded = y_train.map(lambda x: encoding(input_sequence=x, tokens_dictionary=y_dict_w2i, max_len=t_max_len))\n",
    "y_val_encoded = y_val.map(lambda x: encoding(input_sequence=x, tokens_dictionary=y_dict_w2i, max_len=t_max_len))\n",
    "y_test_encoded = y_test.map(lambda x: encoding(input_sequence=x, tokens_dictionary=y_dict_w2i, max_len=t_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:05.472118Z",
     "iopub.status.busy": "2021-10-22T17:50:05.471714Z",
     "iopub.status.idle": "2021-10-22T17:50:05.483604Z",
     "shell.execute_reply": "2021-10-22T17:50:05.482876Z",
     "shell.execute_reply.started": "2021-10-22T17:50:05.472082Z"
    },
    "id": "xcktJ4xJIREr",
    "outputId": "b6b068a4-41f3-4385-9e9c-6696612ed3d5"
   },
   "outputs": [],
   "source": [
    "print(X_train_encoded.head())\n",
    "print()\n",
    "print(y_train_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:05.486833Z",
     "iopub.status.busy": "2021-10-22T17:50:05.486623Z",
     "iopub.status.idle": "2021-10-22T17:50:05.498870Z",
     "shell.execute_reply": "2021-10-22T17:50:05.498130Z",
     "shell.execute_reply.started": "2021-10-22T17:50:05.486809Z"
    },
    "id": "y0kF3eS9IREr",
    "outputId": "7598fa90-f626-492c-95c3-3a4065f3b49a"
   },
   "outputs": [],
   "source": [
    "print(X_val_encoded.head())\n",
    "print()\n",
    "print(y_val_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:05.500562Z",
     "iopub.status.busy": "2021-10-22T17:50:05.500308Z",
     "iopub.status.idle": "2021-10-22T17:50:05.512878Z",
     "shell.execute_reply": "2021-10-22T17:50:05.512140Z",
     "shell.execute_reply.started": "2021-10-22T17:50:05.500529Z"
    },
    "id": "yYVtbu8XIREr",
    "outputId": "d74c7bd3-5dc4-43d3-a1c0-6b7be6df611d"
   },
   "outputs": [],
   "source": [
    "print(X_test_encoded.head())\n",
    "print()\n",
    "print(y_test_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGr7sFE8IREr"
   },
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:05.514671Z",
     "iopub.status.busy": "2021-10-22T17:50:05.514332Z",
     "iopub.status.idle": "2021-10-22T17:50:06.208956Z",
     "shell.execute_reply": "2021-10-22T17:50:06.208129Z",
     "shell.execute_reply.started": "2021-10-22T17:50:05.514569Z"
    },
    "id": "j-n3Q4LkIREr"
   },
   "outputs": [],
   "source": [
    "# Convert pd.Series to PyTorch Tensors\n",
    "# NB: set the values in X_train, X_val and X_test as a list of arrays (as opposed to array of arrays) --- see above\n",
    "\n",
    "x_train_tensor = torch.Tensor(list(X_train_encoded.values))\n",
    "x_val_tensor = torch.Tensor(list(X_val_encoded.values))\n",
    "x_test_tensor = torch.Tensor(list(X_test_encoded.values))\n",
    "y_train_tensor = torch.Tensor(list(y_train_encoded.values))\n",
    "y_val_tensor = torch.Tensor(list(y_val_encoded.values))\n",
    "y_test_tensor = torch.Tensor(list(y_test_encoded.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:06.210916Z",
     "iopub.status.busy": "2021-10-22T17:50:06.210605Z",
     "iopub.status.idle": "2021-10-22T17:50:06.217752Z",
     "shell.execute_reply": "2021-10-22T17:50:06.217097Z",
     "shell.execute_reply.started": "2021-10-22T17:50:06.210879Z"
    },
    "id": "YMp_CrCyIREr"
   },
   "outputs": [],
   "source": [
    "# Create a full dataset (like a DataFrame in Pandas) from the two tensors\n",
    "train_dataset =  TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "356vQK1bIREw"
   },
   "source": [
    "### PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:06.219681Z",
     "iopub.status.busy": "2021-10-22T17:50:06.219290Z",
     "iopub.status.idle": "2021-10-22T17:50:06.227729Z",
     "shell.execute_reply": "2021-10-22T17:50:06.227024Z",
     "shell.execute_reply.started": "2021-10-22T17:50:06.219646Z"
    },
    "id": "r03WQ0sZIREw"
   },
   "outputs": [],
   "source": [
    "# For small dataset is fine to use the whole training data at every training step (i.e. batch gradient descent). \n",
    "# If we want to go serious about all this, we must use mini-batch gradient descent. Thus, we need mini-batches. \n",
    "# Thus, we need to slice our dataset accordingly. Do you want to do it manually?! Me neither!\n",
    "# So we use the 'DataLoader' class for this job. We tell it which dataset to use, the desired mini-batch size and if we‚Äôd \n",
    "# like to shuffle it or not. That‚Äôs it!\n",
    "# Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time.\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "# To retrieve a sample mini-batch, one can simply run the command below.\n",
    "# It will return a list containing two tensors: one for the features, another one for the labels:\n",
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIs-TGPBIREw"
   },
   "source": [
    "### Initialise Classifier\n",
    "\n",
    "At its core, the training routine is responsible for instantiating the model, iterating over the dataset, computing the output of the model when given the data as input, computing the loss (how wrong the model is), and updating the model proportional to the loss. \n",
    "\n",
    "Although this may seem like a lot of details to manage, there are not many places to change the training routine, and as such it will become habitual in your deep learning development process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:06.229647Z",
     "iopub.status.busy": "2021-10-22T17:50:06.229451Z",
     "iopub.status.idle": "2021-10-22T17:50:06.241540Z",
     "shell.execute_reply": "2021-10-22T17:50:06.240838Z",
     "shell.execute_reply.started": "2021-10-22T17:50:06.229621Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(X_dict_w2i))\n",
    "print(X_embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:06.243370Z",
     "iopub.status.busy": "2021-10-22T17:50:06.243055Z",
     "iopub.status.idle": "2021-10-22T17:50:06.249581Z",
     "shell.execute_reply": "2021-10-22T17:50:06.248648Z",
     "shell.execute_reply.started": "2021-10-22T17:50:06.243337Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(y_dict_w2i))\n",
    "print(y_embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:06.251676Z",
     "iopub.status.busy": "2021-10-22T17:50:06.251297Z",
     "iopub.status.idle": "2021-10-22T17:50:14.696423Z",
     "shell.execute_reply": "2021-10-22T17:50:14.695558Z",
     "shell.execute_reply.started": "2021-10-22T17:50:06.251574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialise encoder & decoder\n",
    "\n",
    "e_emb_num, e_emb_dim = X_embeddings_matrix.shape\n",
    "e_hidden_size = 1024\n",
    "e_num_layers = 1\n",
    "e_dropout_p = 0.5\n",
    "e_batch_first = True\n",
    "e_bias=True\n",
    "e_bidirectional=True\n",
    "e_pretrained_embeddings=X_embeddings_matrix\n",
    "\n",
    "d_emb_num, d_emb_dim = y_embeddings_matrix.shape\n",
    "v_dim=3\n",
    "d_num_layers = 1\n",
    "d_hidden_size = 1024\n",
    "d_dropout_p = 0.5\n",
    "d_batch_first = True\n",
    "d_bias=True\n",
    "d_bidirectional=False\n",
    "d_pretrained_embeddings=y_embeddings_matrix\n",
    "\n",
    "padding_idx = 0\n",
    "\n",
    "\n",
    "\n",
    "encoder_gru = Encoder(emb_dim=e_emb_dim, emb_num=e_emb_num, hidden_size=e_hidden_size, num_layers=e_num_layers, \n",
    "                      dropout_p=e_dropout_p, batch_first=e_batch_first, bias=e_bias, bidirectional=e_bidirectional, \n",
    "                      pretrained_embeddings=e_pretrained_embeddings, padding_idx=padding_idx).to(device)\n",
    "\n",
    "\n",
    "decoder_gru = Decoder_Bahdanau_GRU(emb_dim=d_emb_dim, emb_num=d_emb_num, encoder_hidden_size=e_hidden_size, \n",
    "                                   decoder_hidden_size=d_hidden_size, v_dim=v_dim, num_layers=d_num_layers, \n",
    "                                   dropout_p=d_dropout_p, batch_first=d_batch_first, bias=d_bias, \n",
    "                                   bidirectional=d_bidirectional, pretrained_embeddings=d_pretrained_embeddings, \n",
    "                                   padding_idx=padding_idx).to(device)\n",
    "\n",
    "# decoder_gru = Decoder_dot_GRU(emb_dim=d_emb_dim, emb_num=d_emb_num, encoder_hidden_size=e_hidden_size, \n",
    "#                                    decoder_hidden_size=d_hidden_size, num_layers=d_num_layers, \n",
    "#                                    dropout_p=d_dropout_p, batch_first=d_batch_first, bias=d_bias, \n",
    "#                                    bidirectional=d_bidirectional, pretrained_embeddings=d_pretrained_embeddings, \n",
    "#                                    padding_idx=padding_idx).to(device)\n",
    "\n",
    "# decoder_gru = Decoder_Luong_GRU(emb_dim=d_emb_dim, emb_num=d_emb_num, encoder_hidden_size=e_hidden_size, \n",
    "#                                 decoder_hidden_size=d_hidden_size, num_layers=d_num_layers, \n",
    "#                                 dropout_p=d_dropout_p, batch_first=d_batch_first, bias=d_bias, \n",
    "#                                 bidirectional=d_bidirectional, pretrained_embeddings=d_pretrained_embeddings, \n",
    "#                                 padding_idx=padding_idx).to(device)\n",
    "\n",
    "print(encoder_gru)\n",
    "print()\n",
    "print(decoder_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:14.698360Z",
     "iopub.status.busy": "2021-10-22T17:50:14.697906Z",
     "iopub.status.idle": "2021-10-22T17:50:14.707324Z",
     "shell.execute_reply": "2021-10-22T17:50:14.705282Z",
     "shell.execute_reply.started": "2021-10-22T17:50:14.698317Z"
    },
    "id": "xTGgYFVKIREw"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = Seq2Seq(encoder_gru, decoder_gru, target_vocab_size=d_emb_num).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:14.709204Z",
     "iopub.status.busy": "2021-10-22T17:50:14.708893Z",
     "iopub.status.idle": "2021-10-22T17:50:14.717398Z",
     "shell.execute_reply": "2021-10-22T17:50:14.716422Z",
     "shell.execute_reply.started": "2021-10-22T17:50:14.709165Z"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvVdH3s2IREz"
   },
   "source": [
    "## Training loop\n",
    "\n",
    "The training loop is composed of two loops: an inner loop over minibatches in the dataset, and an outer loop, which repeats the inner loop a number of times. In the inner loop, losses are computed for each minibatch, and the optimizer is used to\n",
    "update the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder parameters before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:14.719571Z",
     "iopub.status.busy": "2021-10-22T17:50:14.719039Z",
     "iopub.status.idle": "2021-10-22T17:50:14.728555Z",
     "shell.execute_reply": "2021-10-22T17:50:14.727562Z",
     "shell.execute_reply.started": "2021-10-22T17:50:14.719529Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_params_before = params(encoder_gru.emb)\n",
    "# print(encoder_params_before, '\\n')\n",
    "\n",
    "# # check params initialised by the classifier\n",
    "# print(check_params(encoder_gru), '\\n')\n",
    "\n",
    "# trainable / all params before training\n",
    "encoder_trainable_params, encoder_all_params = num_params(encoder_gru)\n",
    "print('The encoder has {:,} trainable parameters'.format(encoder_trainable_params))\n",
    "print('The encoder has {:,} parameters overall'.format(encoder_all_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder parameters before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:14.730437Z",
     "iopub.status.busy": "2021-10-22T17:50:14.730122Z",
     "iopub.status.idle": "2021-10-22T17:50:14.739827Z",
     "shell.execute_reply": "2021-10-22T17:50:14.739027Z",
     "shell.execute_reply.started": "2021-10-22T17:50:14.730401Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_params_before = params(decoder_gru.emb)\n",
    "# print(decoder_params_before, '\\n')\n",
    "\n",
    "# # check params initialised by the classifier\n",
    "# print(check_params(decoder_gru), '\\n')\n",
    "\n",
    "# trainable / all params before training\n",
    "decoder_trainable_params, decoder_all_params = num_params(decoder_gru)\n",
    "print('The decoder has {:,} trainable parameters'.format(decoder_trainable_params))\n",
    "print('The decoder has {:,} parameters overall'.format(decoder_all_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:50:14.741753Z",
     "iopub.status.busy": "2021-10-22T17:50:14.741274Z",
     "iopub.status.idle": "2021-10-22T17:53:49.128073Z",
     "shell.execute_reply": "2021-10-22T17:53:49.126166Z",
     "shell.execute_reply.started": "2021-10-22T17:50:14.741712Z"
    },
    "id": "TkaxSFo1IREz",
    "outputId": "4f337825-cb0e-41dc-a751-4d98b05c72d4"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "num_epochs = 10\n",
    "num_epoch_freezed = 3\n",
    "\n",
    "epoch_loss_train = 0.0\n",
    "epoch_loss_train_lst = []\n",
    "\n",
    "epoch_loss_val = 0.0\n",
    "epoch_loss_val_lst = []\n",
    "\n",
    "s = 'He is doing a great job!'\n",
    "# s = ['He is doing a great job!']\n",
    "# s = pd.Series(['He is doing a great job!'])\n",
    "\n",
    "# For a certain number of epochs (defined by 'n_epoch_freezed'), the emebdding matrix is frozen, then it is unfrozen \n",
    "# i.e. the embeddings get trained (except for the padding vector which remains 0)\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch < num_epoch_freezed:   \n",
    "        pass   # keep the embedding layer frozen (i.e. classifier.emb.weight.requires_grad=False as set in section 8 above)\n",
    "    else: \n",
    "        encoder_gru.emb.weight.requires_grad=True\n",
    "        decoder_gru.emb.weight.requires_grad=True\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "    print(\"Epoch: {} / {}\".format(epoch+1, num_epochs))\n",
    "    \n",
    "    ####################################################### TRAINING ###################################################\n",
    "    \n",
    "    model.train(True)\n",
    "        \n",
    "    for i, (x_train, y_train) in enumerate(train_loader):\n",
    "        input = x_train.long().to(device)\n",
    "        target = y_train.long().to(device)\n",
    "\n",
    "        # Pass the input and target for model's forward method\n",
    "        output = model(input, target, tfr=0.5)\n",
    "        \n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target.permute(1,0)\n",
    "        target = target[1:].reshape(-1)\n",
    "        \n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values using the gradients we calculated using bp \n",
    "        optimizer.step()\n",
    "        epoch_loss_train += loss.item()\n",
    "        \n",
    "#     print(\"Epoch_Loss Train: {}\".format(epoch_loss_train / len(train_loader) / (epoch+1)))\n",
    "    epoch_loss_train_lst.append(epoch_loss_train / len(train_loader) / (epoch+1))\n",
    "    \n",
    "    ###################################################### VALIDATION ##################################################\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    #We use torch.no_grad() which reduces memory usage and speeds up computation.\n",
    "    with torch.no_grad():     #https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/3 : torch.no_grad() deals with the autograd engine and stops it from calculating the gradients, which is the recommended way of doing validation\n",
    "        for i, (x_val, y_val) in enumerate(val_loader):\n",
    "            input = x_val.long().to(device)\n",
    "            target = y_val.long().to(device) \n",
    "            \n",
    "            # Pass the input and target for model's forward method\n",
    "            output = model(input, target, tfr=0.5)\n",
    "            \n",
    "            output = output[1:].reshape(-1, output.shape[2])\n",
    "            target = target.permute(1,0)\n",
    "            target = target[1:].reshape(-1)\n",
    "            \n",
    "            # Calculate the loss value for every epoch\n",
    "            loss = criterion(output, target)\n",
    "            epoch_loss_val += loss.item()\n",
    "    \n",
    "    epoch_loss_val_lst.append(epoch_loss_val / len(val_loader) / (epoch+1))\n",
    "    \n",
    "    \n",
    "    print('Train Loss: {:.6f} | Val Loss: {:.6f}'.format(epoch_loss_train / len(train_loader) / (epoch+1),\n",
    "                                                         epoch_loss_val / len(val_loader) / (epoch+1))) \n",
    "    \n",
    "    ###################################################### TRANSLATION ################################################    \n",
    "    \n",
    "    translation = translation_e2e(s, 20)\n",
    "    print('Translated sentence: \\n {}'.format(translation))    \n",
    "    print()\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:53:49.130222Z",
     "iopub.status.busy": "2021-10-22T17:53:49.129936Z",
     "iopub.status.idle": "2021-10-22T17:53:49.135384Z",
     "shell.execute_reply": "2021-10-22T17:53:49.134706Z",
     "shell.execute_reply.started": "2021-10-22T17:53:49.130181Z"
    },
    "id": "bTTUIr6IIREz"
   },
   "outputs": [],
   "source": [
    "print('It took {:,.0f} mins to complete'.format((end - start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:53:49.137397Z",
     "iopub.status.busy": "2021-10-22T17:53:49.136935Z",
     "iopub.status.idle": "2021-10-22T17:53:49.383652Z",
     "shell.execute_reply": "2021-10-22T17:53:49.382894Z",
     "shell.execute_reply.started": "2021-10-22T17:53:49.137361Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(epoch_loss_train_lst)), epoch_loss_train_lst, label = \"train losses\")\n",
    "plt.plot(range(len(epoch_loss_val_lst)), epoch_loss_val_lst, label = \"val losses\")\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Losses')\n",
    "plt.title('Train and Val Losses by Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder parameters after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:53:49.385560Z",
     "iopub.status.busy": "2021-10-22T17:53:49.385143Z",
     "iopub.status.idle": "2021-10-22T17:53:49.393310Z",
     "shell.execute_reply": "2021-10-22T17:53:49.392571Z",
     "shell.execute_reply.started": "2021-10-22T17:53:49.385520Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_params_after = params(encoder_gru.emb)\n",
    "# print(encoder_params_after, '\\n')\n",
    "\n",
    "# # check changes in params\n",
    "# print(encoder_params_after == encoder_params_before, '\\n')\n",
    "\n",
    "# # check gradients\n",
    "# print(encoder_gru.emb.weight.grad, '\\n')\n",
    "\n",
    "# trainable / all params after training\n",
    "encoder_trainable_params, encoder_all_params = num_params(encoder_gru)\n",
    "print('The encoder has {:,} trainable parameters'.format(encoder_trainable_params))\n",
    "print('The encoder has {:,} parameters overall'.format(encoder_all_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder parameters after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:53:49.395173Z",
     "iopub.status.busy": "2021-10-22T17:53:49.394439Z",
     "iopub.status.idle": "2021-10-22T17:53:49.403397Z",
     "shell.execute_reply": "2021-10-22T17:53:49.402634Z",
     "shell.execute_reply.started": "2021-10-22T17:53:49.395132Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_params_after = params(decoder_gru.emb)\n",
    "# print(decoder_params_after, '\\n')\n",
    "\n",
    "# # check changes in params\n",
    "# print(decoder_params_after == decoder_params_before, '\\n')\n",
    "\n",
    "# # check gradients\n",
    "# print(decoder_gru.emb.weight.grad, '\\n')\n",
    "\n",
    "# trainable / all params after training\n",
    "decoder_trainable_params, decoder_all_params = num_params(decoder_gru)\n",
    "print('The decoder has {:,} trainable parameters'.format(decoder_trainable_params))\n",
    "print('The decoder has {:,} parameters overall'.format(decoder_all_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:53:49.405182Z",
     "iopub.status.busy": "2021-10-22T17:53:49.404822Z",
     "iopub.status.idle": "2021-10-22T17:53:50.978624Z",
     "shell.execute_reply": "2021-10-22T17:53:50.977064Z",
     "shell.execute_reply.started": "2021-10-22T17:53:49.405074Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch_loss_test = 0.0\n",
    "\n",
    "model.eval()\n",
    "    \n",
    "#We use torch.no_grad() which reduces memory usage and speeds up computation.\n",
    "with torch.no_grad():     #https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/3 : torch.no_grad() deals with the autograd engine and stops it from calculating the gradients, which is the recommended way of doing validation\n",
    "    for i, (x_test, y_test) in enumerate(test_loader):\n",
    "        input = x_test.long().to(device)\n",
    "        target = y_test.long().to(device) \n",
    "\n",
    "        # Pass the input and target for model's forward method\n",
    "        output = model(input, target, tfr=0.5)\n",
    "\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target.permute(1,0)\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss_test += loss.item()\n",
    "\n",
    "print('Test Loss: {:.6f}'.format(epoch_loss_test / len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:53:50.980736Z",
     "iopub.status.busy": "2021-10-22T17:53:50.980457Z",
     "iopub.status.idle": "2021-10-22T17:53:51.002762Z",
     "shell.execute_reply": "2021-10-22T17:53:51.002071Z",
     "shell.execute_reply.started": "2021-10-22T17:53:50.980698Z"
    }
   },
   "outputs": [],
   "source": [
    "inf_data_sample = inf_data.sample(20)\n",
    "inf_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:53:51.004672Z",
     "iopub.status.busy": "2021-10-22T17:53:51.003927Z",
     "iopub.status.idle": "2021-10-22T17:53:51.249567Z",
     "shell.execute_reply": "2021-10-22T17:53:51.248907Z",
     "shell.execute_reply.started": "2021-10-22T17:53:51.004631Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "candidate, reference, bleuscore = bleu(inf_data_sample['eng'], inf_data_sample['fra'], max_length=20, TOK=T_TOK)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:53:51.251164Z",
     "iopub.status.busy": "2021-10-22T17:53:51.250917Z",
     "iopub.status.idle": "2021-10-22T17:53:51.259236Z",
     "shell.execute_reply": "2021-10-22T17:53:51.258516Z",
     "shell.execute_reply.started": "2021-10-22T17:53:51.251128Z"
    }
   },
   "outputs": [],
   "source": [
    "print('It took {:,.0f} mins to complete'.format((end - start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:53:51.261145Z",
     "iopub.status.busy": "2021-10-22T17:53:51.260785Z",
     "iopub.status.idle": "2021-10-22T17:53:51.268191Z",
     "shell.execute_reply": "2021-10-22T17:53:51.267512Z",
     "shell.execute_reply.started": "2021-10-22T17:53:51.261110Z"
    }
   },
   "outputs": [],
   "source": [
    "candidate = candidate.map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:55:47.532256Z",
     "iopub.status.busy": "2021-10-22T17:55:47.531982Z",
     "iopub.status.idle": "2021-10-22T17:55:47.537612Z",
     "shell.execute_reply": "2021-10-22T17:55:47.536490Z",
     "shell.execute_reply.started": "2021-10-22T17:55:47.532228Z"
    }
   },
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({'source': inf_data_sample['eng'],\n",
    "                           'reference': inf_data_sample['fra'],\n",
    "                           'candidate': candidate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:55:48.812531Z",
     "iopub.status.busy": "2021-10-22T17:55:48.811917Z",
     "iopub.status.idle": "2021-10-22T17:55:48.837058Z",
     "shell.execute_reply": "2021-10-22T17:55:48.836277Z",
     "shell.execute_reply.started": "2021-10-22T17:55:48.812490Z"
    }
   },
   "outputs": [],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-22T17:56:00.622178Z",
     "iopub.status.busy": "2021-10-22T17:56:00.621696Z",
     "iopub.status.idle": "2021-10-22T17:56:00.627019Z",
     "shell.execute_reply": "2021-10-22T17:56:00.626280Z",
     "shell.execute_reply.started": "2021-10-22T17:56:00.622138Z"
    }
   },
   "outputs": [],
   "source": [
    "bleuscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
